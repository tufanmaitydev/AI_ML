{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "NLP_Functions.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tufanmaitydev/AI_ML/blob/master/NLP_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyWGaxGYUOdm",
        "colab_type": "raw"
      },
      "source": [
        "https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeajapEMUOdq",
        "colab_type": "code",
        "colab": {},
        "outputId": "d719c268-84ed-423e-b546-44bc12db70a3"
      },
      "source": [
        "import re \n",
        "\n",
        "def _remove_regex(input_text, regex_pattern):\n",
        "    urls = re.finditer(regex_pattern, input_text) \n",
        "    for i in urls: \n",
        "        input_text = re.sub(i.group().strip(), '', input_text)\n",
        "    return input_text\n",
        "\n",
        "regex_pattern = \"#[\\w]*\"  \n",
        "\n",
        "_remove_regex(\"remove this #hashtag from analytics vidhya\", regex_pattern)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'remove this  from analytics vidhya'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2zFX8YRUOdz",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c97e8db-0535-4898-a6de-226c61760c24"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "lem = WordNetLemmatizer()\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer \n",
        "stem = PorterStemmer()\n",
        "\n",
        "word = \"multiplying\" \n",
        "print(lem.lemmatize(word, \"v\"))\n",
        "print(stem.stem(word))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "multiply\n",
            "multipli\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-Zxk-a0UOd3",
        "colab_type": "code",
        "colab": {},
        "outputId": "fda20ea0-f8b4-4373-91ac-ecc1733a71e7"
      },
      "source": [
        "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
        "def _lookup_words(input_text):\n",
        "    words = input_text.split() \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "        if word.lower() in lookup_dict:\n",
        "            word = lookup_dict[word.lower()]\n",
        "        new_words.append(word) new_text = \" \".join(new_words) \n",
        "        return new_text\n",
        "\n",
        "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-9025b619dfd5>, line 8)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-9025b619dfd5>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    new_words.append(word) new_text = \" \".join(new_words)\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tKMN7MnUOd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc7-aBKiUOd-",
        "colab_type": "code",
        "colab": {},
        "outputId": "d8c03f3d-be7b-4b1c-c712-c9cf8004f862"
      },
      "source": [
        "#nltk.download('maxent_treebank_pos_tagger')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\tufan.maity\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hWZZ4BqUOeC",
        "colab_type": "code",
        "colab": {},
        "outputId": "0850b506-70f7-4077-e702-213d94a80519"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
        "tokens = word_tokenize(text)\n",
        "print (pos_tag(tokens))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8pmQCj6UOeF",
        "colab_type": "code",
        "colab": {},
        "outputId": "66202450-dcd0-4053-d15c-73fe6a8f6691"
      },
      "source": [
        "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
        "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
        "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
        "doc_complete = [doc1, doc2, doc3]\n",
        "doc_clean = [doc.split() for doc in doc_complete]\n",
        "\n",
        "import gensim \n",
        "import corpora\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Training LDA model on the document term matrix\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
        "\n",
        "# Results \n",
        "print(ldamodel.print_topics())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'corpus'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-26-d03d3b975109>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mINFO\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'corpus'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_RuehR9UOeJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "90801130-1bdd-4927-904d-076d6f4acf98"
      },
      "source": [
        "def generate_ngrams(text, n):\n",
        "    words = text.split()\n",
        "    output = []  \n",
        "    for i in range(len(words)-n+1):\n",
        "        output.append(words[i:i+n])\n",
        "    return output\n",
        "\n",
        "generate_ngrams('this is a sample text', 2)\n",
        "# [['this', 'is'], ['is', 'a'], ['a', 'sample'], , ['sample', 'text']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s59SVNn3UOeN",
        "colab_type": "code",
        "colab": {},
        "outputId": "abcdc0ae-d545-45bb-df73-28b8469ee165"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "obj = TfidfVectorizer()\n",
        "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
        "X = obj.fit_transform(corpus)\n",
        "print (X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.34520501686496574\n",
            "  (0, 4)\t0.444514311537431\n",
            "  (0, 2)\t0.5844829010200651\n",
            "  (0, 7)\t0.5844829010200651\n",
            "  (1, 3)\t0.652490884512534\n",
            "  (1, 0)\t0.652490884512534\n",
            "  (1, 1)\t0.3853716274664007\n",
            "  (2, 5)\t0.5844829010200651\n",
            "  (2, 6)\t0.5844829010200651\n",
            "  (2, 1)\t0.34520501686496574\n",
            "  (2, 4)\t0.444514311537431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNPOlm86UOeR",
        "colab_type": "code",
        "colab": {},
        "outputId": "27a03a1d-b1d9-4291-a765-75ec8c156e4c"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
        "# train the model on your corpus  \n",
        "model = Word2Vec(sentences, min_count = 1)\n",
        "print (model.wv.similarity('data', 'science'))\n",
        "#print (model['learning'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.02728900899895588\n",
            "[-3.7689907e-03 -4.5704409e-03 -4.8403060e-03 -4.3976321e-03\n",
            "  2.6484299e-03 -1.8470643e-04 -1.4306945e-03  1.6051953e-03\n",
            " -4.4934819e-03  3.2657242e-04  1.5824274e-03  2.1361606e-04\n",
            "  2.6585325e-03  3.4903809e-03 -2.8584986e-03  3.2721779e-03\n",
            " -4.2950795e-03  1.7314442e-03 -2.6628713e-03 -3.6838215e-03\n",
            "  7.2030741e-04  4.3405634e-03  2.8003659e-03  1.7142670e-03\n",
            " -1.4561572e-03  3.5474058e-03 -3.0246100e-03 -3.7018510e-03\n",
            "  4.4646105e-03 -4.2990725e-03 -4.0078065e-03 -1.8242844e-04\n",
            "  2.7333564e-04 -4.0703514e-03 -1.5536045e-04  1.1422887e-03\n",
            " -6.7103203e-05 -4.3603573e-03  1.3514843e-03  1.8617522e-03\n",
            "  3.6295606e-03 -2.7542184e-03 -3.3490176e-03 -4.0011462e-03\n",
            " -4.4226977e-03  3.8341214e-03 -4.0574810e-03 -1.1782767e-04\n",
            "  3.4682606e-03  3.1209714e-03 -8.3196250e-04  9.8817924e-04\n",
            " -4.7906586e-03 -6.6146860e-04 -3.7469766e-03 -3.4572450e-03\n",
            "  4.6042437e-03 -2.3445382e-03 -7.4090360e-04  3.5549514e-04\n",
            " -3.9033033e-04  2.4497842e-03 -1.8009322e-03  1.1318395e-03\n",
            " -3.6732646e-04 -2.4653052e-04  2.8828424e-03  1.3878618e-03\n",
            "  1.5660772e-03  1.6430928e-03  2.7032110e-03  4.9011689e-03\n",
            "  3.4529125e-04 -1.6595289e-03 -3.7840058e-03 -1.6304890e-03\n",
            "  4.2339838e-03  1.1957698e-03 -4.5399996e-03 -3.9542140e-03\n",
            " -1.2283006e-03  4.3590777e-03 -4.4464990e-03 -6.3078664e-04\n",
            "  1.4647391e-03  3.5634538e-04 -3.7298941e-03  3.7264938e-03\n",
            " -3.7036468e-03 -2.2691386e-03  1.0435765e-03 -3.7226710e-03\n",
            "  2.6245606e-03 -4.1827229e-03 -3.0811487e-03 -3.9960463e-03\n",
            " -7.5653236e-04  1.2066896e-03 -3.6885371e-04 -2.9378235e-03]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\tufan.maity\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning:\n",
            "\n",
            "Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt9RrvawUOeU",
        "colab_type": "code",
        "colab": {},
        "outputId": "d34fdf07-de79-4027-d8f6-4a1ef41f881c"
      },
      "source": [
        "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
        "from textblob import TextBlob\n",
        "training_corpus = [\n",
        "                   ('I am exhausted of this work.', 'Class_B'),\n",
        "                   (\"I can't cooperate with this\", 'Class_B'),\n",
        "                   ('He is my badest enemy!', 'Class_B'),\n",
        "                   ('My management is poor.', 'Class_B'),\n",
        "                   ('I love this burger.', 'Class_A'),\n",
        "                   ('This is an brilliant place!', 'Class_A'),\n",
        "                   ('I feel very good about these dates.', 'Class_A'),\n",
        "                   ('This is my best work.', 'Class_A'),\n",
        "                   (\"What an awesome view\", 'Class_A'),\n",
        "                   ('I do not like this dish', 'Class_B')]\n",
        "test_corpus = [\n",
        "                (\"I am not feeling well today.\", 'Class_B'), \n",
        "                (\"I feel brilliant!\", 'Class_A'), \n",
        "                ('Gary is a friend of mine.', 'Class_A'), \n",
        "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
        "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
        "\n",
        "model = NBC(training_corpus) \n",
        "print(model.classify(\"Their codes are amazing.\"))\n",
        "print(model.classify(\"I don't like their computer.\"))\n",
        "print(model.accuracy(test_corpus))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class_A\n",
            "Class_B\n",
            "0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtECOHQ4UOeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def levenshtein(s1,s2): \n",
        "    if len(s1) > len(s2):\n",
        "        s1,s2 = s2,s1 \n",
        "    distances = range(len(s1) + 1) \n",
        "    for index2,char2 in enumerate(s2):\n",
        "        newDistances = [index2+1]\n",
        "        for index1,char1 in enumerate(s1):\n",
        "            if char1 == char2:\n",
        "                newDistances.append(distances[index1]) \n",
        "            else:\n",
        "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
        "        distances = newDistances \n",
        "    return distances[-1]\n",
        "\n",
        "print(levenshtein(\"analyze\",\"analyse\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fex__wPvUOec",
        "colab_type": "code",
        "colab": {},
        "outputId": "476ec46e-bf2c-437b-f321-5b645980d29d"
      },
      "source": [
        "import fuzzy \n",
        "soundex = fuzzy.Soundex(4) \n",
        "print (soundex('ankit'))\n",
        "print (soundex('aunkit'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A523\n",
            "A523\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeZ6_jUrUOeg",
        "colab_type": "code",
        "colab": {},
        "outputId": "a1087805-d736-416f-d44f-1fa9bb17030c"
      },
      "source": [
        "import math\n",
        "from collections import Counter\n",
        "def get_cosine(vec1, vec2):\n",
        "    common = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in common])\n",
        "\n",
        "    sum1 = sum([vec1[x]**2 for x in vec1.keys()]) \n",
        "    sum2 = sum([vec2[x]**2 for x in vec2.keys()]) \n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "   \n",
        "    if not denominator:\n",
        "        return 0.0 \n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "def text_to_vector(text): \n",
        "    words = text.split() \n",
        "    return Counter(words)\n",
        "\n",
        "text1 = 'This is an article on analytics vidhya' \n",
        "text2 = 'article on analytics vidhya is about natural language processing'\n",
        "\n",
        "vector1 = text_to_vector(text1) \n",
        "vector2 = text_to_vector(text2) \n",
        "cosine = get_cosine(vector1, vector2)\n",
        "cosine"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.629940788348712"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAYmLXiVUOej",
        "colab_type": "code",
        "colab": {},
        "outputId": "152ee9d6-ae2b-4ca7-df71-9c8028d290a2"
      },
      "source": [
        "text1 = 'This is an article on analytics vidhya' \n",
        "text2 = 'This is an article on analytics vidhya'\n",
        "\n",
        "vector1 = text_to_vector(text1) \n",
        "vector2 = text_to_vector(text2) \n",
        "cosine = get_cosine(vector1, vector2)\n",
        "cosine"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999999999999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dEYDkgbUOem",
        "colab_type": "code",
        "colab": {},
        "outputId": "492ac6a1-8d19-4537-9774-3b018b89e07f"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import svm \n",
        "\n",
        "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
        "train_data = []\n",
        "train_labels = []\n",
        "for row in training_corpus:\n",
        "    train_data.append(row[0])\n",
        "    train_labels.append(row[1])\n",
        "\n",
        "test_data = [] \n",
        "test_labels = [] \n",
        "for row in test_corpus:\n",
        "    test_data.append(row[0]) \n",
        "    test_labels.append(row[1])\n",
        "\n",
        "# Create feature vectors \n",
        "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
        "# Train the feature vectors\n",
        "train_vectors = vectorizer.fit_transform(train_data)\n",
        "# Apply model on test data \n",
        "test_vectors = vectorizer.transform(test_data)\n",
        "\n",
        "# Perform classification with SVM, kernel=linear \n",
        "model = svm.SVC(kernel='linear') \n",
        "model.fit(train_vectors, train_labels) \n",
        "prediction = model.predict(test_vectors)\n",
        "print (classification_report(test_labels, prediction))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class_A       0.50      0.67      0.57         3\n",
            "     Class_B       0.50      0.33      0.40         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xB2dMuGbUOeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}