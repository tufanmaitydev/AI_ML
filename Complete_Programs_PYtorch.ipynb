{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Complete Programs PYtorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tufanmaitydev/AI_ML/blob/master/Complete_Programs_PYtorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5ps5TxjcHVn5"
      },
      "source": [
        "First Basic Program \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fmGSwhFzHajo",
        "outputId": "184b9ef1-0176-43a8-9093-338c20454673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3846
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "\n",
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "\n",
        "w_list = []\n",
        "mse_list = []\n",
        "\n",
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    print(\"MSE=\", l_sum / 3)\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / 3)\n",
        "\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njJrkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5bed74e550>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4J_nifyLMwRk",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2eBkQJFcM7N8"
      },
      "source": [
        "2. Manual Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xiKh-zEWNBPr",
        "outputId": "c3a5601f-d4a3-4817-a213-568bc0790169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = 1.0  # a random guess: random value\n",
        "\n",
        "# our model forward pass\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "\n",
        "# compute gradient\n",
        "def gradient(x, y):  # d_loss/d_w\n",
        "    return 2 * x * (x * w - y)\n",
        "\n",
        "# Before training\n",
        "print(\"predict (before training)\",  4, forward(4))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        grad = gradient(x_val, y_val)\n",
        "        w = w - 0.01 * grad\n",
        "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
        "        l = loss(x_val, y_val)\n",
        "\n",
        "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
        "\n",
        "# After training\n",
        "print(\"predict (after training)\",  \"4 hours\", forward(4))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 2.69\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1.47\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 0.8\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 0.24\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 0.02\n",
            "predict (after training) 4 hours 7.804863933862125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o7NjtOcSNN8U",
        "outputId": "ab22c9e8-926a-4a46-b65e-1f8b9f7196e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x60bce000 @  0x7f7997e122a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2H_G-UBNxgk",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R2BcPUI3NpQ8"
      },
      "source": [
        "3.Auto Gradient\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0BOexa82NsvT",
        "outputId": "225080f5-e982-4bb1-c806-a0412b0c8fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]\n",
        "\n",
        "w = Variable(torch.Tensor([1.0]),  requires_grad=True)  # Any random value\n",
        "\n",
        "# our model forward pass\n",
        "\n",
        "\n",
        "def forward(x):\n",
        "    return x * w\n",
        "\n",
        "# Loss function\n",
        "\n",
        "\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)\n",
        "\n",
        "# Before training\n",
        "print(\"predict (before training)\",  4, forward(4).data[0])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        l = loss(x_val, y_val)\n",
        "        l.backward()\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
        "        w.data = w.data - 0.01 * w.grad.data\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(\"progress:\", epoch, l.data[0])\n",
        "\n",
        "# After training\n",
        "print(\"predict (after training)\",  4, forward(4).data[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) 4 tensor(4.)\n",
            "\tgrad:  1.0 2.0 tensor(-2.)\n",
            "\tgrad:  2.0 4.0 tensor(-7.8400)\n",
            "\tgrad:  3.0 6.0 tensor(-16.2288)\n",
            "progress: 0 tensor(7.3159)\n",
            "\tgrad:  1.0 2.0 tensor(-1.4786)\n",
            "\tgrad:  2.0 4.0 tensor(-5.7962)\n",
            "\tgrad:  3.0 6.0 tensor(-11.9981)\n",
            "progress: 1 tensor(3.9988)\n",
            "\tgrad:  1.0 2.0 tensor(-1.0932)\n",
            "\tgrad:  2.0 4.0 tensor(-4.2852)\n",
            "\tgrad:  3.0 6.0 tensor(-8.8704)\n",
            "progress: 2 tensor(2.1857)\n",
            "\tgrad:  1.0 2.0 tensor(-0.8082)\n",
            "\tgrad:  2.0 4.0 tensor(-3.1681)\n",
            "\tgrad:  3.0 6.0 tensor(-6.5580)\n",
            "progress: 3 tensor(1.1946)\n",
            "\tgrad:  1.0 2.0 tensor(-0.5975)\n",
            "\tgrad:  2.0 4.0 tensor(-2.3422)\n",
            "\tgrad:  3.0 6.0 tensor(-4.8484)\n",
            "progress: 4 tensor(0.6530)\n",
            "\tgrad:  1.0 2.0 tensor(-0.4417)\n",
            "\tgrad:  2.0 4.0 tensor(-1.7316)\n",
            "\tgrad:  3.0 6.0 tensor(-3.5845)\n",
            "progress: 5 tensor(0.3569)\n",
            "\tgrad:  1.0 2.0 tensor(-0.3266)\n",
            "\tgrad:  2.0 4.0 tensor(-1.2802)\n",
            "\tgrad:  3.0 6.0 tensor(-2.6500)\n",
            "progress: 6 tensor(0.1951)\n",
            "\tgrad:  1.0 2.0 tensor(-0.2414)\n",
            "\tgrad:  2.0 4.0 tensor(-0.9465)\n",
            "\tgrad:  3.0 6.0 tensor(-1.9592)\n",
            "progress: 7 tensor(0.1066)\n",
            "\tgrad:  1.0 2.0 tensor(-0.1785)\n",
            "\tgrad:  2.0 4.0 tensor(-0.6997)\n",
            "\tgrad:  3.0 6.0 tensor(-1.4485)\n",
            "progress: 8 tensor(0.0583)\n",
            "\tgrad:  1.0 2.0 tensor(-0.1320)\n",
            "\tgrad:  2.0 4.0 tensor(-0.5173)\n",
            "\tgrad:  3.0 6.0 tensor(-1.0709)\n",
            "progress: 9 tensor(0.0319)\n",
            "predict (after training) 4 tensor(7.8049)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_QEeV_2fOPyD",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0dUD9-rjOdAB"
      },
      "source": [
        "Linear Regresion in Pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1nEMP2D2Oh4D",
        "outputId": "32cef48a-990d-4f9c-e330-9efd7f333a21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0]]))\n",
        "y_data = Variable(torch.Tensor([[2.0], [4.0], [6.0]]))\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(size_average=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(500):\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "# After training\n",
        "hour_var = Variable(torch.Tensor([[4.0]]))\n",
        "y_pred = model(hour_var)\n",
        "print(\"predict (after training)\",  4, model(hour_var).data[0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (after training) 4 tensor(7.9827)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUuSwVzkOpeT",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYq8uwC3OxDj"
      },
      "source": [
        "Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n6Wp1h1yOlYT",
        "outputId": "49fbaa8d-041b-42b0-935c-1edfcdd42470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x_data = Variable(torch.Tensor([[1.0], [2.0], [3.0], [4.0]]))\n",
        "y_data = Variable(torch.Tensor([[0.], [0.], [1.], [1.]]))\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data.\n",
        "        \"\"\"\n",
        "        y_pred = F.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "# our model\n",
        "model = Model()\n",
        "\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1000):\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    \n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# After training\n",
        "hour_var = Variable(torch.Tensor([[1.0]]))\n",
        "print(\"predict 1 hour \", 1.0, model(hour_var).data[0][0] > 0.5)\n",
        "hour_var = Variable(torch.Tensor([[7.0]]))\n",
        "print(\"predict 7 hours\", 7.0, model(hour_var).data[0][0] > 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predict 1 hour  1.0 tensor(0, dtype=torch.uint8)\n",
            "predict 7 hours 7.0 tensor(1, dtype=torch.uint8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pZ1JxZqmPa8b",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F0ivYocoQHn7"
      },
      "source": [
        "Data loder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6mqHAydQI7J",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PpSTjCCtQRqi",
        "colab": {}
      },
      "source": [
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('diabetes.csv.gz',\n",
        "                        delimiter=',', dtype=np.float32)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PuxNM3leQv9q",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "utwfyc_TQc1x",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "236BwdAXPZSh",
        "colab": {}
      },
      "source": [
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31LmP_i6QwvB",
        "outputId": "01c8ee6b-1cb8-4065-a26e-a68121ef4ceb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50813
        }
      },
      "source": [
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        # Run your training process\n",
        "        print(epoch, i, \"inputs\", inputs.data, \"labels\", labels.data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 inputs tensor([[-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n",
            "        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n",
            "        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n",
            "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
            "        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n",
            "        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n",
            "        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n",
            "        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n",
            "        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n",
            "        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n",
            "        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n",
            "        [-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n",
            "        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n",
            "        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n",
            "        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n",
            "        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n",
            "        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n",
            "        [ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n",
            "        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n",
            "        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n",
            "        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n",
            "        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n",
            "        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000],\n",
            "        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n",
            "        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n",
            "        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n",
            "        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n",
            "        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n",
            "        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n",
            "        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n",
            "        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 1 inputs tensor([[-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n",
            "        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n",
            "        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n",
            "        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n",
            "        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n",
            "        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n",
            "        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n",
            "        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n",
            "        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n",
            "        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n",
            "        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n",
            "        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n",
            "        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n",
            "        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n",
            "        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n",
            "        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n",
            "        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n",
            "        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n",
            "        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n",
            "        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n",
            "        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n",
            "        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n",
            "        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n",
            "        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n",
            "        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000],\n",
            "        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n",
            "        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n",
            "        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n",
            "        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n",
            "        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n",
            "        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n",
            "        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 2 inputs tensor([[-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n",
            "        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n",
            "        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n",
            "        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n",
            "        [-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n",
            "        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n",
            "        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n",
            "        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n",
            "        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n",
            "        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n",
            "        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n",
            "        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n",
            "        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n",
            "        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n",
            "        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n",
            "        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n",
            "        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n",
            "        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n",
            "        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n",
            "        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n",
            "        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n",
            "        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n",
            "        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n",
            "        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n",
            "        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n",
            "        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n",
            "        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n",
            "        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n",
            "        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n",
            "        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667],\n",
            "        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n",
            "        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "0 3 inputs tensor([[ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n",
            "        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n",
            "        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n",
            "        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n",
            "        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n",
            "        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n",
            "        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n",
            "        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n",
            "        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333],\n",
            "        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n",
            "        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n",
            "        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667],\n",
            "        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n",
            "        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n",
            "        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n",
            "        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n",
            "        [-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n",
            "        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n",
            "        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n",
            "        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000],\n",
            "        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n",
            "        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n",
            "        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n",
            "        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n",
            "        [-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n",
            "        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n",
            "        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n",
            "        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n",
            "        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n",
            "        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n",
            "        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "0 4 inputs tensor([[-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n",
            "        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n",
            "        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n",
            "        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n",
            "        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n",
            "        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n",
            "        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n",
            "        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n",
            "        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n",
            "        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n",
            "        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667],\n",
            "        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n",
            "        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n",
            "        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n",
            "        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n",
            "        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n",
            "        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n",
            "        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n",
            "        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n",
            "        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n",
            "        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n",
            "        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n",
            "        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n",
            "        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n",
            "        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n",
            "        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n",
            "        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n",
            "        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n",
            "        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n",
            "        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n",
            "        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "0 5 inputs tensor([[-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n",
            "        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n",
            "        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n",
            "        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n",
            "        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n",
            "        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n",
            "        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n",
            "        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n",
            "        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n",
            "        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n",
            "        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n",
            "        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n",
            "        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n",
            "        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n",
            "        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n",
            "        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n",
            "        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n",
            "        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n",
            "        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n",
            "        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n",
            "        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n",
            "        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n",
            "        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n",
            "        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n",
            "        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667],\n",
            "        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n",
            "        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n",
            "        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n",
            "        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n",
            "        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n",
            "        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "0 6 inputs tensor([[ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n",
            "        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n",
            "        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n",
            "        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n",
            "        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n",
            "        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n",
            "        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n",
            "        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n",
            "        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n",
            "        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n",
            "        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n",
            "        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n",
            "        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n",
            "        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n",
            "        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667],\n",
            "        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n",
            "        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n",
            "        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n",
            "        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n",
            "        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n",
            "        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n",
            "        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n",
            "        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n",
            "        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n",
            "        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n",
            "        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n",
            "        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n",
            "        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n",
            "        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n",
            "        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n",
            "        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n",
            "        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 7 inputs tensor([[-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n",
            "        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n",
            "        [-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n",
            "        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n",
            "        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n",
            "        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n",
            "        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n",
            "        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n",
            "        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n",
            "        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n",
            "        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n",
            "        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n",
            "        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n",
            "        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n",
            "        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n",
            "        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n",
            "        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n",
            "        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n",
            "        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n",
            "        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n",
            "        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000],\n",
            "        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n",
            "        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n",
            "        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n",
            "        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n",
            "        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n",
            "        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n",
            "        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n",
            "        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
            "        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 8 inputs tensor([[-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n",
            "        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n",
            "        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n",
            "        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n",
            "        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n",
            "        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n",
            "        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n",
            "        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n",
            "        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n",
            "        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n",
            "        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n",
            "        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n",
            "        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n",
            "        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n",
            "        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n",
            "        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n",
            "        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n",
            "        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n",
            "        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n",
            "        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n",
            "        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n",
            "        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n",
            "        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n",
            "        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n",
            "        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n",
            "        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n",
            "        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n",
            "        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n",
            "        [-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n",
            "        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n",
            "        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 9 inputs tensor([[-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n",
            "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
            "        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n",
            "        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n",
            "        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n",
            "        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n",
            "        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n",
            "        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n",
            "        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n",
            "        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n",
            "        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n",
            "        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n",
            "        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n",
            "        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n",
            "        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n",
            "        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n",
            "        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n",
            "        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n",
            "        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n",
            "        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n",
            "        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n",
            "        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n",
            "        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n",
            "        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n",
            "        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n",
            "        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n",
            "        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n",
            "        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n",
            "        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n",
            "        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n",
            "        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "0 10 inputs tensor([[-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n",
            "        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n",
            "        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n",
            "        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n",
            "        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000],\n",
            "        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n",
            "        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n",
            "        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n",
            "        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n",
            "        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n",
            "        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n",
            "        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n",
            "        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n",
            "        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n",
            "        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n",
            "        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n",
            "        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667],\n",
            "        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333],\n",
            "        [-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n",
            "        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000],\n",
            "        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000],\n",
            "        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n",
            "        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n",
            "        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n",
            "        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n",
            "        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n",
            "        [-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n",
            "        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n",
            "        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n",
            "        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n",
            "        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 11 inputs tensor([[ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n",
            "        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n",
            "        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n",
            "        [ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n",
            "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n",
            "        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n",
            "        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n",
            "        [ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n",
            "        [-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n",
            "        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n",
            "        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n",
            "        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n",
            "        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n",
            "        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n",
            "        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n",
            "        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n",
            "        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n",
            "        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n",
            "        [-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n",
            "        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n",
            "        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n",
            "        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n",
            "        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333],\n",
            "        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n",
            "        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n",
            "        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n",
            "        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n",
            "        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n",
            "        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n",
            "        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "0 12 inputs tensor([[ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n",
            "        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n",
            "        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n",
            "        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n",
            "        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n",
            "        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n",
            "        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n",
            "        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n",
            "        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n",
            "        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n",
            "        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n",
            "        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n",
            "        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n",
            "        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n",
            "        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n",
            "        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n",
            "        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n",
            "        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n",
            "        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n",
            "        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n",
            "        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n",
            "        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n",
            "        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n",
            "        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n",
            "        [-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n",
            "        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n",
            "        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n",
            "        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n",
            "        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n",
            "        [-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "0 13 inputs tensor([[-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n",
            "        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
            "        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n",
            "        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n",
            "        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n",
            "        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n",
            "        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n",
            "        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n",
            "        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n",
            "        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n",
            "        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n",
            "        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n",
            "        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n",
            "        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n",
            "        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n",
            "        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n",
            "        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n",
            "        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n",
            "        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n",
            "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
            "        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n",
            "        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333],\n",
            "        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n",
            "        [-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n",
            "        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n",
            "        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n",
            "        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n",
            "        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n",
            "        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n",
            "        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n",
            "        [ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n",
            "        [-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 14 inputs tensor([[-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n",
            "        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n",
            "        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n",
            "        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n",
            "        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n",
            "        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n",
            "        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n",
            "        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n",
            "        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n",
            "        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n",
            "        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n",
            "        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n",
            "        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n",
            "        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n",
            "        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n",
            "        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n",
            "        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n",
            "        [-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n",
            "        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n",
            "        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n",
            "        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n",
            "        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667],\n",
            "        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n",
            "        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n",
            "        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333],\n",
            "        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n",
            "        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n",
            "        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n",
            "        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n",
            "        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n",
            "        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 15 inputs tensor([[-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n",
            "        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n",
            "        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n",
            "        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n",
            "        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n",
            "        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n",
            "        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n",
            "        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n",
            "        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n",
            "        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n",
            "        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n",
            "        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n",
            "        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n",
            "        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n",
            "        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n",
            "        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n",
            "        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n",
            "        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n",
            "        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n",
            "        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n",
            "        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n",
            "        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n",
            "        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n",
            "        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n",
            "        [-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n",
            "        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n",
            "        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n",
            "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n",
            "        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n",
            "        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n",
            "        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 16 inputs tensor([[-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n",
            "        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n",
            "        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n",
            "        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n",
            "        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n",
            "        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n",
            "        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n",
            "        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n",
            "        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n",
            "        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n",
            "        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n",
            "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n",
            "        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n",
            "        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n",
            "        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n",
            "        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n",
            "        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n",
            "        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n",
            "        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n",
            "        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n",
            "        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n",
            "        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n",
            "        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n",
            "        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n",
            "        [ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n",
            "        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n",
            "        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n",
            "        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n",
            "        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n",
            "        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n",
            "        [-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n",
            "        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "0 17 inputs tensor([[-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n",
            "        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n",
            "        [ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n",
            "        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000],\n",
            "        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n",
            "        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n",
            "        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n",
            "        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n",
            "        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n",
            "        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n",
            "        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n",
            "        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n",
            "        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n",
            "        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n",
            "        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n",
            "        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n",
            "        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n",
            "        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n",
            "        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n",
            "        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n",
            "        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n",
            "        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n",
            "        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n",
            "        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n",
            "        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n",
            "        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n",
            "        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n",
            "        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n",
            "        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n",
            "        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n",
            "        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n",
            "        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "0 18 inputs tensor([[ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n",
            "        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n",
            "        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n",
            "        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n",
            "        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n",
            "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
            "        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n",
            "        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n",
            "        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n",
            "        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n",
            "        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n",
            "        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n",
            "        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n",
            "        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n",
            "        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n",
            "        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n",
            "        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n",
            "        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n",
            "        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n",
            "        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n",
            "        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n",
            "        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n",
            "        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n",
            "        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n",
            "        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n",
            "        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n",
            "        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n",
            "        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n",
            "        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n",
            "        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667],\n",
            "        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n",
            "        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "0 19 inputs tensor([[-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n",
            "        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n",
            "        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n",
            "        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n",
            "        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n",
            "        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n",
            "        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n",
            "        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n",
            "        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n",
            "        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n",
            "        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n",
            "        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n",
            "        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n",
            "        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n",
            "        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n",
            "        [-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n",
            "        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n",
            "        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n",
            "        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667],\n",
            "        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n",
            "        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n",
            "        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n",
            "        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n",
            "        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n",
            "        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n",
            "        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n",
            "        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n",
            "        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n",
            "        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n",
            "        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n",
            "        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n",
            "        [-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 20 inputs tensor([[-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n",
            "        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n",
            "        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n",
            "        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n",
            "        [-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n",
            "        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n",
            "        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667],\n",
            "        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n",
            "        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n",
            "        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n",
            "        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n",
            "        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333],\n",
            "        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n",
            "        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n",
            "        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n",
            "        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n",
            "        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n",
            "        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n",
            "        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n",
            "        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333],\n",
            "        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n",
            "        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n",
            "        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n",
            "        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n",
            "        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n",
            "        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n",
            "        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n",
            "        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n",
            "        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n",
            "        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n",
            "        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n",
            "        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "0 21 inputs tensor([[-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n",
            "        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n",
            "        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n",
            "        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n",
            "        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n",
            "        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n",
            "        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n",
            "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
            "        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n",
            "        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n",
            "        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n",
            "        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n",
            "        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n",
            "        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n",
            "        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n",
            "        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n",
            "        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n",
            "        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n",
            "        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n",
            "        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n",
            "        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n",
            "        [-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n",
            "        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n",
            "        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n",
            "        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n",
            "        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n",
            "        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n",
            "        [-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n",
            "        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n",
            "        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n",
            "        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "0 22 inputs tensor([[-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n",
            "        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n",
            "        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n",
            "        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n",
            "        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n",
            "        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n",
            "        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n",
            "        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n",
            "        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n",
            "        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n",
            "        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n",
            "        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n",
            "        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n",
            "        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n",
            "        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n",
            "        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n",
            "        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n",
            "        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n",
            "        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n",
            "        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n",
            "        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n",
            "        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n",
            "        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n",
            "        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n",
            "        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n",
            "        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n",
            "        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n",
            "        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n",
            "        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n",
            "        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n",
            "        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n",
            "        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "0 23 inputs tensor([[-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n",
            "        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n",
            "        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n",
            "        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n",
            "        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000],\n",
            "        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n",
            "        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n",
            "        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n",
            "        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n",
            "        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n",
            "        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n",
            "        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n",
            "        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n",
            "        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n",
            "        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n",
            "        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n",
            "        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n",
            "        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n",
            "        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n",
            "        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n",
            "        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n",
            "        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n",
            "        [-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 0 inputs tensor([[-0.1765,  0.0754,  0.2131,  0.0000,  0.0000, -0.1177, -0.8497, -0.6667],\n",
            "        [-0.7647, -0.0553,  0.1148, -0.6364, -0.8203, -0.2250, -0.5875,  0.0000],\n",
            "        [ 0.0000,  0.0452,  0.0492, -0.2525, -0.8487,  0.0015, -0.6311, -0.9667],\n",
            "        [ 0.7647,  0.3668,  0.1475, -0.3535, -0.7400,  0.1058, -0.9360, -0.2667],\n",
            "        [ 0.5294,  0.0653,  0.1475,  0.0000,  0.0000,  0.0194, -0.8523,  0.0333],\n",
            "        [-0.7647,  0.9799,  0.1475,  1.0000,  0.0000,  0.0343, -0.5756,  0.3667],\n",
            "        [-0.7647,  0.2060, -0.1148,  0.0000,  0.0000, -0.2012, -0.6781, -0.8000],\n",
            "        [-0.6471,  0.1156, -0.0492, -0.3737, -0.8960, -0.1207, -0.6994, -0.9667],\n",
            "        [-0.6471,  0.2965,  0.0492, -0.4141, -0.7281, -0.2131, -0.8796, -0.7667],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.1919,  0.0000,  0.1356, -0.8804, -0.7000],\n",
            "        [ 0.1765,  0.2261,  0.2787, -0.3737,  0.0000, -0.1773, -0.6294, -0.2000],\n",
            "        [-0.7647,  0.1256,  0.2787,  0.0101, -0.6690,  0.1744, -0.9172, -0.9000],\n",
            "        [-0.7647,  0.4673,  0.1475, -0.2323, -0.1489, -0.1654, -0.7788, -0.7333],\n",
            "        [-0.8824,  0.4673, -0.0820,  0.0000,  0.0000, -0.1148, -0.5850, -0.7333],\n",
            "        [-0.7647,  0.3970,  0.2295,  0.0000,  0.0000, -0.2370, -0.9240, -0.7333],\n",
            "        [-0.8824,  0.1759, -0.0164, -0.5354, -0.7494,  0.0075, -0.6687, -0.8000],\n",
            "        [ 0.0000,  0.1759,  0.0820, -0.3737, -0.5556, -0.0820, -0.6456, -0.9667],\n",
            "        [-0.2941,  0.0251,  0.4754, -0.2121,  0.0000,  0.0641, -0.4910, -0.7667],\n",
            "        [-0.2941, -0.1256,  0.3115,  0.0000,  0.0000, -0.3085, -0.9949, -0.6333],\n",
            "        [ 0.1765,  0.7990,  0.1475,  0.0000,  0.0000,  0.0462, -0.8958, -0.4667],\n",
            "        [-0.2941, -0.1457,  0.2787,  0.0000,  0.0000, -0.0700, -0.7404, -0.3000],\n",
            "        [ 0.0000,  0.0251,  0.2787, -0.1919, -0.7872,  0.0283, -0.8634, -0.9000],\n",
            "        [-0.5294,  0.4573,  0.3443, -0.6364,  0.0000, -0.0313, -0.8659,  0.6333],\n",
            "        [ 0.0000, -0.0151,  0.3443, -0.6970, -0.8014, -0.2489, -0.8113, -0.9667],\n",
            "        [-0.1765, -0.0553,  0.0492, -0.4949, -0.8132, -0.0075, -0.4364, -0.3333],\n",
            "        [-0.5294,  0.1759,  0.0492, -0.4545, -0.7163, -0.0104, -0.8702, -0.9000],\n",
            "        [-0.6471,  0.4271,  0.3115, -0.6970,  0.0000, -0.0343, -0.8958,  0.4000],\n",
            "        [-0.8824, -0.2864,  0.0164,  0.0000,  0.0000, -0.3502, -0.7114, -0.8333],\n",
            "        [-0.1765, -0.1859,  0.2787, -0.1919, -0.8865,  0.3920, -0.8437, -0.3000],\n",
            "        [-0.8824, -0.1357,  0.0820,  0.0505, -0.8463,  0.2310, -0.2835, -0.7333],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.3535, -0.8676, -0.2489, -0.9573,  0.0000],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.4141, -0.5366, -0.0462, -0.6874, -0.3000]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 1 inputs tensor([[-0.8824,  0.5377,  0.3443, -0.1515,  0.1466,  0.2101, -0.4799, -0.9333],\n",
            "        [-0.7647, -0.1759, -0.1475, -0.5556, -0.7281, -0.1505,  0.3843, -0.8667],\n",
            "        [-0.1765, -0.1658,  0.2787, -0.4747, -0.8322, -0.1267, -0.4116, -0.5000],\n",
            "        [-0.4118,  0.0854,  0.1803, -0.1313, -0.8227,  0.0760, -0.8420, -0.6000],\n",
            "        [-0.6471, -0.1055,  0.2131, -0.6768, -0.7991, -0.0939, -0.5961, -0.4333],\n",
            "        [-0.8824, -0.1156, -0.5082, -0.1515, -0.7660,  0.6393, -0.6430, -0.8333],\n",
            "        [-0.2941,  0.0251,  0.3443,  0.0000,  0.0000, -0.0820, -0.9129, -0.5000],\n",
            "        [ 1.0000,  0.6382,  0.1803, -0.1717, -0.7305,  0.2191, -0.3689, -0.1333],\n",
            "        [ 0.0000,  0.0050,  0.4426,  0.2121, -0.7400,  0.3949, -0.2451, -0.6667],\n",
            "        [-0.7647,  0.4271,  0.3443, -0.6364, -0.8487, -0.2638, -0.4167,  0.0000],\n",
            "        [ 0.0588,  0.7085,  0.2131, -0.3737,  0.0000,  0.3115, -0.7225, -0.2667],\n",
            "        [-0.5294, -0.0452, -0.0164, -0.3535,  0.0000,  0.0551, -0.8241, -0.7667],\n",
            "        [ 0.0588,  0.1960,  0.3115, -0.2929,  0.0000, -0.1356, -0.8420, -0.7333],\n",
            "        [-0.8824,  0.7286,  0.1148, -0.0101,  0.3688,  0.2638, -0.4671, -0.7667],\n",
            "        [-0.5294,  0.4874, -0.0164, -0.4545, -0.2482, -0.0790, -0.9385, -0.7333],\n",
            "        [ 0.0588,  0.3065,  0.1475,  0.0000,  0.0000,  0.0194, -0.5098, -0.2000],\n",
            "        [-0.6471,  0.3065,  0.2787, -0.5354, -0.8132, -0.1535, -0.7908, -0.5667],\n",
            "        [-0.7647, -0.0151, -0.0164, -0.6566, -0.7163,  0.0343, -0.8975, -0.9667],\n",
            "        [ 0.1765,  0.3970,  0.3115,  0.0000,  0.0000, -0.1922,  0.1640,  0.2000],\n",
            "        [-0.2941, -0.1960,  0.3115, -0.2727,  0.0000,  0.1863, -0.9155, -0.7667],\n",
            "        [-0.5294,  0.1658,  0.1803, -0.7576, -0.7943, -0.3413, -0.6712, -0.4667],\n",
            "        [ 0.0588,  0.5678,  0.4098,  0.0000,  0.0000, -0.2608, -0.8702,  0.0667],\n",
            "        [-0.6471,  0.2864,  0.1803, -0.4949, -0.5508, -0.0343, -0.5978, -0.8000],\n",
            "        [ 0.0000,  0.0251,  0.0492, -0.0707, -0.8156,  0.2101, -0.6430,  0.0000],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.3535, -0.5863,  0.0194, -0.8446, -0.5000],\n",
            "        [-0.6471,  0.5075,  0.2459,  0.0000,  0.0000, -0.3741, -0.8898, -0.4667],\n",
            "        [-0.0588,  0.8894,  0.2787,  0.0000,  0.0000,  0.4277, -0.9496, -0.2667],\n",
            "        [-0.4118, -0.2261,  0.3443, -0.1717, -0.9007,  0.0671, -0.9334, -0.5333],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
            "        [ 0.2941,  0.0352,  0.1148, -0.1919,  0.0000,  0.3770, -0.9590, -0.3000],\n",
            "        [ 0.1765,  0.2965,  0.0164, -0.2727,  0.0000,  0.2280, -0.6900, -0.4333],\n",
            "        [ 0.5294,  0.0452,  0.1803,  0.0000,  0.0000, -0.0700, -0.6695, -0.4333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "1 2 inputs tensor([[-0.2941,  0.9598,  0.1475,  0.0000,  0.0000, -0.0790, -0.7865, -0.6667],\n",
            "        [-0.7647,  0.0854, -0.1475, -0.4747, -0.8511, -0.0313, -0.7950, -0.9667],\n",
            "        [-0.4118,  0.2462,  0.2131,  0.0000,  0.0000,  0.0134, -0.8787, -0.4333],\n",
            "        [-0.6471,  0.2563, -0.0492,  0.0000,  0.0000, -0.0581, -0.9377, -0.9000],\n",
            "        [-0.1765,  0.9598,  0.1475, -0.3333, -0.6572, -0.2519, -0.9274,  0.1333],\n",
            "        [ 0.1765,  0.0151,  0.4098, -0.2525,  0.0000,  0.3592, -0.0965, -0.4333],\n",
            "        [-0.7647,  0.7487,  0.4426, -0.2525, -0.7163,  0.3264, -0.5149, -0.9000],\n",
            "        [-0.1765,  0.7889,  0.3770,  0.0000,  0.0000,  0.1893, -0.7839, -0.3333],\n",
            "        [ 0.2941,  0.3668,  0.3770, -0.2929, -0.6927, -0.1565, -0.8446, -0.3000],\n",
            "        [-0.7647,  0.4673,  0.2459, -0.2929, -0.5414,  0.1386, -0.7857, -0.7333],\n",
            "        [-0.6471,  0.6382,  0.1475, -0.6364, -0.7518, -0.0581, -0.8377, -0.7667],\n",
            "        [-0.8824,  0.6382,  0.1803,  0.0000,  0.0000,  0.1624, -0.0231, -0.6000],\n",
            "        [-0.6471,  0.0653, -0.1148, -0.5758, -0.6265, -0.0790, -0.8173, -0.9000],\n",
            "        [-0.5294, -0.0754,  0.3115,  0.0000,  0.0000,  0.2578, -0.8642, -0.7333],\n",
            "        [ 0.0000,  0.0251,  0.2295, -0.5354,  0.0000,  0.0000, -0.5781,  0.0000],\n",
            "        [ 0.0000,  0.0553,  0.4754,  0.0000,  0.0000, -0.1177, -0.8984, -0.1667],\n",
            "        [-0.4118,  0.4372,  0.2787,  0.0000,  0.0000,  0.3413, -0.9044, -0.1333],\n",
            "        [-0.0588,  0.0553,  0.6393, -0.2727,  0.0000,  0.2906, -0.8625, -0.2000],\n",
            "        [-0.2941, -0.0352,  0.0000,  0.0000,  0.0000, -0.2936, -0.9044, -0.7667],\n",
            "        [-0.8824,  0.4372,  0.2131, -0.5556, -0.8558, -0.2191, -0.8480,  0.0000],\n",
            "        [-0.5294,  0.4271,  0.4098,  0.0000,  0.0000,  0.3115, -0.5158, -0.9667],\n",
            "        [-0.6471, -0.3869,  0.3443, -0.4343,  0.0000,  0.0253, -0.8591, -0.1667],\n",
            "        [-0.1765,  0.0653,  0.5082, -0.6364,  0.0000, -0.3234, -0.8659, -0.1000],\n",
            "        [-0.4118,  0.1055,  0.1148,  0.0000,  0.0000, -0.2250, -0.8173, -0.7000],\n",
            "        [ 0.0000, -0.0653,  0.6393, -0.2121, -0.8298,  0.2936, -0.1947, -0.5333],\n",
            "        [-0.2941,  0.2965,  0.4754, -0.8586, -0.2293, -0.4158, -0.5696,  0.3000],\n",
            "        [ 0.0000,  0.1357,  0.3115, -0.6768,  0.0000, -0.0760, -0.3202,  0.0000],\n",
            "        [-0.4118,  0.1156,  0.1803, -0.4343,  0.0000, -0.2876, -0.7190, -0.8000],\n",
            "        [-0.0588, -0.0854,  0.3443,  0.0000,  0.0000,  0.0611, -0.5653,  0.5667],\n",
            "        [-0.5294,  0.7186,  0.1803,  0.0000,  0.0000,  0.2996, -0.6576, -0.8333],\n",
            "        [-0.1765,  0.1457,  0.0820,  0.0000,  0.0000, -0.0224, -0.8463, -0.3000],\n",
            "        [-0.7647,  0.4171, -0.0492, -0.3131, -0.6974, -0.2429, -0.4697, -0.9000]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 3 inputs tensor([[-0.4118,  0.1558,  0.6066,  0.0000,  0.0000,  0.5768, -0.8881, -0.7667],\n",
            "        [-0.1765,  0.4271,  0.4754, -0.5152,  0.1348, -0.0939, -0.9573, -0.2667],\n",
            "        [ 0.4118, -0.0754,  0.0164, -0.8586, -0.3901, -0.1773, -0.2758, -0.2333],\n",
            "        [-0.2941,  0.2563,  0.1148, -0.3939, -0.7163, -0.1058, -0.6704, -0.6333],\n",
            "        [-0.4118,  0.0000,  0.3115, -0.3535,  0.0000,  0.2221, -0.7711, -0.4667],\n",
            "        [-0.8824,  0.1658,  0.1475, -0.4343,  0.0000, -0.1833, -0.8924,  0.0000],\n",
            "        [-0.7647,  0.2261,  0.1475, -0.4545,  0.0000,  0.0969, -0.7763, -0.8000],\n",
            "        [-0.1765,  0.1457,  0.2459, -0.6566, -0.7400, -0.2906, -0.6687, -0.6667],\n",
            "        [-0.1765,  0.5276,  0.4426, -0.1111,  0.0000,  0.4903, -0.7788, -0.5000],\n",
            "        [-0.8824, -0.1055,  0.2459, -0.3131, -0.9125, -0.0700, -0.9026, -0.9333],\n",
            "        [ 0.0000,  0.8090,  0.2787,  0.2727, -0.9669,  0.7705,  1.0000, -0.8667],\n",
            "        [-0.7647,  0.0050, -0.1148, -0.4343, -0.7518,  0.1267, -0.6413, -0.9000],\n",
            "        [-0.6471,  0.7085,  0.0492, -0.2525, -0.4681,  0.0283, -0.7626, -0.7000],\n",
            "        [ 0.1765,  0.3367,  0.1148,  0.0000,  0.0000, -0.1952, -0.8574, -0.5000],\n",
            "        [-0.1765,  0.6884,  0.4426, -0.1515, -0.2411,  0.1386, -0.3945, -0.3667],\n",
            "        [-0.6471,  0.4874,  0.0820, -0.4949,  0.0000, -0.0313, -0.8480, -0.9667],\n",
            "        [-0.7647,  0.1256,  0.0820, -0.5556,  0.0000, -0.2548, -0.8044, -0.9000],\n",
            "        [-0.6471,  0.2261,  0.2787,  0.0000,  0.0000, -0.3145, -0.8497, -0.3667],\n",
            "        [ 0.0000,  0.2663,  0.3770, -0.4141, -0.4917, -0.0849, -0.6225, -0.9000],\n",
            "        [-0.5294,  0.2764,  0.4426, -0.7778, -0.6336,  0.0283, -0.5559, -0.7667],\n",
            "        [-0.8824,  0.1357,  0.0492, -0.2929,  0.0000,  0.0015, -0.6029,  0.0000],\n",
            "        [-0.2941,  0.1558, -0.0164, -0.2121,  0.0000,  0.0045, -0.8574, -0.3667],\n",
            "        [ 0.0000,  0.6181, -0.1803,  0.0000,  0.0000, -0.3472, -0.8497,  0.4667],\n",
            "        [-0.8824,  0.3970,  0.0164, -0.1717,  0.1348,  0.2131, -0.6089,  0.0000],\n",
            "        [-0.7647,  0.1859,  0.3115,  0.0000,  0.0000,  0.2787, -0.4748,  0.0000],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.1919,  0.0000,  0.2310, -0.5482, -0.7667],\n",
            "        [ 0.0000,  0.1156,  0.0656,  0.0000,  0.0000, -0.2668, -0.5030, -0.6667],\n",
            "        [-0.8824, -0.0452,  0.0820, -0.7374, -0.9102, -0.4158, -0.7814, -0.8667],\n",
            "        [-0.6471, -0.1156, -0.0492, -0.7778, -0.8723, -0.2608, -0.8386, -0.9667],\n",
            "        [-0.7647, -0.1156, -0.0492, -0.4747, -0.9622, -0.1535, -0.4125, -0.9667],\n",
            "        [-0.5294, -0.0452,  0.0492,  0.0000,  0.0000, -0.0462, -0.9291, -0.6667],\n",
            "        [-0.7647, -0.1256, -0.0492, -0.6768, -0.8771, -0.0253, -0.9249, -0.8667]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 4 inputs tensor([[-0.0588,  0.1256,  0.1803,  0.0000,  0.0000, -0.2966, -0.3493,  0.2333],\n",
            "        [-0.5294,  0.1156,  0.1803, -0.0505, -0.5106,  0.1058,  0.1204,  0.1667],\n",
            "        [ 0.0000, -0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8480, -0.8667],\n",
            "        [-0.6471, -0.2161,  0.1475,  0.0000,  0.0000, -0.0313, -0.8360, -0.4000],\n",
            "        [-0.6471, -0.1658, -0.0492, -0.3737, -0.9574,  0.0224, -0.7797, -0.8667],\n",
            "        [ 0.0588,  0.6482,  0.2787,  0.0000,  0.0000, -0.0224, -0.9402, -0.2000],\n",
            "        [-0.5294,  0.5176,  0.4754, -0.2323,  0.0000, -0.1148, -0.8155, -0.5000],\n",
            "        [ 0.0000,  0.1859,  0.3770, -0.0505, -0.4563,  0.3651, -0.5961, -0.6667],\n",
            "        [-0.4118, -0.1457,  0.2131, -0.5556,  0.0000, -0.1356, -0.0213, -0.6333],\n",
            "        [ 0.0000,  0.2965,  0.8033, -0.0707, -0.6927,  1.0000, -0.7942, -0.8333],\n",
            "        [-0.5294,  0.3166,  0.1148, -0.5758, -0.6076, -0.0134, -0.9300, -0.7667],\n",
            "        [-0.5294,  0.1055,  0.2459, -0.5960, -0.7636, -0.1535, -0.9658, -0.8000],\n",
            "        [-0.6471, -0.1759,  0.1475,  0.0000,  0.0000, -0.3711, -0.7344, -0.8667],\n",
            "        [-0.4118,  0.0553,  0.1803, -0.4141, -0.2317,  0.0999, -0.9308, -0.7667],\n",
            "        [-0.7647,  0.2965,  0.3770,  0.0000,  0.0000, -0.1654, -0.8241, -0.8000],\n",
            "        [-0.0588,  0.2663,  0.4426, -0.2727, -0.7447,  0.1475, -0.7686, -0.0667],\n",
            "        [ 0.0000,  0.2965,  0.3115,  0.0000,  0.0000, -0.0700, -0.4663, -0.7333],\n",
            "        [ 0.0000, -0.2161,  0.4426, -0.4141, -0.9054,  0.0999, -0.6960,  0.0000],\n",
            "        [ 0.4118,  0.5176,  0.1475, -0.1919, -0.3593,  0.2459, -0.4330, -0.4333],\n",
            "        [ 0.0000,  0.3467, -0.0492, -0.5960, -0.3121, -0.2131, -0.7660,  0.0000],\n",
            "        [ 0.0000,  0.0452,  0.2459,  0.0000,  0.0000, -0.4516, -0.5696, -0.8000],\n",
            "        [-0.6471,  0.7487, -0.0492, -0.5556, -0.5414, -0.0194, -0.5602, -0.5000],\n",
            "        [-0.6471,  0.7387,  0.3770, -0.3333,  0.1206,  0.0641, -0.8463, -0.9667],\n",
            "        [ 0.0000, -0.1357,  0.1148, -0.3535,  0.0000,  0.0671, -0.8634, -0.8667],\n",
            "        [-0.7647, -0.1055,  0.4754, -0.3939,  0.0000, -0.0015, -0.8173, -0.3000],\n",
            "        [-0.5294,  0.2362,  0.3115, -0.6970, -0.5839, -0.0462, -0.6883, -0.5667],\n",
            "        [-0.4118,  0.4774,  0.2787,  0.0000,  0.0000,  0.0045, -0.8804,  0.4667],\n",
            "        [-0.7647,  0.2362, -0.2131, -0.3535, -0.6099,  0.2548, -0.6225, -0.8333],\n",
            "        [-0.6471,  0.1558,  0.0820, -0.2121, -0.6690,  0.1356, -0.9385, -0.7667],\n",
            "        [-0.1765,  0.3367,  0.3770,  0.0000,  0.0000,  0.1982, -0.4722, -0.4667],\n",
            "        [-0.7647, -0.1658,  0.0820, -0.5354, -0.8818, -0.0402, -0.6422, -0.9667],\n",
            "        [-0.5294, -0.0050,  0.1803, -0.6566,  0.0000, -0.2370, -0.8155, -0.7667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 5 inputs tensor([[-0.4118,  0.2161,  0.1803, -0.5354, -0.7352, -0.2191, -0.8574, -0.7000],\n",
            "        [-0.6471,  0.2060,  0.1475, -0.3939, -0.6809,  0.2787, -0.6806, -0.7000],\n",
            "        [-0.6471,  0.1357, -0.2787, -0.7374,  0.0000, -0.3323, -0.9471, -0.9667],\n",
            "        [-0.8824,  0.0000, -0.2131, -0.5960,  0.0000, -0.2638, -0.9471, -0.9667],\n",
            "        [-0.7647, -0.2462,  0.0492, -0.5152, -0.8700, -0.1148, -0.7506, -0.6000],\n",
            "        [-0.7647,  0.2161,  0.1475, -0.3535, -0.7754,  0.1654, -0.3100, -0.9333],\n",
            "        [-0.4118, -0.0050, -0.1148, -0.4343, -0.8038,  0.0134, -0.6405, -0.7000],\n",
            "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8437, -0.7000],\n",
            "        [-0.6471, -0.1960,  0.3443, -0.3737, -0.8345,  0.0194,  0.0367, -0.8000],\n",
            "        [-0.5294,  0.4673,  0.5082,  0.0000,  0.0000, -0.0700, -0.6063,  0.3333],\n",
            "        [-0.8824,  0.0754, -0.1803, -0.6162,  0.0000, -0.1565, -0.9120, -0.7333],\n",
            "        [ 0.0588,  0.0251,  0.2459, -0.2525,  0.0000, -0.0194, -0.4987, -0.1667],\n",
            "        [-0.4118,  0.0955,  0.2295, -0.4747,  0.0000,  0.0730, -0.6003,  0.3000],\n",
            "        [-0.5294,  0.7387,  0.1475, -0.7172, -0.6028, -0.1148, -0.7583, -0.6000],\n",
            "        [ 0.0000,  0.0050,  0.1475, -0.4747, -0.8818, -0.0820, -0.5568,  0.0000],\n",
            "        [-0.4118,  0.3769,  0.7705,  0.0000,  0.0000,  0.4545, -0.8728, -0.4667],\n",
            "        [-0.6471,  0.1156, -0.0820, -0.2121,  0.0000, -0.1028, -0.5909, -0.7000],\n",
            "        [-0.7647,  0.2965,  0.2131, -0.4747, -0.5154, -0.0104, -0.5619, -0.8667],\n",
            "        [-0.2941, -0.1960,  0.0820, -0.3939,  0.0000, -0.2191, -0.7993, -0.3333],\n",
            "        [-0.7647,  0.2261,  0.2459, -0.4545, -0.5272,  0.0700, -0.6541, -0.8333],\n",
            "        [-0.7647,  0.2462,  0.1148, -0.4343, -0.5154, -0.0194, -0.3194, -0.7000],\n",
            "        [-0.0588,  0.0955,  0.2459, -0.2121, -0.7305, -0.1684, -0.5201, -0.6667],\n",
            "        [-0.8824, -0.0050, -0.0492, -0.7980,  0.0000, -0.2429, -0.5961,  0.0000],\n",
            "        [ 0.4118,  0.0653,  0.3115,  0.0000,  0.0000, -0.2966, -0.9496, -0.2333],\n",
            "        [-0.5294, -0.1457, -0.0492, -0.5556, -0.8842, -0.1714, -0.8053, -0.7667],\n",
            "        [-0.4118,  0.2261,  0.4098,  0.0000,  0.0000,  0.0343, -0.8190, -0.6000],\n",
            "        [-0.6471,  0.0352,  0.1803, -0.3939, -0.6407, -0.1773, -0.4432, -0.8000],\n",
            "        [ 0.0000,  0.0854,  0.1148, -0.5960,  0.0000, -0.1863, -0.3945, -0.6333],\n",
            "        [-0.8824, -0.1859,  0.1803, -0.6364, -0.9054, -0.2072, -0.8249, -0.9000],\n",
            "        [-0.4118, -0.2161, -0.2131,  0.0000,  0.0000,  0.0045, -0.5081, -0.8667],\n",
            "        [-0.5294,  0.2060,  0.1148,  0.0000,  0.0000, -0.1177, -0.4611, -0.5667],\n",
            "        [-0.8824,  0.0000,  0.1148, -0.2929,  0.0000, -0.0462, -0.7344, -0.9667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 6 inputs tensor([[-0.6471,  0.7387,  0.3443, -0.0303,  0.0993,  0.1446,  0.7583, -0.8667],\n",
            "        [-0.4118, -0.0251,  0.2459, -0.4545,  0.0000,  0.0611, -0.7438,  0.0333],\n",
            "        [-0.6471,  0.0251,  0.2131,  0.0000,  0.0000, -0.1207, -0.9633, -0.6333],\n",
            "        [-0.8824,  0.4975,  0.1148, -0.4141, -0.6998, -0.1267, -0.7686, -0.3000],\n",
            "        [-0.6471,  0.1256,  0.2131, -0.3939,  0.0000, -0.0581, -0.8984, -0.8667],\n",
            "        [-0.5294,  0.5678,  0.2295,  0.0000,  0.0000,  0.4396, -0.8634, -0.6333],\n",
            "        [ 0.4118,  0.4070,  0.3443, -0.1313, -0.2317,  0.1684, -0.6157,  0.2333],\n",
            "        [-0.1765,  0.1457,  0.0492,  0.0000,  0.0000, -0.1833, -0.4415, -0.5667],\n",
            "        [-0.7647, -0.2864,  0.1475, -0.4545,  0.0000, -0.1654, -0.5662, -0.9667],\n",
            "        [ 0.0588,  0.2060,  0.1803, -0.5556, -0.8676, -0.3800, -0.4406, -0.1000],\n",
            "        [-0.2941,  0.1759,  0.5738,  0.0000,  0.0000, -0.1446, -0.9325, -0.7000],\n",
            "        [ 0.0588,  0.5276,  0.2787, -0.3131, -0.5957,  0.0194, -0.3040, -0.6000],\n",
            "        [-0.6471,  0.2161, -0.1475,  0.0000,  0.0000,  0.0730, -0.9582, -0.8667],\n",
            "        [ 0.1765, -0.3166,  0.7377, -0.5354, -0.8842,  0.0581, -0.8232, -0.1333],\n",
            "        [-0.5294,  0.3266,  0.0000,  0.0000,  0.0000, -0.0194, -0.8087, -0.9333],\n",
            "        [-0.8824,  0.3065, -0.0164, -0.5354, -0.5981, -0.1475, -0.4757,  0.0000],\n",
            "        [-0.1765,  0.2462,  0.1475, -0.3333, -0.4917, -0.2399, -0.9291, -0.4667],\n",
            "        [ 0.0588, -0.4271,  0.3115, -0.2525,  0.0000, -0.0224, -0.9846, -0.3333],\n",
            "        [ 0.0588,  0.4573,  0.3115, -0.0707, -0.6927,  0.1297, -0.5226, -0.3667],\n",
            "        [-0.5294,  0.8392,  0.0000,  0.0000,  0.0000, -0.1535, -0.8856, -0.5000],\n",
            "        [-0.8824,  0.1156,  0.5410,  0.0000,  0.0000, -0.0224, -0.8403, -0.2000],\n",
            "        [-0.1765,  0.2965,  0.1148, -0.0101, -0.7045,  0.1475, -0.6917, -0.2667],\n",
            "        [ 0.1765,  0.2563,  0.1475, -0.4747, -0.7281, -0.0730, -0.8915, -0.3333],\n",
            "        [-0.6471, -0.0050, -0.1148, -0.6162, -0.7967, -0.2370, -0.9351, -0.9000],\n",
            "        [-0.7647,  0.1558,  0.0492, -0.5556,  0.0000, -0.0820, -0.7071,  0.0000],\n",
            "        [-0.7647, -0.0653,  0.0492, -0.3535, -0.6217,  0.1326, -0.4910, -0.9333],\n",
            "        [ 0.0000,  0.4171,  0.3770, -0.4747,  0.0000, -0.0343, -0.6968, -0.9667],\n",
            "        [ 0.0000,  0.2563,  0.1148,  0.0000,  0.0000, -0.2638, -0.8907,  0.0000],\n",
            "        [-0.4118,  0.5578,  0.3770, -0.1111,  0.2884,  0.1535, -0.5380, -0.5667],\n",
            "        [ 0.0000,  0.0653,  0.1475, -0.2525, -0.6501,  0.1744, -0.5500, -0.9667],\n",
            "        [-0.4118, -0.0050,  0.2131, -0.4545,  0.0000, -0.1356, -0.8933, -0.6333],\n",
            "        [-0.7647, -0.1658,  0.0656, -0.4343, -0.8440,  0.0969, -0.5295, -0.9000]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 7 inputs tensor([[ 0.0000,  0.2060,  0.2131, -0.6364, -0.8511, -0.0909, -0.8232, -0.8333],\n",
            "        [-0.2941, -0.0653, -0.1803, -0.3939, -0.8487, -0.1446, -0.7626, -0.9333],\n",
            "        [-0.6471,  0.0754,  0.0164, -0.7374, -0.8865, -0.3174, -0.4876, -0.9333],\n",
            "        [-0.2941,  0.0754,  0.4426,  0.0000,  0.0000,  0.0969, -0.4458, -0.6667],\n",
            "        [ 0.1765,  0.1156,  0.1475, -0.4545,  0.0000, -0.1803, -0.9462, -0.3667],\n",
            "        [-0.5294, -0.1558,  0.4754, -0.5354, -0.8676,  0.1773, -0.9308, -0.8667],\n",
            "        [ 0.0000, -0.0854,  0.3115,  0.0000,  0.0000, -0.0343, -0.5534, -0.8000],\n",
            "        [-0.8824,  0.1658,  0.2787, -0.4141, -0.5745,  0.0760, -0.6430, -0.8667],\n",
            "        [-0.8824,  0.0352, -0.5082, -0.2323, -0.8038,  0.2906, -0.9103, -0.6000],\n",
            "        [-0.8824, -0.0653, -0.0820, -0.7778,  0.0000, -0.3294, -0.7105, -0.9667],\n",
            "        [ 0.0000,  0.3568,  0.1148, -0.1515, -0.4090,  0.2608, -0.7549, -0.9000],\n",
            "        [-0.6471,  0.0854,  0.0164, -0.5152,  0.0000, -0.2250, -0.8762, -0.8667],\n",
            "        [-0.5294,  0.2261,  0.1148,  0.0000,  0.0000,  0.0432, -0.7301, -0.7333],\n",
            "        [ 0.0000,  0.0754, -0.0164, -0.4949,  0.0000, -0.2131, -0.9530, -0.9333],\n",
            "        [ 0.0000,  0.0151,  0.0164,  0.0000,  0.0000, -0.3472, -0.7797, -0.8667],\n",
            "        [ 0.0000,  0.0151,  0.0656, -0.4343,  0.0000, -0.2668, -0.8642, -0.9667],\n",
            "        [-0.8824,  0.1457,  0.0820, -0.2727, -0.5272,  0.1356, -0.8198,  0.0000],\n",
            "        [ 0.1765,  0.2261,  0.1148,  0.0000,  0.0000, -0.0700, -0.8463, -0.3333],\n",
            "        [-0.8824,  0.1256,  0.3115, -0.0909, -0.6879,  0.0373, -0.8813, -0.9000],\n",
            "        [-0.0588,  0.0050,  0.2459,  0.0000,  0.0000,  0.1535, -0.9044, -0.3000],\n",
            "        [-0.7647,  0.2261, -0.1475, -0.1313, -0.6265,  0.0790, -0.3698, -0.7667],\n",
            "        [ 0.1765,  0.0854,  0.0820,  0.0000,  0.0000, -0.0343, -0.8343, -0.3000],\n",
            "        [ 0.0000,  0.0553,  0.0492, -0.1717, -0.6643,  0.2370, -0.9189, -0.9667],\n",
            "        [ 0.1765,  0.2965,  0.2459, -0.4343, -0.7116,  0.0700, -0.8275, -0.4000],\n",
            "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
            "        [-0.5294,  0.4171,  0.2131,  0.0000,  0.0000, -0.1773, -0.8582, -0.3667],\n",
            "        [-0.5294,  0.8995,  0.8033, -0.3737,  0.0000, -0.1505, -0.4859, -0.4667],\n",
            "        [-0.7647,  0.0553, -0.0492, -0.1919, -0.7778,  0.0402, -0.8745, -0.8667],\n",
            "        [-0.6471,  0.1357, -0.1803, -0.7980, -0.7991, -0.1207, -0.5320, -0.8667],\n",
            "        [-0.8824,  0.3065,  0.1475, -0.7374, -0.7518, -0.2280, -0.6635, -0.9667],\n",
            "        [-0.6471,  0.1658,  0.0000,  0.0000,  0.0000, -0.2996, -0.9069, -0.9333],\n",
            "        [ 0.0000,  0.4774,  0.3934,  0.0909,  0.0000,  0.2757, -0.7464, -0.9000]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 8 inputs tensor([[-0.2941,  0.0553,  0.3115, -0.4343,  0.0000, -0.0313, -0.3168, -0.8333],\n",
            "        [-0.5294,  0.2563,  0.3115,  0.0000,  0.0000, -0.0373, -0.6089, -0.8000],\n",
            "        [-0.5294, -0.0352, -0.0820, -0.6566, -0.8842, -0.3800, -0.7763, -0.8333],\n",
            "        [-0.6471,  0.5879,  0.1475, -0.3939, -0.2246,  0.0581, -0.7728, -0.5333],\n",
            "        [-0.1765,  0.7990,  0.5574, -0.3737,  0.0000,  0.0194, -0.9266,  0.3000],\n",
            "        [-0.8824, -0.0251,  0.1148, -0.5758,  0.0000, -0.1893, -0.1315, -0.9667],\n",
            "        [ 0.0000,  0.0754,  0.2459,  0.0000,  0.0000,  0.3502, -0.4808, -0.9000],\n",
            "        [-0.8824,  0.2261,  0.4754,  0.0303, -0.4799,  0.4814, -0.7891, -0.6667],\n",
            "        [-0.1765,  0.8794,  0.1148, -0.2121, -0.2813,  0.1237, -0.8497, -0.3333],\n",
            "        [ 0.5294,  0.2663,  0.4754,  0.0000,  0.0000,  0.2936, -0.5687, -0.3000],\n",
            "        [-0.4118,  0.6281,  0.7049,  0.0000,  0.0000,  0.1237, -0.9377,  0.0333],\n",
            "        [ 0.0000, -0.4271, -0.0164,  0.0000,  0.0000, -0.3532, -0.4389,  0.5333],\n",
            "        [-0.1765,  0.5980,  0.0492,  0.0000,  0.0000, -0.1833, -0.8155, -0.3667],\n",
            "        [ 0.0000,  0.1960,  0.0820, -0.4545,  0.0000,  0.1565, -0.8454, -0.9667],\n",
            "        [-0.7647,  0.2764, -0.2459, -0.5758, -0.2080,  0.0253, -0.9163, -0.9667],\n",
            "        [-0.7647, -0.0754,  0.0164, -0.4343,  0.0000, -0.0581, -0.9556, -0.9000],\n",
            "        [-0.8824, -0.0251,  0.1475, -0.6970,  0.0000, -0.4575, -0.9411,  0.0000],\n",
            "        [-0.7647, -0.2563,  0.0000,  0.0000,  0.0000,  0.0000, -0.9795, -0.9667],\n",
            "        [-0.7647, -0.1859, -0.0164, -0.5556,  0.0000, -0.1744, -0.8190, -0.8667],\n",
            "        [-0.0588,  0.0854,  0.1475,  0.0000,  0.0000, -0.0909, -0.2511, -0.6000],\n",
            "        [-0.5294,  0.4472, -0.0492, -0.4343, -0.6690, -0.1207, -0.8215, -0.4667],\n",
            "        [ 0.0000,  0.1759,  0.3115, -0.3737, -0.8747,  0.3472, -0.9906, -0.9000],\n",
            "        [-0.6471,  0.6281, -0.1475, -0.2323,  0.0000,  0.1088, -0.5098, -0.9000],\n",
            "        [ 0.0000,  0.3970,  0.0164, -0.6566, -0.5035, -0.3413, -0.8898,  0.0000],\n",
            "        [-0.4118, -0.5578,  0.0164,  0.0000,  0.0000, -0.2548, -0.5653, -0.5000],\n",
            "        [ 0.0588,  0.3467,  0.2131, -0.3333, -0.8582, -0.2280, -0.6738,  1.0000],\n",
            "        [ 0.0000,  0.0955,  0.4426, -0.3939,  0.0000, -0.0313, -0.3365, -0.4333],\n",
            "        [-0.8824,  0.2864, -0.2131, -0.0909, -0.5414,  0.2072, -0.5431, -0.9000],\n",
            "        [ 0.1765, -0.0955,  0.3934, -0.3535,  0.0000,  0.0402, -0.3621,  0.1667],\n",
            "        [-0.2941,  0.6281,  0.0164,  0.0000,  0.0000, -0.2757, -0.9146, -0.0333],\n",
            "        [ 0.0000,  0.7990, -0.1803, -0.2727, -0.6241,  0.1267, -0.6781, -0.9667],\n",
            "        [-0.8824,  0.4070,  0.2131, -0.4747, -0.5745, -0.2817, -0.3595, -0.9333]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 9 inputs tensor([[-0.5294,  0.4774,  0.2131, -0.4949, -0.3073,  0.0402, -0.7378, -0.7000],\n",
            "        [-0.7647, -0.1457,  0.0656,  0.0000,  0.0000,  0.1803, -0.2724, -0.8000],\n",
            "        [-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
            "        [-0.4118,  0.5879,  0.1475,  0.0000,  0.0000, -0.1118, -0.8898,  0.4000],\n",
            "        [-0.7647,  0.2060,  0.2459, -0.2525, -0.7518,  0.1833, -0.8830, -0.7333],\n",
            "        [-0.7647,  0.2965,  0.0000,  0.0000,  0.0000,  0.1475, -0.8070, -0.3333],\n",
            "        [ 0.1765,  0.6281,  0.3770,  0.0000,  0.0000, -0.1744, -0.9112,  0.1000],\n",
            "        [ 0.1765,  0.6884,  0.2131,  0.0000,  0.0000,  0.1326, -0.6080, -0.5667],\n",
            "        [-0.2941,  0.9095,  0.5082,  0.0000,  0.0000,  0.0581, -0.8292,  0.5000],\n",
            "        [-0.4118,  0.4472,  0.3443, -0.4747, -0.3262, -0.0462, -0.6806,  0.2333],\n",
            "        [ 0.0000, -0.3266,  0.2459,  0.0000,  0.0000,  0.3502, -0.9009, -0.1667],\n",
            "        [ 0.0000,  0.6784,  0.0000,  0.0000,  0.0000, -0.0373, -0.3501, -0.7000],\n",
            "        [-0.7647,  0.2764, -0.0492, -0.5152, -0.3499, -0.1744,  0.2997, -0.8667],\n",
            "        [-0.8824,  0.2060,  0.3115, -0.0303, -0.5272,  0.1595, -0.0743, -0.3333],\n",
            "        [-0.5294,  0.4472,  0.3443, -0.3535,  0.0000,  0.1475, -0.5935, -0.4667],\n",
            "        [-0.2941, -0.0050, -0.0164, -0.6162, -0.8723, -0.1982, -0.6422, -0.6333],\n",
            "        [ 0.5294,  0.0653,  0.1803,  0.0909,  0.0000,  0.0909, -0.9146, -0.2000],\n",
            "        [-0.8824,  0.7387,  0.2131,  0.0000,  0.0000,  0.0969, -0.9915, -0.4333],\n",
            "        [-0.2941,  0.9497,  0.2787,  0.0000,  0.0000, -0.2996, -0.9564,  0.2667],\n",
            "        [-0.8824,  0.1156,  0.0164, -0.7374, -0.5697, -0.2846, -0.9488, -0.9333],\n",
            "        [-0.8824,  0.5779,  0.1803, -0.5758, -0.6028, -0.2370, -0.9616, -0.9000],\n",
            "        [-0.8824,  0.1960, -0.2787, -0.0505, -0.8511,  0.0581, -0.8275, -0.8667],\n",
            "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n",
            "        [-0.6471,  0.3266,  0.3115,  0.0000,  0.0000,  0.0253, -0.7233, -0.2333],\n",
            "        [-0.5294,  0.1055,  0.0820,  0.0000,  0.0000, -0.0492, -0.6644, -0.7333],\n",
            "        [-0.8824, -0.1156,  0.0164, -0.5152, -0.8960, -0.1088, -0.7062, -0.9333],\n",
            "        [-0.8824,  0.0653,  0.1475, -0.4343, -0.6809,  0.0194, -0.9453, -0.9667],\n",
            "        [-0.0588,  0.5477,  0.2787, -0.3535,  0.0000, -0.0343, -0.6883, -0.2000],\n",
            "        [-0.0588, -0.1457, -0.0984, -0.5960,  0.0000, -0.2727, -0.9505, -0.3000],\n",
            "        [ 0.4118,  0.2161,  0.2787, -0.6566,  0.0000, -0.2101, -0.8454,  0.3667],\n",
            "        [-0.6471,  0.8090,  0.0492, -0.4949, -0.8345,  0.0134, -0.8352, -0.8333],\n",
            "        [-0.6471,  0.2864,  0.2787,  0.0000,  0.0000, -0.3711, -0.8377,  0.1333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 10 inputs tensor([[-0.8824,  0.0955, -0.3770, -0.6364, -0.7163, -0.3115, -0.7190, -0.8333],\n",
            "        [-0.4118,  0.0452,  0.2131,  0.0000,  0.0000, -0.1416, -0.9360, -0.1000],\n",
            "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000],\n",
            "        [ 0.6471,  0.0050,  0.2787, -0.4949, -0.5650,  0.0909, -0.7148, -0.1667],\n",
            "        [-0.6471,  0.5879,  0.2459, -0.2727, -0.4208, -0.0581, -0.3399, -0.7667],\n",
            "        [-0.5294,  0.3467,  0.1803,  0.0000,  0.0000, -0.2906, -0.8301,  0.3000],\n",
            "        [ 0.0000,  0.3166,  0.0820, -0.1919,  0.0000,  0.0224, -0.8992, -0.9667],\n",
            "        [-0.7647,  0.5879,  0.4754,  0.0000,  0.0000, -0.0581, -0.3792,  0.5000],\n",
            "        [-0.7647, -0.1558, -0.1803, -0.5354, -0.8203, -0.0939, -0.2400,  0.0000],\n",
            "        [-0.6471,  0.8291,  0.2131,  0.0000,  0.0000, -0.0909, -0.7720, -0.7333],\n",
            "        [-0.8824, -0.1759,  0.0492, -0.7374, -0.7754, -0.3681, -0.7122, -0.9333],\n",
            "        [-0.1765,  0.5075,  0.0820, -0.1515, -0.1915,  0.0343, -0.4535, -0.3000],\n",
            "        [-0.6471,  0.2362,  0.6393, -0.2929, -0.4326,  0.7079, -0.3151, -0.9667],\n",
            "        [-0.8824, -0.1156,  0.2787, -0.4141, -0.8203, -0.0462, -0.7549, -0.7333],\n",
            "        [-0.4118,  0.1658,  0.2131, -0.4141,  0.0000, -0.0373, -0.5030, -0.5333],\n",
            "        [-0.8824,  0.2864,  0.4426, -0.2121, -0.7400,  0.0879, -0.1640, -0.4667],\n",
            "        [-0.8824,  0.0754,  0.1803, -0.3939, -0.8061, -0.0820, -0.3655, -0.9000],\n",
            "        [ 0.0000,  0.3869,  0.0000,  0.0000,  0.0000,  0.0820, -0.2699, -0.8667],\n",
            "        [-0.4118,  0.3668,  0.3770, -0.1717, -0.7920,  0.0432, -0.8224, -0.5333],\n",
            "        [-0.1765,  0.0553,  0.0000,  0.0000,  0.0000,  0.0000, -0.8061, -0.9000],\n",
            "        [-0.5294,  0.2965,  0.4098, -0.5960, -0.3617,  0.0462, -0.8693, -0.9333],\n",
            "        [ 0.0000,  0.8894,  0.3443, -0.7172, -0.5626, -0.0462, -0.4842, -0.9667],\n",
            "        [-0.8824,  0.8995, -0.0164, -0.5354,  1.0000, -0.1028, -0.7267,  0.2667],\n",
            "        [ 0.0000, -0.0452,  0.3115, -0.0909, -0.7825,  0.0879, -0.7848, -0.8333],\n",
            "        [-0.8824,  0.3166,  0.0492, -0.7172, -0.0189, -0.2936, -0.7344,  0.0000],\n",
            "        [ 0.0000,  0.2362,  0.4426, -0.2525,  0.0000,  0.0492, -0.8984, -0.7333],\n",
            "        [ 0.1765,  0.1558,  0.6066,  0.0000,  0.0000, -0.2846, -0.1939, -0.5667],\n",
            "        [-0.2941,  0.0352,  0.1803, -0.3535, -0.5508,  0.1237, -0.7899,  0.1333],\n",
            "        [-0.2941,  0.1457,  0.4426,  0.0000,  0.0000, -0.1714, -0.8557,  0.5000],\n",
            "        [-0.8824,  0.0653,  0.2459,  0.0000,  0.0000,  0.1177, -0.8984, -0.8333],\n",
            "        [-0.8824,  0.1759,  0.4426, -0.5152, -0.6572,  0.0283, -0.7225, -0.3667],\n",
            "        [-0.7647, -0.0553,  0.2459, -0.6364, -0.8440, -0.0581, -0.5124, -0.9333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 11 inputs tensor([[-0.1765,  0.3668,  0.4754,  0.0000,  0.0000, -0.1088, -0.8873, -0.0333],\n",
            "        [ 0.2941,  0.3869,  0.2459,  0.0000,  0.0000, -0.0104, -0.7079, -0.5333],\n",
            "        [-0.2941,  0.0955, -0.0164, -0.4545,  0.0000, -0.2548, -0.8907, -0.8000],\n",
            "        [-0.7647, -0.0854,  0.0164,  0.0000,  0.0000, -0.1863, -0.6183, -0.9667],\n",
            "        [ 0.0000,  0.2161,  0.0820, -0.3939, -0.6099,  0.0224, -0.8933, -0.6000],\n",
            "        [-0.8824,  0.2864,  0.3443, -0.6566, -0.5674, -0.1803, -0.9684, -0.9667],\n",
            "        [-0.6471,  0.3970, -0.1148,  0.0000,  0.0000, -0.2370, -0.7233, -0.9667],\n",
            "        [-0.4118, -0.2663, -0.0164,  0.0000,  0.0000, -0.2012, -0.8377, -0.8000],\n",
            "        [-0.8824, -0.0352,  1.0000,  0.0000,  0.0000, -0.3323, -0.8898, -0.8000],\n",
            "        [ 0.0000,  0.6281,  0.2459, -0.2727,  0.0000,  0.4784, -0.7558, -0.8333],\n",
            "        [ 0.0000,  0.8090,  0.0820, -0.2121,  0.0000,  0.2519,  0.5500, -0.8667],\n",
            "        [-0.7647, -0.0050,  0.1475, -0.6768, -0.8960, -0.3920, -0.8659, -0.8000],\n",
            "        [ 0.5294,  0.5377,  0.4426, -0.2525, -0.6690,  0.2101, -0.0640, -0.4000],\n",
            "        [-0.6471,  0.0251, -0.2787, -0.5960, -0.7778, -0.0820, -0.7250, -0.8333],\n",
            "        [-0.7647,  0.0050,  0.1475,  0.0505, -0.8652,  0.2072, -0.4885, -0.8667],\n",
            "        [-0.7647,  0.0653, -0.0820, -0.4545, -0.6099, -0.1356, -0.7028, -0.9667],\n",
            "        [-0.5294,  0.1759,  0.0164, -0.7576,  0.0000, -0.1148, -0.7421, -0.7000],\n",
            "        [-0.8824,  0.0050,  0.1803, -0.7576, -0.8345, -0.2459, -0.5047, -0.7667],\n",
            "        [-0.6471,  0.2462,  0.3115, -0.3333, -0.6927, -0.0104, -0.8061, -0.8333],\n",
            "        [-0.5294,  0.5477,  0.1803, -0.4141, -0.7021, -0.0671, -0.7780, -0.4667],\n",
            "        [-0.8824, -0.0352,  0.0492, -0.4545, -0.7943, -0.0104, -0.8198,  0.0000],\n",
            "        [-0.8824,  0.4372,  0.4098, -0.3939, -0.2199, -0.1028, -0.3049, -0.9333],\n",
            "        [-0.8824, -0.0854,  0.0492, -0.5152,  0.0000, -0.1297, -0.9026,  0.0000],\n",
            "        [ 0.0000, -0.0653, -0.0164, -0.4949, -0.7825, -0.1446, -0.6123, -0.9667],\n",
            "        [-0.2941, -0.0151, -0.0492, -0.3333, -0.5508,  0.0134, -0.6994, -0.2667],\n",
            "        [-0.1765,  0.3367,  0.4426, -0.6970, -0.6336, -0.0343, -0.8429, -0.4667],\n",
            "        [-0.0588,  0.8693,  0.4754, -0.2929, -0.4681,  0.0283, -0.7054, -0.4667],\n",
            "        [-0.4118, -0.1156,  0.2787, -0.3939,  0.0000, -0.1773, -0.8463, -0.4667],\n",
            "        [-0.4118, -0.1357,  0.1148, -0.4343, -0.8322, -0.0999, -0.7558, -0.9000],\n",
            "        [-0.8824,  0.2663, -0.0164,  0.0000,  0.0000, -0.1028, -0.7686, -0.1333],\n",
            "        [-0.4118,  0.5879,  0.3770, -0.1717, -0.5035,  0.1744, -0.7293, -0.7333],\n",
            "        [-0.7647,  0.4673,  0.0000,  0.0000,  0.0000, -0.1803, -0.8617, -0.7667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "1 12 inputs tensor([[-0.0588,  0.4372,  0.0820,  0.0000,  0.0000,  0.0402, -0.9564, -0.3333],\n",
            "        [-0.0588,  0.2060,  0.4098,  0.0000,  0.0000, -0.1535, -0.8454, -0.9667],\n",
            "        [-0.1765,  0.4271, -0.0164, -0.3333, -0.5508, -0.1416, -0.4799,  0.3333],\n",
            "        [-0.0588,  0.1859,  0.1803, -0.6162,  0.0000, -0.3115,  0.1939, -0.1667],\n",
            "        [-0.0588,  0.9799,  0.2131,  0.0000,  0.0000, -0.2280, -0.0495, -0.4000],\n",
            "        [ 0.0000,  0.4573,  0.0000,  0.0000,  0.0000,  0.3174, -0.5286, -0.6667],\n",
            "        [-0.5294, -0.0050,  0.1148, -0.2323,  0.0000, -0.0224, -0.9428, -0.6000],\n",
            "        [-0.1765,  0.5075,  0.2787, -0.4141, -0.7021,  0.0492, -0.4757,  0.1000],\n",
            "        [-0.7647, -0.0955,  0.1148, -0.1515,  0.0000,  0.1386, -0.6371, -0.8000],\n",
            "        [-0.8824,  0.0000,  0.2131, -0.5960, -0.9456, -0.1744, -0.8113,  0.0000],\n",
            "        [ 0.0000,  0.0553,  0.3770,  0.0000,  0.0000, -0.1684, -0.4338,  0.3667],\n",
            "        [-0.6471,  0.7387,  0.2787, -0.2121, -0.5626,  0.0075, -0.2383, -0.6667],\n",
            "        [-0.8824, -0.1256, -0.0164, -0.2525, -0.8227,  0.1088, -0.6319, -0.9667],\n",
            "        [-0.8824,  1.0000,  0.2459, -0.1313,  0.0000,  0.2787,  0.1238, -0.9667],\n",
            "        [ 0.4118,  0.4070,  0.3934, -0.3333,  0.0000,  0.1148, -0.8582, -0.3333],\n",
            "        [-0.8824, -0.2663, -0.1803, -0.7980,  0.0000, -0.3145, -0.8548,  0.0000],\n",
            "        [ 0.0588,  0.4070,  0.5410,  0.0000,  0.0000, -0.0253, -0.4398, -0.2000],\n",
            "        [ 0.0000,  0.5276,  0.3443, -0.2121, -0.3570,  0.2370, -0.8360, -0.8000],\n",
            "        [-0.7647,  0.0050,  0.1148, -0.4949, -0.8322,  0.1475, -0.7899, -0.8333],\n",
            "        [-0.5294, -0.0553,  0.0656, -0.5556,  0.0000, -0.2638, -0.9402,  0.0000],\n",
            "        [-0.8824, -0.1256,  0.1148, -0.3131, -0.8180,  0.1207, -0.7242, -0.9000],\n",
            "        [ 0.4118,  0.0050,  0.3770, -0.3333, -0.7518, -0.1058, -0.6499, -0.1667],\n",
            "        [-0.8824, -0.0653,  0.1475, -0.3737,  0.0000, -0.0939, -0.7976, -0.9333],\n",
            "        [-0.0588,  0.0050,  0.2131, -0.1919, -0.4917,  0.1744, -0.5021, -0.2667],\n",
            "        [-0.8824, -0.2864,  0.2787,  0.0101, -0.8936, -0.0104, -0.7062,  0.0000],\n",
            "        [-0.7647,  0.0050,  0.0492, -0.5354,  0.0000, -0.1148, -0.7523,  0.0000],\n",
            "        [-0.8824,  0.6482,  0.3443, -0.1313, -0.8416, -0.0224, -0.7754, -0.0333],\n",
            "        [-0.5294,  0.2563,  0.1475, -0.6364, -0.7116, -0.1386, -0.0897, -0.2000],\n",
            "        [-0.7647,  0.5578, -0.1475, -0.4545,  0.2766,  0.1535, -0.8617, -0.8667],\n",
            "        [-0.2941,  0.4472,  0.1803, -0.4545, -0.4610,  0.0104, -0.8488, -0.3667],\n",
            "        [-0.8824,  0.1256,  0.1803, -0.3939, -0.5839,  0.0253, -0.6157, -0.8667],\n",
            "        [-0.8824,  0.2462,  0.2131, -0.2727,  0.0000, -0.1714, -0.9812, -0.7000]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 13 inputs tensor([[-0.0588, -0.0050,  0.3770,  0.0000,  0.0000,  0.0551, -0.7353, -0.0333],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.2929, -0.7872, -0.3502, -0.9342, -0.9667],\n",
            "        [-0.2941,  0.8392,  0.5410,  0.0000,  0.0000,  0.2161,  0.1810, -0.2000],\n",
            "        [-0.8824, -0.1558,  0.0492, -0.5354, -0.7281,  0.0999, -0.6644, -0.7667],\n",
            "        [ 0.0000,  0.0151,  0.2459,  0.0000,  0.0000,  0.0641, -0.8975, -0.8333],\n",
            "        [ 0.1765, -0.2462,  0.3443,  0.0000,  0.0000, -0.0075, -0.8420, -0.4333],\n",
            "        [-0.4118,  0.1558,  0.2459,  0.0000,  0.0000, -0.0700, -0.7737, -0.2333],\n",
            "        [-0.8824, -0.1658,  0.1148,  0.0000,  0.0000, -0.4575, -0.5337, -0.8000],\n",
            "        [ 0.0000, -0.1558,  0.3443, -0.3737, -0.7045,  0.1386, -0.8676, -0.9333],\n",
            "        [-0.6471,  0.7688,  0.4098, -0.4545, -0.6312, -0.0075, -0.0811,  0.0333],\n",
            "        [-0.0588,  0.9497,  0.3115,  0.0000,  0.0000, -0.2221, -0.5961,  0.5333],\n",
            "        [-0.8824,  0.2864,  0.6066, -0.1717, -0.8629, -0.0462,  0.0615, -0.6000],\n",
            "        [-0.6471, -0.1859,  0.4098, -0.6768, -0.8440, -0.1803, -0.8053, -0.9667],\n",
            "        [ 0.2941, -0.1457,  0.2131,  0.0000,  0.0000, -0.1028, -0.8104, -0.5333],\n",
            "        [-0.7647,  0.1457,  0.1148, -0.5556,  0.0000, -0.1446, -0.9880, -0.8667],\n",
            "        [-0.7647,  0.1256,  0.1148, -0.5556, -0.7778,  0.0164, -0.7976, -0.8333],\n",
            "        [-0.0588, -0.2563,  0.1475, -0.1919, -0.8842,  0.0522, -0.4646, -0.4000],\n",
            "        [-0.4118,  0.6884,  0.0492,  0.0000,  0.0000, -0.0194, -0.9513, -0.3333],\n",
            "        [-0.8824,  0.5176, -0.0164,  0.0000,  0.0000, -0.2221, -0.9137, -0.9667],\n",
            "        [-0.8824,  0.8191,  0.0492, -0.3939, -0.5745,  0.0164, -0.7865, -0.4333],\n",
            "        [-0.8824,  0.1558,  0.1475, -0.3939, -0.7731,  0.0313, -0.6149, -0.6333],\n",
            "        [-0.6471, -0.1256, -0.0164, -0.6364,  0.0000, -0.3502, -0.6874,  0.0000],\n",
            "        [-0.1765,  0.9497,  0.1148, -0.4343,  0.0000,  0.0700, -0.4304, -0.3333],\n",
            "        [-0.7647, -0.1256,  0.0000, -0.5354,  0.0000, -0.1386, -0.4065, -0.8667],\n",
            "        [-0.0588,  0.0754,  0.3115,  0.0000,  0.0000, -0.2668, -0.3356, -0.5667],\n",
            "        [ 0.0000,  0.3568,  0.5410, -0.0707, -0.6572,  0.2101, -0.8241, -0.8333],\n",
            "        [ 0.0000,  0.3869, -0.0164, -0.2929, -0.6052,  0.0313, -0.6106,  0.0000],\n",
            "        [-0.7647,  0.3467,  0.1475,  0.0000,  0.0000, -0.1386, -0.6038, -0.9333],\n",
            "        [-0.7647,  0.3065,  0.5738,  0.0000,  0.0000, -0.3264, -0.8377,  0.0000],\n",
            "        [-0.4118,  0.3065,  0.3443,  0.0000,  0.0000,  0.1654, -0.2502, -0.4667],\n",
            "        [-0.0588,  0.5176,  0.2787, -0.3535, -0.5035,  0.2787, -0.6260, -0.5000],\n",
            "        [-0.5294,  0.9799,  0.1475, -0.2121,  0.7589,  0.0939,  0.9223, -0.6667]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 14 inputs tensor([[ 0.0000, -0.1558,  0.0492, -0.5556, -0.8440,  0.0671, -0.6012,  0.0000],\n",
            "        [-0.6471,  0.5879,  0.0492, -0.7374, -0.0851, -0.0700, -0.8147, -0.9000],\n",
            "        [-0.4118,  0.0653,  0.3443, -0.3939,  0.0000,  0.1773, -0.8224, -0.4333],\n",
            "        [-0.7647, -0.0050, -0.0164, -0.6566, -0.6217,  0.0909, -0.6798,  0.0000],\n",
            "        [-0.8824,  0.1859, -0.0492, -0.2727, -0.7778, -0.0075, -0.8437, -0.9333],\n",
            "        [ 0.0000,  0.2663,  0.4098, -0.4545, -0.7163, -0.1833, -0.6268,  0.0000],\n",
            "        [ 0.0000,  0.8191,  0.4426, -0.1111,  0.2057,  0.2906, -0.8770, -0.8333],\n",
            "        [-0.2941,  0.0553,  0.1475, -0.3535, -0.8392, -0.0820, -0.9624, -0.4667],\n",
            "        [-0.0588, -0.0452,  0.1803,  0.0000,  0.0000,  0.0969, -0.6524,  0.2000],\n",
            "        [-0.8824,  0.4372,  0.3770, -0.5354, -0.2671,  0.2638, -0.1477, -0.9667],\n",
            "        [-0.2941, -0.0754,  0.0164, -0.3535, -0.7021, -0.0462, -0.9940, -0.1667],\n",
            "        [-0.0588,  0.7990,  0.1803, -0.1515, -0.6927, -0.0253, -0.4526, -0.5000],\n",
            "        [ 0.0000,  0.3166,  0.4426,  0.0000,  0.0000, -0.0581, -0.4321, -0.6333],\n",
            "        [ 0.0000,  0.1960,  0.0000,  0.0000,  0.0000, -0.0343, -0.9462, -0.9000],\n",
            "        [ 0.1765, -0.0553,  0.1803, -0.6364,  0.0000, -0.3115, -0.5585,  0.1667],\n",
            "        [-0.8824, -0.1960, -0.0984,  0.0000,  0.0000, -0.4307, -0.8463,  0.0000],\n",
            "        [-0.7647, -0.0754,  0.2459, -0.5960,  0.0000, -0.2787,  0.3834, -0.7667],\n",
            "        [ 0.0000, -0.0452,  0.3934, -0.4949, -0.9149,  0.1148, -0.8557, -0.9000],\n",
            "        [-0.8824,  0.6784,  0.2131, -0.6566, -0.6596, -0.3025, -0.6849, -0.6000],\n",
            "        [-0.5294,  0.2362,  0.0164,  0.0000,  0.0000, -0.0462, -0.8736, -0.5333],\n",
            "        [ 0.0000,  0.0754,  0.0164, -0.3939, -0.8251,  0.0909, -0.4202, -0.8667],\n",
            "        [-0.5294,  0.3668,  0.1475,  0.0000,  0.0000, -0.0700, -0.0572, -0.9667],\n",
            "        [ 0.0000,  0.2764,  0.3115, -0.2525, -0.5035,  0.0820, -0.3800, -0.9333],\n",
            "        [ 0.0588,  0.6482,  0.3770, -0.5758,  0.0000, -0.0820, -0.3570, -0.6333],\n",
            "        [-0.6471, -0.0352,  0.2787, -0.2121,  0.0000,  0.1118, -0.8634, -0.3667],\n",
            "        [ 0.0000,  0.2864,  0.1148, -0.6162, -0.5745, -0.0909,  0.1213, -0.8667],\n",
            "        [-0.7647, -0.3166,  0.0164, -0.7374, -0.9645, -0.4009, -0.8471, -0.9333],\n",
            "        [ 0.0000,  0.2462, -0.0820, -0.7374, -0.7518, -0.3502, -0.6806,  0.0000],\n",
            "        [-0.2941,  0.6583,  0.1148, -0.4747, -0.6028,  0.0015, -0.5278, -0.0667],\n",
            "        [-0.0588,  0.7688,  0.4754, -0.3131, -0.2908,  0.0045, -0.6678,  0.2333],\n",
            "        [-0.7647,  0.2864,  0.2787, -0.2525, -0.5697,  0.2906, -0.0213, -0.6667],\n",
            "        [-0.8824,  0.1960,  0.4098, -0.2121, -0.4799,  0.3592, -0.3766, -0.7333]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "1 15 inputs tensor([[-0.5294,  0.1457,  0.0656,  0.0000,  0.0000, -0.3472, -0.6977, -0.4667],\n",
            "        [-0.0588,  0.6784,  0.7377, -0.0707, -0.4539,  0.1207, -0.9257, -0.2667],\n",
            "        [-0.4118,  0.8794,  0.2459, -0.4545, -0.5106,  0.2996, -0.1836,  0.0667],\n",
            "        [-0.7647,  0.5779,  0.2131, -0.2929,  0.0402,  0.1744, -0.9522, -0.7000],\n",
            "        [-0.0588, -0.1558,  0.2131, -0.3737,  0.0000,  0.1416, -0.6763, -0.4000],\n",
            "        [-0.8824,  0.2563, -0.1803, -0.1919, -0.6052, -0.0075, -0.2451, -0.7667],\n",
            "        [-0.5294,  0.3266,  0.4098, -0.3737,  0.0000, -0.1654, -0.7088,  0.4000],\n",
            "        [-0.7647,  0.2261, -0.0164, -0.6364, -0.7494, -0.1118, -0.4543, -0.9667],\n",
            "        [-0.6471,  0.3065,  0.0492,  0.0000,  0.0000, -0.3115, -0.7985, -0.9667],\n",
            "        [-0.8824,  0.1960,  0.4426, -0.1717, -0.5981,  0.3502, -0.6336, -0.8333],\n",
            "        [-0.8824,  0.0050,  0.2131, -0.7576, -0.8913, -0.4188, -0.9394, -0.7667],\n",
            "        [-0.7647,  0.4472, -0.0492, -0.3333, -0.6809, -0.0581, -0.7062, -0.8667],\n",
            "        [-0.8824, -0.2060,  0.2295, -0.3939,  0.0000, -0.0462, -0.7284, -0.9667],\n",
            "        [-0.1765,  0.6080, -0.1148, -0.3535, -0.5863, -0.0909, -0.5645, -0.4000],\n",
            "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
            "        [ 0.0588, -0.2764,  0.2787, -0.4949,  0.0000, -0.0581, -0.8275, -0.4333],\n",
            "        [ 0.0000,  0.0251,  0.4098, -0.6566, -0.7518, -0.1267, -0.4731, -0.8000],\n",
            "        [-0.6471, -0.1960,  0.0000,  0.0000,  0.0000,  0.0000, -0.9180, -0.9667],\n",
            "        [-0.8824,  0.1156,  0.4098, -0.6162,  0.0000, -0.1028, -0.9445, -0.9333],\n",
            "        [-0.1765,  0.6181,  0.4098,  0.0000,  0.0000, -0.0939, -0.9257, -0.1333],\n",
            "        [-0.2941,  0.2563,  0.2459,  0.0000,  0.0000,  0.0075, -0.9633,  0.1000],\n",
            "        [-0.5294,  0.3769,  0.3770,  0.0000,  0.0000, -0.0700, -0.8514, -0.7000],\n",
            "        [-0.5294,  0.2965, -0.0164, -0.7576, -0.4539, -0.1803, -0.6166, -0.6667],\n",
            "        [-0.8824,  0.0050,  0.0820, -0.6970, -0.8676, -0.2966, -0.4979, -0.8333],\n",
            "        [-0.2941,  0.4774,  0.3115,  0.0000,  0.0000, -0.1207, -0.9146, -0.0333],\n",
            "        [ 0.0000,  0.4070,  0.0656, -0.4747, -0.6927,  0.2697, -0.6985, -0.9000],\n",
            "        [-0.7647,  0.1256,  0.2295, -0.3535,  0.0000,  0.0641, -0.9402,  0.0000],\n",
            "        [-0.2941,  0.1457,  0.0000,  0.0000,  0.0000,  0.0000, -0.9052, -0.8333],\n",
            "        [-0.6471, -0.0050,  0.3115, -0.7778, -0.8487, -0.4247, -0.8241, -0.7000],\n",
            "        [-0.7647,  0.2864,  0.0492, -0.1515,  0.0000,  0.1922, -0.1264, -0.9000],\n",
            "        [-0.8824, -0.0050,  0.1803, -0.3939, -0.9574,  0.1505, -0.7148,  0.0000],\n",
            "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 16 inputs tensor([[-0.6471,  0.1156,  0.4754, -0.7576, -0.8156, -0.1535, -0.6439, -0.7333],\n",
            "        [-0.4118,  0.3970,  0.3115, -0.2929, -0.6217, -0.0581, -0.7583, -0.8667],\n",
            "        [ 0.0588,  0.8492,  0.3934, -0.6970,  0.0000, -0.1058, -0.0307, -0.0667],\n",
            "        [-0.8824, -0.0854, -0.1148, -0.4949, -0.7636, -0.2489, -0.8668, -0.9333],\n",
            "        [-0.4118,  0.8995,  0.0492, -0.3333, -0.2317, -0.0700, -0.5687, -0.7333],\n",
            "        [ 0.0000,  0.8090,  0.4754, -0.4747, -0.7872,  0.0879, -0.7985, -0.5333],\n",
            "        [-0.4118,  0.4774,  0.2295,  0.0000,  0.0000, -0.1088, -0.6960, -0.7667],\n",
            "        [-0.2941,  0.6683,  0.2131,  0.0000,  0.0000, -0.2072, -0.8070,  0.5000],\n",
            "        [ 0.0588,  0.4573,  0.4426, -0.3131, -0.6099, -0.0969, -0.4082,  0.0667],\n",
            "        [-0.8824, -0.1960,  0.2131, -0.7778, -0.8582, -0.1058, -0.6166, -0.9667],\n",
            "        [-0.6471,  0.7186,  0.1803, -0.3333, -0.6809, -0.0075, -0.8967, -0.9000],\n",
            "        [-0.1765,  0.8492,  0.3770, -0.3333,  0.0000,  0.0581, -0.7635, -0.3333],\n",
            "        [-0.8824, -0.2060,  0.3115, -0.4949, -0.9125, -0.2429, -0.5687, -0.9667],\n",
            "        [-0.1765,  0.0251,  0.2131, -0.1919, -0.7518,  0.1088, -0.8924, -0.2000],\n",
            "        [-0.7647, -0.0452, -0.1148, -0.7172, -0.7920, -0.2221, -0.4278, -0.9667],\n",
            "        [-0.7647, -0.4372, -0.0820, -0.4343, -0.8936, -0.2787, -0.7831, -0.9667],\n",
            "        [-0.7647, -0.0050,  0.0000,  0.0000,  0.0000, -0.3383, -0.9744, -0.9333],\n",
            "        [-0.1765,  0.3769,  0.4754, -0.1717,  0.0000, -0.0462, -0.7327, -0.4000],\n",
            "        [-0.4118,  0.2362,  0.2131, -0.1919, -0.8180,  0.0164, -0.8369, -0.7667],\n",
            "        [-0.8824,  0.0754,  0.1148, -0.6162,  0.0000, -0.2101, -0.9257, -0.9000],\n",
            "        [ 0.5294, -0.2362, -0.0164,  0.0000,  0.0000, -0.0224, -0.9129, -0.3333],\n",
            "        [-0.4118,  0.1759,  0.5082,  0.0000,  0.0000,  0.0164, -0.7788, -0.4333],\n",
            "        [-0.0588,  0.9698,  0.2459, -0.4141, -0.3381,  0.1177, -0.5500,  0.2000],\n",
            "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
            "        [-0.6471, -0.0352, -0.0820, -0.3131, -0.7281, -0.2638, -0.2605, -0.4000],\n",
            "        [-0.8824,  0.6884,  0.4426, -0.4141,  0.0000,  0.0432, -0.2938,  0.0333],\n",
            "        [-0.8824, -0.2864, -0.2131, -0.6364, -0.8203, -0.3920, -0.7908, -0.9667],\n",
            "        [-0.4118,  0.3668,  0.3443,  0.0000,  0.0000,  0.0000, -0.5201,  0.6000],\n",
            "        [-0.7647,  0.0653,  0.0492, -0.2929, -0.7187, -0.0909,  0.1289, -0.5667],\n",
            "        [-0.5294,  0.1859,  0.1475,  0.0000,  0.0000,  0.3264, -0.2946, -0.8333],\n",
            "        [ 0.0000,  0.0452,  0.0492, -0.5354, -0.7258, -0.1714, -0.6789, -0.9333],\n",
            "        [-0.1765,  0.0955,  0.3115, -0.3737,  0.0000,  0.0700, -0.1042, -0.2667]]) labels tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n",
            "1 17 inputs tensor([[-0.1765,  0.0653, -0.0164, -0.5152,  0.0000, -0.2101, -0.8138, -0.7333],\n",
            "        [-0.0588,  0.2060,  0.0000,  0.0000,  0.0000, -0.1058, -0.9103, -0.4333],\n",
            "        [ 0.0588,  0.6583,  0.4426,  0.0000,  0.0000, -0.0939, -0.8087, -0.0667],\n",
            "        [-0.2941,  0.5176,  0.0164, -0.3737, -0.7163,  0.0581, -0.4757, -0.7667],\n",
            "        [ 0.5294,  0.5276,  0.4754, -0.3333, -0.9314, -0.2012, -0.4424, -0.2667],\n",
            "        [ 0.0000,  0.9900,  0.0820, -0.3535, -0.3522,  0.2310, -0.6379, -0.7667],\n",
            "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n",
            "        [-0.8824,  0.0955, -0.0820, -0.5758, -0.6809, -0.2489, -0.3553, -0.9333],\n",
            "        [-0.4118, -0.0452,  0.1803, -0.3333,  0.0000,  0.1237, -0.7506, -0.8000],\n",
            "        [-0.1765,  0.0050,  0.0000,  0.0000,  0.0000, -0.1058, -0.6533, -0.6333],\n",
            "        [-0.7647,  0.0854,  0.3115,  0.0000,  0.0000, -0.1952, -0.8454,  0.0333],\n",
            "        [-0.8824,  0.9698,  0.2459, -0.2727, -0.4113,  0.0879, -0.3194, -0.7333],\n",
            "        [-0.1765,  0.8794, -0.1803, -0.3333, -0.0733,  0.0104, -0.3612, -0.5667],\n",
            "        [-0.6471,  0.0050,  0.1148, -0.5354, -0.8085, -0.0581, -0.2562, -0.7667],\n",
            "        [-0.2941,  0.2563,  0.2787, -0.3737,  0.0000, -0.1773, -0.5841, -0.0667],\n",
            "        [ 0.0000,  0.6281,  0.2459,  0.1313, -0.7636,  0.5857, -0.4184, -0.8667],\n",
            "        [-0.8824, -0.0452, -0.0164, -0.6364, -0.8629, -0.2876, -0.8446, -0.9667],\n",
            "        [-0.7647,  0.1960,  0.0000,  0.0000,  0.0000, -0.4158, -0.3561,  0.7000],\n",
            "        [-0.4118,  0.3970,  0.0492, -0.2929, -0.6690, -0.1475, -0.7156, -0.8333],\n",
            "        [-0.7647,  0.1759,  0.4754, -0.6162, -0.8322, -0.2489, -0.7993,  0.0000],\n",
            "        [ 0.0000,  0.3769,  0.3770, -0.4545,  0.0000, -0.1863, -0.8693,  0.2667],\n",
            "        [-0.8824,  0.2462, -0.0164, -0.3535,  0.0000,  0.0671, -0.6277,  0.0000],\n",
            "        [ 0.0000,  0.1357,  0.2459,  0.0000,  0.0000, -0.0075, -0.8292, -0.9333],\n",
            "        [ 0.1765,  0.0151,  0.2459, -0.0303, -0.5745, -0.0194, -0.9206,  0.4000],\n",
            "        [-0.8824,  0.3367,  0.6721, -0.4343, -0.6690, -0.0224, -0.8668, -0.2000],\n",
            "        [-0.0588,  0.8191,  0.1148, -0.2727,  0.1702, -0.1028, -0.5414,  0.3000],\n",
            "        [-0.8824,  0.0352,  0.3115, -0.7778, -0.8061, -0.4218, -0.6473, -0.9667],\n",
            "        [-0.2941,  0.3467,  0.1475, -0.5354, -0.6927,  0.0551, -0.6038, -0.7333],\n",
            "        [ 0.0588,  0.1256,  0.3443, -0.5152,  0.0000, -0.1595,  0.0282, -0.0333],\n",
            "        [-0.8824, -0.0955,  0.1148, -0.8384,  0.0000, -0.2697, -0.0948, -0.5000],\n",
            "        [ 0.2941,  0.3869,  0.2131, -0.4747, -0.6596,  0.0760, -0.5909, -0.0333],\n",
            "        [-0.7647,  0.0553,  0.3115, -0.0909, -0.5485,  0.0045, -0.4594, -0.7333]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "1 18 inputs tensor([[ 0.4118, -0.1558,  0.1803, -0.3737,  0.0000, -0.1148, -0.8130, -0.1667],\n",
            "        [-0.8824,  0.1960, -0.1148, -0.7374, -0.8818, -0.3353, -0.8915, -0.9000],\n",
            "        [-0.8824,  0.2261,  0.0492, -0.3535, -0.6312,  0.0462, -0.4757, -0.7000],\n",
            "        [ 0.2941,  0.3568,  0.0000,  0.0000,  0.0000,  0.5589, -0.5730, -0.3667],\n",
            "        [ 0.0588,  0.2462,  0.1475, -0.3333, -0.0496,  0.0551, -0.8258, -0.5667],\n",
            "        [-0.7647,  0.0553,  0.2295,  0.0000,  0.0000, -0.3055, -0.5884,  0.0667],\n",
            "        [-0.4118,  0.1256,  0.0820,  0.0000,  0.0000,  0.1267, -0.8437, -0.3333],\n",
            "        [-0.6471, -0.2563,  0.1148, -0.4343, -0.8936, -0.1148, -0.8164, -0.9333],\n",
            "        [-0.8824,  0.3668,  0.2131,  0.0101, -0.5177,  0.1148, -0.7259, -0.9000],\n",
            "        [-0.8824, -0.0754,  0.0164, -0.4949, -0.9031, -0.4188, -0.6550, -0.8667],\n",
            "        [-0.6471, -0.0955,  0.2787,  0.0000,  0.0000,  0.2727, -0.5892,  0.0000],\n",
            "        [-0.0588, -0.3467,  0.1803, -0.5354,  0.0000, -0.0462, -0.5542, -0.3000],\n",
            "        [-0.7647,  0.1256,  0.4098, -0.1515, -0.6217,  0.1446, -0.8565, -0.7667],\n",
            "        [-0.7647, -0.0050, -0.1475, -0.6970, -0.7778, -0.2668, -0.5226,  0.0000],\n",
            "        [-0.8824,  0.8090,  0.0000,  0.0000,  0.0000,  0.2906, -0.8258, -0.3333],\n",
            "        [-0.6471, -0.0050,  0.0164, -0.6162, -0.8251, -0.3502, -0.8284, -0.8333],\n",
            "        [ 0.0588,  0.5477,  0.2787, -0.3939, -0.7636, -0.0790, -0.9266, -0.2000],\n",
            "        [-0.6471,  0.8794,  0.1475, -0.5556, -0.5272,  0.0849, -0.7182, -0.5000],\n",
            "        [-0.7647, -0.0352,  0.1148, -0.7374, -0.8842, -0.3711, -0.5141, -0.8333],\n",
            "        [-0.8824, -0.0251,  0.0492, -0.6162, -0.8061, -0.4575, -0.8113,  0.0000],\n",
            "        [-0.7647,  0.0854,  0.0164, -0.7980, -0.3428, -0.2459, -0.3143, -0.9667],\n",
            "        [-0.4118, -0.1156,  0.0820, -0.5758, -0.9456, -0.2727, -0.7746, -0.7000],\n",
            "        [-0.7647,  0.0251,  0.4098, -0.2727, -0.7163,  0.3562, -0.9582, -0.9333],\n",
            "        [ 0.0000,  0.1960,  0.0492, -0.6364, -0.7825,  0.0402, -0.4475, -0.9333],\n",
            "        [-0.5294, -0.0854,  0.1475, -0.3535, -0.7920, -0.0134, -0.6857, -0.9667],\n",
            "        [-0.4118,  0.3266,  0.3115,  0.0000,  0.0000, -0.2012, -0.9078,  0.6000],\n",
            "        [ 0.0588, -0.0854,  0.1148,  0.0000,  0.0000, -0.2787, -0.8958,  0.2333],\n",
            "        [ 0.0588,  0.5678,  0.4098, -0.4343, -0.6336,  0.0224, -0.0512, -0.3000],\n",
            "        [-0.0588,  0.2663,  0.2131, -0.2323, -0.8227, -0.2280, -0.9283, -0.4000],\n",
            "        [-0.0588,  0.2462,  0.2459, -0.5152,  0.4184, -0.1446, -0.4799,  0.0333],\n",
            "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
            "        [-0.0588,  0.5578,  0.0164, -0.4747,  0.1702,  0.0134, -0.6029, -0.1667]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "1 19 inputs tensor([[-0.6471,  0.2663,  0.4426, -0.1717, -0.4444,  0.1714, -0.4654, -0.8000],\n",
            "        [-0.8824,  0.2161,  0.2787, -0.2121, -0.8251,  0.1624, -0.8437, -0.7667],\n",
            "        [-0.7647,  0.7588,  0.4426,  0.0000,  0.0000, -0.3174, -0.7882, -0.9667],\n",
            "        [-0.0588,  0.3367,  0.1803,  0.0000,  0.0000, -0.0194, -0.8360, -0.4000],\n",
            "        [-0.8824, -0.0452,  0.3443, -0.4949, -0.5745,  0.0432, -0.8676, -0.2667],\n",
            "        [-0.5294,  0.1256,  0.2787, -0.1919,  0.0000,  0.1744, -0.8651, -0.4333],\n",
            "        [-0.8824,  0.0251,  0.2131,  0.0000,  0.0000,  0.1773, -0.8164, -0.3000],\n",
            "        [-0.7647,  0.0754,  0.2131, -0.3939, -0.7636,  0.0015, -0.7216, -0.9333],\n",
            "        [ 0.0000,  0.4673,  0.3443,  0.0000,  0.0000,  0.2072,  0.4543, -0.2333],\n",
            "        [ 0.0000,  0.4171,  0.0000,  0.0000,  0.0000,  0.2638, -0.8915, -0.7333],\n",
            "        [-0.8824, -0.2261, -0.0820, -0.3939, -0.8676, -0.0075,  0.0017, -0.9000],\n",
            "        [-0.7647, -0.3166,  0.1475, -0.3535, -0.8440, -0.2548, -0.9069, -0.8667],\n",
            "        [-0.5294, -0.0955,  0.4426, -0.0505, -0.8723,  0.1237, -0.7575, -0.7333],\n",
            "        [-0.6471,  0.4171,  0.0000,  0.0000,  0.0000, -0.1058, -0.4167, -0.8000],\n",
            "        [ 0.0000,  0.0553,  0.1148, -0.5556,  0.0000, -0.4039, -0.8651, -0.9667],\n",
            "        [-0.8824, -0.1859,  0.2131, -0.1717, -0.8652,  0.3800, -0.1307, -0.6333],\n",
            "        [ 0.0000,  0.4673,  0.1475,  0.0000,  0.0000,  0.1297, -0.7814, -0.7667],\n",
            "        [-0.8824,  0.2563,  0.1475, -0.5152, -0.7400, -0.2757, -0.8779, -0.8667],\n",
            "        [-0.1765,  0.5980,  0.0820,  0.0000,  0.0000, -0.0939, -0.7395, -0.5000],\n",
            "        [ 0.0000,  0.0151,  0.0492, -0.6566,  0.0000, -0.3741, -0.8514,  0.0000],\n",
            "        [-0.2941,  0.5477,  0.2787, -0.1717, -0.6690,  0.3741, -0.5790, -0.8000],\n",
            "        [ 0.0000,  0.7789, -0.0164, -0.4141,  0.1300,  0.0313, -0.1512,  0.0000],\n",
            "        [-0.5294,  0.2864,  0.1475,  0.0000,  0.0000,  0.0224, -0.8079, -0.9000],\n",
            "        [ 0.6471,  0.7588,  0.0164, -0.3939,  0.0000,  0.0015, -0.8856, -0.4333],\n",
            "        [-0.4118,  0.0955,  0.0164, -0.1717, -0.6950,  0.0671, -0.6277, -0.8667],\n",
            "        [-0.6471,  0.2965,  0.5082, -0.0101, -0.6336,  0.0849, -0.2400, -0.6333],\n",
            "        [-0.7647, -0.1156,  0.2131, -0.6162, -0.8747, -0.1356, -0.8711, -0.9667],\n",
            "        [ 0.0588,  0.2362,  0.1475, -0.1111, -0.7778, -0.0134, -0.7472, -0.3667],\n",
            "        [-0.4118,  0.1759,  0.4098, -0.3939, -0.7518,  0.1654, -0.8523, -0.3000],\n",
            "        [-0.5294,  0.1558,  0.1803,  0.0000,  0.0000, -0.1386, -0.7455, -0.1667],\n",
            "        [-0.8824,  0.3869,  0.3443,  0.0000,  0.0000,  0.1952, -0.8651, -0.7667],\n",
            "        [-0.5294, -0.0050,  0.2459, -0.6970, -0.8794, -0.3085, -0.8762,  0.0000]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 20 inputs tensor([[-0.7647, -0.0955,  0.1475, -0.6566,  0.0000, -0.1863, -0.9940, -0.9667],\n",
            "        [-0.8824, -0.2060, -0.0164, -0.1515, -0.8865,  0.2966, -0.4876, -0.9333],\n",
            "        [-0.5294,  0.8492,  0.2787, -0.2121, -0.3452,  0.1028, -0.8412, -0.6667],\n",
            "        [-0.8824,  0.0955, -0.0492, -0.6364, -0.7258, -0.1505, -0.8796, -0.9667],\n",
            "        [ 0.0000,  0.0251, -0.1475,  0.0000,  0.0000, -0.2519,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.3266,  0.2787,  0.0000,  0.0000, -0.0343, -0.7310,  0.0000],\n",
            "        [-0.2941, -0.0754,  0.5082,  0.0000,  0.0000, -0.4069, -0.9061, -0.7667],\n",
            "        [ 0.0000,  0.1457,  0.3115, -0.3131, -0.3262,  0.3174, -0.9240, -0.8000],\n",
            "        [-0.5294,  0.0352, -0.0164, -0.3333, -0.5461, -0.2846, -0.2417, -0.6000],\n",
            "        [-0.2941,  0.2462,  0.1803,  0.0000,  0.0000, -0.1773, -0.7523, -0.7333],\n",
            "        [-0.4118, -0.0352,  0.2131, -0.6364, -0.8416,  0.0015, -0.2152, -0.2667],\n",
            "        [-0.6471,  0.9397,  0.1475, -0.3737,  0.0000,  0.0402, -0.8608, -0.8667],\n",
            "        [ 0.0588, -0.1055,  0.0164,  0.0000,  0.0000, -0.3294, -0.9453, -0.6000],\n",
            "        [-0.8824,  0.4774,  0.5410, -0.1717,  0.0000,  0.4694, -0.7609, -0.8000],\n",
            "        [ 0.0000,  0.1859,  0.0492, -0.5354, -0.7896,  0.0000,  0.4116,  0.0000],\n",
            "        [-0.2941,  0.0854, -0.2787, -0.5960, -0.6927, -0.2846, -0.3723, -0.5333],\n",
            "        [-0.8824,  0.0151, -0.1803, -0.6970, -0.9149, -0.2787, -0.6174, -0.8333],\n",
            "        [-0.8824,  0.0553, -0.0492,  0.0000,  0.0000, -0.2757, -0.9069,  0.0000],\n",
            "        [ 0.0000,  0.2362,  0.1803,  0.0000,  0.0000,  0.0820, -0.8463,  0.0333],\n",
            "        [ 0.0000,  0.6583,  0.4754, -0.3333,  0.6076,  0.5589, -0.7020, -0.9333],\n",
            "        [-0.5294,  0.0955,  0.0492, -0.1111, -0.7660,  0.0373, -0.2938, -0.8333],\n",
            "        [ 0.0000,  0.6583,  0.2459, -0.1313, -0.3972,  0.4277, -0.8454, -0.8333],\n",
            "        [-0.0588,  0.1055,  0.2459,  0.0000,  0.0000, -0.1714, -0.8642,  0.2333],\n",
            "        [-0.2941,  0.0452,  0.2131, -0.6364, -0.6312, -0.1088, -0.4500, -0.3333],\n",
            "        [-0.8824, -0.0452,  0.2131, -0.5758, -0.8274, -0.2280, -0.4919, -0.5000],\n",
            "        [-0.8824, -0.1256,  0.2787, -0.4545, -0.9244,  0.0313, -0.9804, -0.9667],\n",
            "        [ 0.0000, -0.0854,  0.1148, -0.3535, -0.5035,  0.1893, -0.7412, -0.8667],\n",
            "        [-0.1765,  0.4774,  0.2459,  0.0000,  0.0000,  0.1744, -0.8471, -0.2667],\n",
            "        [ 0.0000,  0.2462,  0.1475, -0.5960,  0.0000, -0.1833, -0.8497, -0.5000],\n",
            "        [ 0.0000, -0.2563, -0.1475, -0.7980, -0.9149, -0.1714, -0.8369, -0.9667],\n",
            "        [-0.8824,  0.2663, -0.0820, -0.4141, -0.6407, -0.1446, -0.3826,  0.0000],\n",
            "        [-0.5294,  0.1055,  0.5082,  0.0000,  0.0000,  0.1207, -0.9035, -0.7000]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]])\n",
            "1 21 inputs tensor([[ 0.0000,  0.8995,  0.7049, -0.4949,  0.0000,  0.0224, -0.6951, -0.3333],\n",
            "        [-0.4118,  0.0352,  0.7705, -0.2525,  0.0000,  0.1684, -0.8061,  0.4667],\n",
            "        [-0.5294,  0.5477,  0.0164, -0.3737, -0.3286, -0.0224, -0.8642, -0.9333],\n",
            "        [-0.8824, -0.0251,  0.0820, -0.6970, -0.6690, -0.3085, -0.6507, -0.9667],\n",
            "        [-0.5294,  0.5879,  0.2787,  0.0000,  0.0000, -0.0194, -0.3809, -0.6667],\n",
            "        [-0.7647, -0.0955,  0.3115, -0.7172, -0.8700, -0.2727, -0.8540, -0.9000],\n",
            "        [-0.7647,  0.2563, -0.0164, -0.5960, -0.6690,  0.0075, -0.9915, -0.6667],\n",
            "        [-0.5294, -0.2362,  0.0164,  0.0000,  0.0000,  0.0134, -0.7327, -0.8667],\n",
            "        [ 0.0000, -0.0553,  0.1475, -0.4545, -0.7281,  0.2966, -0.7703,  0.0000],\n",
            "        [-0.4118,  0.2864,  0.3115,  0.0000,  0.0000,  0.0313, -0.9436, -0.2000],\n",
            "        [-0.5294,  0.1457,  0.0492,  0.0000,  0.0000, -0.1386, -0.9590, -0.9000],\n",
            "        [-0.1765,  0.9698,  0.4754,  0.0000,  0.0000,  0.1863, -0.6815, -0.3333],\n",
            "        [-0.6471,  0.6985,  0.2131, -0.6162, -0.7045, -0.1088, -0.8377, -0.6667],\n",
            "        [ 0.2941,  0.1156,  0.3770, -0.1919,  0.0000,  0.3949, -0.2767, -0.2000],\n",
            "        [-0.2941,  0.0000,  0.1148, -0.1717,  0.0000,  0.1624, -0.4458, -0.3333],\n",
            "        [ 0.0000, -0.2663,  0.0000,  0.0000,  0.0000, -0.3711, -0.7746, -0.8667],\n",
            "        [-0.2941,  0.5477,  0.2131, -0.3535, -0.5437, -0.1267, -0.3501, -0.4000],\n",
            "        [-0.7647,  0.5578,  0.2131, -0.6566, -0.7731, -0.2072, -0.6968, -0.8000],\n",
            "        [-0.8824, -0.1055, -0.6066, -0.6162, -0.9409, -0.1714, -0.5892,  0.0000],\n",
            "        [-0.2941,  0.0352,  0.0820,  0.0000,  0.0000, -0.2757, -0.8540, -0.7333],\n",
            "        [ 0.0000,  0.7990,  0.4754, -0.4545,  0.0000,  0.3145, -0.4808, -0.9333],\n",
            "        [ 0.5294,  0.5879,  0.8689,  0.0000,  0.0000,  0.2608, -0.8471, -0.2333],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.7576, -0.8983, -0.1893, -0.5713, -0.9000],\n",
            "        [-0.2941,  0.1960, -0.1803, -0.5556, -0.5839, -0.1922,  0.0589, -0.6000],\n",
            "        [ 0.5294,  0.4573,  0.3443, -0.6162, -0.7400, -0.3383, -0.8574,  0.2000],\n",
            "        [-0.8824,  0.8191,  0.2787, -0.1515, -0.3073,  0.1922,  0.0077, -0.9667],\n",
            "        [-0.4118,  0.2663,  0.2787, -0.4545, -0.9480, -0.1177, -0.6917, -0.3667],\n",
            "        [-0.2941,  0.3467,  0.3115, -0.2525, -0.1253,  0.3770, -0.8634, -0.1667],\n",
            "        [-0.7647,  0.0151, -0.0492, -0.6566, -0.3735, -0.2787, -0.5423, -0.9333],\n",
            "        [-0.6471,  0.0653,  0.1803,  0.0000,  0.0000, -0.2310, -0.8898, -0.8000],\n",
            "        [-0.5294,  0.4673,  0.2787,  0.0000,  0.0000,  0.1475, -0.6225,  0.5333],\n",
            "        [ 0.0000, -0.0452,  0.0492, -0.2121, -0.7518,  0.3294, -0.7541, -0.9667]]) labels tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 22 inputs tensor([[ 0.0000,  0.3166,  0.0000,  0.0000,  0.0000,  0.2876, -0.8360, -0.8333],\n",
            "        [-0.7647,  0.0050,  0.0820, -0.5960, -0.7872, -0.0194, -0.3262, -0.7667],\n",
            "        [-0.6471, -0.1558,  0.1148, -0.3939, -0.7494, -0.0492, -0.5619, -0.8667],\n",
            "        [-0.7647, -0.1558,  0.0000,  0.0000,  0.0000,  0.0000, -0.8070,  0.0000],\n",
            "        [ 0.0000,  0.3769,  0.1475, -0.2323,  0.0000, -0.0104, -0.9214, -0.9667],\n",
            "        [-0.7647,  0.0955,  0.5082,  0.0000,  0.0000,  0.2727, -0.3450,  0.1000],\n",
            "        [ 0.0000, -0.0251,  0.0492, -0.2727, -0.7636,  0.0969, -0.5542, -0.8667],\n",
            "        [ 0.0000,  0.5176,  0.4754, -0.0707,  0.0000,  0.2548, -0.7498,  0.0000],\n",
            "        [-0.6471, -0.1558,  0.1803, -0.3535,  0.0000,  0.1088, -0.8386, -0.7667],\n",
            "        [-0.6471,  0.9196,  0.1148, -0.6970, -0.6927, -0.0790, -0.8113, -0.5667],\n",
            "        [ 0.0000,  0.7387,  0.2787, -0.3535, -0.3735,  0.3860, -0.0769,  0.2333],\n",
            "        [ 0.0000,  0.1759,  0.0000,  0.0000,  0.0000,  0.0075, -0.2707, -0.2333],\n",
            "        [ 0.4118, -0.1156,  0.2131, -0.1919, -0.8723,  0.0522, -0.7438, -0.1000],\n",
            "        [-0.8824,  0.3970, -0.2459, -0.6162, -0.8038, -0.1446, -0.5081, -0.9667],\n",
            "        [-0.5294, -0.0955,  0.0000,  0.0000,  0.0000, -0.1654, -0.5457, -0.6667],\n",
            "        [-0.2941, -0.0854,  0.0000,  0.0000,  0.0000, -0.1118, -0.6388, -0.6667],\n",
            "        [-0.6471,  0.1156,  0.0164,  0.0000,  0.0000, -0.3264, -0.9453,  0.0000],\n",
            "        [-0.7647, -0.1859,  0.1803, -0.6970, -0.8203, -0.1028, -0.5995, -0.8667],\n",
            "        [ 0.0588,  0.2261, -0.0820,  0.0000,  0.0000, -0.0075, -0.1153, -0.6000],\n",
            "        [-0.7647,  0.1156, -0.0164,  0.0000,  0.0000, -0.2191, -0.7737, -0.9333],\n",
            "        [-0.8824,  0.3568, -0.1148,  0.0000,  0.0000, -0.2042, -0.4799,  0.3667],\n",
            "        [-0.8824,  0.9397, -0.1803, -0.6768, -0.1135, -0.2280, -0.5073, -0.9000],\n",
            "        [-0.5294,  0.4673,  0.3934, -0.4545, -0.7636, -0.1386, -0.9052, -0.8000],\n",
            "        [ 0.0000,  0.2563,  0.5738,  0.0000,  0.0000, -0.3294, -0.8429,  0.0000],\n",
            "        [-0.5294, -0.0452,  0.1475, -0.3535,  0.0000, -0.0432, -0.5440, -0.9000],\n",
            "        [ 0.2941,  0.2060,  0.3115, -0.2525, -0.6454,  0.2608, -0.3962, -0.1000],\n",
            "        [-0.8824,  0.4472,  0.3443, -0.0707, -0.5745,  0.3741, -0.7805, -0.1667],\n",
            "        [ 0.5294,  0.2965,  0.0000, -0.3939,  0.0000,  0.1893, -0.5807, -0.2333],\n",
            "        [-0.4118,  0.1457,  0.2131,  0.0000,  0.0000, -0.2578, -0.4313,  0.2000],\n",
            "        [-0.1765, -0.3769,  0.2787,  0.0000,  0.0000, -0.0283, -0.7327, -0.3333],\n",
            "        [-0.1765,  0.0352,  0.0820, -0.3535,  0.0000,  0.1654, -0.7728, -0.6667],\n",
            "        [ 0.0000,  0.3769,  0.1148, -0.7172, -0.6501, -0.2608, -0.9445,  0.0000]]) labels tensor([[0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.]])\n",
            "1 23 inputs tensor([[-0.2941,  0.2362,  0.1803, -0.0909, -0.4563,  0.0015, -0.4406, -0.5667],\n",
            "        [-0.5294, -0.0251, -0.0164, -0.5354,  0.0000, -0.1595, -0.6883, -0.9667],\n",
            "        [ 0.0588,  0.0653, -0.1475,  0.0000,  0.0000, -0.0700, -0.7421, -0.3000],\n",
            "        [-0.7647, -0.0955, -0.0164,  0.0000,  0.0000, -0.2996, -0.9035, -0.8667],\n",
            "        [-0.0588,  0.2060,  0.2787,  0.0000,  0.0000, -0.2548, -0.7173,  0.4333],\n",
            "        [ 0.0000, -0.0050,  0.0000,  0.0000,  0.0000, -0.2548, -0.8506, -0.9667],\n",
            "        [-0.8824,  0.0854,  0.4426, -0.6162,  0.0000, -0.1922, -0.7250, -0.9000],\n",
            "        [ 0.1765, -0.0754,  0.0164,  0.0000,  0.0000, -0.2280, -0.9240, -0.6667],\n",
            "        [-0.1765,  0.1960,  0.0000,  0.0000,  0.0000, -0.2489, -0.8881, -0.4667],\n",
            "        [-0.6471,  0.1658,  0.2131, -0.6970, -0.7518, -0.2161, -0.9752, -0.9000],\n",
            "        [-0.7647,  0.0854,  0.0492,  0.0000,  0.0000, -0.0820, -0.9317,  0.0000],\n",
            "        [-0.7647, -0.0754, -0.1475,  0.0000,  0.0000, -0.1028, -0.9462, -0.9667],\n",
            "        [-0.5294, -0.1658,  0.4098, -0.6162,  0.0000, -0.1267, -0.7959, -0.5667],\n",
            "        [ 0.1765,  0.6181,  0.1148, -0.5354, -0.6879, -0.2399, -0.7882, -0.1333],\n",
            "        [ 0.0000, -0.0653, -0.0164,  0.0000,  0.0000,  0.0522, -0.8420, -0.8667],\n",
            "        [-0.8824, -0.0955,  0.0164, -0.6364, -0.8605, -0.2519,  0.0162, -0.8667],\n",
            "        [-0.1765, -0.0251,  0.2459, -0.3535, -0.7849,  0.2191, -0.3228, -0.6333],\n",
            "        [-0.7647,  0.1055,  0.2131, -0.4141, -0.7045, -0.0343, -0.4705, -0.8000],\n",
            "        [-0.8824,  0.0854, -0.0164, -0.0707, -0.5792,  0.0581, -0.7122, -0.9000],\n",
            "        [-0.4118,  0.6683,  0.2459,  0.0000,  0.0000,  0.3621, -0.7763, -0.8000],\n",
            "        [-0.2941,  0.1156,  0.0492, -0.2121,  0.0000,  0.0194, -0.8446, -0.9000],\n",
            "        [-0.8824,  0.0955, -0.0164, -0.8384, -0.5697, -0.2429, -0.2579,  0.0000],\n",
            "        [ 0.0588,  0.7186,  0.8033, -0.5152, -0.4326,  0.3532, -0.4509,  0.1000]]) labels tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J4HjQ8vuQ0Va",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q_p3oeUXRKeZ",
        "colab": {}
      },
      "source": [
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('diabetes.csv.gz',\n",
        "                        delimiter=',', dtype=np.float32)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "\n",
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = torch.nn.Linear(8, 6)\n",
        "        self.l2 = torch.nn.Linear(6, 4)\n",
        "        self.l3 = torch.nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yAQ3OXf6RQAS",
        "colab": {}
      },
      "source": [
        "# our model\n",
        "model = Model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q2FAXOOyRSvi",
        "outputId": "d8e26ed5-1d89-4dcb-a6e8-15fbb12d6116",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (l1): Linear(in_features=8, out_features=6, bias=True)\n",
              "  (l2): Linear(in_features=6, out_features=4, bias=True)\n",
              "  (l3): Linear(in_features=4, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "obDwqOJhRTmB",
        "outputId": "2cf4702b-b0f0-439a-e96d-93f23e69a288",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.BCELoss(size_average=True)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_JnGu3zRW00",
        "outputId": "20fbb6b5-712f-4744-f5e3-5402aad60d02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 25823
        }
      },
      "source": [
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # wrap them in Variable\n",
        "        inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        y_pred = model(inputs)\n",
        "        print(y_pred)\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(y_pred, labels)\n",
        "    \n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.6173],\n",
            "        [0.6178],\n",
            "        [0.6174],\n",
            "        [0.6161],\n",
            "        [0.6170],\n",
            "        [0.6170],\n",
            "        [0.6153],\n",
            "        [0.6182],\n",
            "        [0.6174],\n",
            "        [0.6159],\n",
            "        [0.6170],\n",
            "        [0.6172],\n",
            "        [0.6169],\n",
            "        [0.6173],\n",
            "        [0.6171],\n",
            "        [0.6160],\n",
            "        [0.6178],\n",
            "        [0.6166],\n",
            "        [0.6168],\n",
            "        [0.6161],\n",
            "        [0.6172],\n",
            "        [0.6165],\n",
            "        [0.6167],\n",
            "        [0.6166],\n",
            "        [0.6166],\n",
            "        [0.6164],\n",
            "        [0.6172],\n",
            "        [0.6171],\n",
            "        [0.6179],\n",
            "        [0.6165],\n",
            "        [0.6172],\n",
            "        [0.6166]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6178],\n",
            "        [0.6175],\n",
            "        [0.6176],\n",
            "        [0.6177],\n",
            "        [0.6178],\n",
            "        [0.6173],\n",
            "        [0.6174],\n",
            "        [0.6173],\n",
            "        [0.6171],\n",
            "        [0.6169],\n",
            "        [0.6180],\n",
            "        [0.6173],\n",
            "        [0.6173],\n",
            "        [0.6170],\n",
            "        [0.6174],\n",
            "        [0.6171],\n",
            "        [0.6178],\n",
            "        [0.6173],\n",
            "        [0.6175],\n",
            "        [0.6172],\n",
            "        [0.6174],\n",
            "        [0.6167],\n",
            "        [0.6181],\n",
            "        [0.6171],\n",
            "        [0.6164],\n",
            "        [0.6175],\n",
            "        [0.6173],\n",
            "        [0.6175],\n",
            "        [0.6181],\n",
            "        [0.6176],\n",
            "        [0.6175],\n",
            "        [0.6173]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6241],\n",
            "        [0.6241],\n",
            "        [0.6244],\n",
            "        [0.6233],\n",
            "        [0.6245],\n",
            "        [0.6241],\n",
            "        [0.6250],\n",
            "        [0.6238],\n",
            "        [0.6238],\n",
            "        [0.6240],\n",
            "        [0.6257],\n",
            "        [0.6255],\n",
            "        [0.6241],\n",
            "        [0.6232],\n",
            "        [0.6244],\n",
            "        [0.6246],\n",
            "        [0.6250],\n",
            "        [0.6240],\n",
            "        [0.6251],\n",
            "        [0.6253],\n",
            "        [0.6231],\n",
            "        [0.6247],\n",
            "        [0.6253],\n",
            "        [0.6243],\n",
            "        [0.6249],\n",
            "        [0.6241],\n",
            "        [0.6244],\n",
            "        [0.6252],\n",
            "        [0.6241],\n",
            "        [0.6237],\n",
            "        [0.6248],\n",
            "        [0.6243]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6285],\n",
            "        [0.6280],\n",
            "        [0.6271],\n",
            "        [0.6278],\n",
            "        [0.6274],\n",
            "        [0.6285],\n",
            "        [0.6279],\n",
            "        [0.6284],\n",
            "        [0.6279],\n",
            "        [0.6282],\n",
            "        [0.6276],\n",
            "        [0.6274],\n",
            "        [0.6272],\n",
            "        [0.6273],\n",
            "        [0.6269],\n",
            "        [0.6278],\n",
            "        [0.6277],\n",
            "        [0.6273],\n",
            "        [0.6281],\n",
            "        [0.6271],\n",
            "        [0.6286],\n",
            "        [0.6271],\n",
            "        [0.6286],\n",
            "        [0.6279],\n",
            "        [0.6282],\n",
            "        [0.6271],\n",
            "        [0.6282],\n",
            "        [0.6273],\n",
            "        [0.6284],\n",
            "        [0.6274],\n",
            "        [0.6284],\n",
            "        [0.6279]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6294],\n",
            "        [0.6294],\n",
            "        [0.6287],\n",
            "        [0.6291],\n",
            "        [0.6299],\n",
            "        [0.6302],\n",
            "        [0.6292],\n",
            "        [0.6291],\n",
            "        [0.6281],\n",
            "        [0.6285],\n",
            "        [0.6281],\n",
            "        [0.6287],\n",
            "        [0.6281],\n",
            "        [0.6289],\n",
            "        [0.6292],\n",
            "        [0.6287],\n",
            "        [0.6286],\n",
            "        [0.6292],\n",
            "        [0.6288],\n",
            "        [0.6287],\n",
            "        [0.6295],\n",
            "        [0.6282],\n",
            "        [0.6283],\n",
            "        [0.6289],\n",
            "        [0.6290],\n",
            "        [0.6292],\n",
            "        [0.6285],\n",
            "        [0.6295],\n",
            "        [0.6290],\n",
            "        [0.6291],\n",
            "        [0.6289],\n",
            "        [0.6287]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6245],\n",
            "        [0.6269],\n",
            "        [0.6261],\n",
            "        [0.6261],\n",
            "        [0.6268],\n",
            "        [0.6255],\n",
            "        [0.6266],\n",
            "        [0.6270],\n",
            "        [0.6271],\n",
            "        [0.6258],\n",
            "        [0.6266],\n",
            "        [0.6261],\n",
            "        [0.6271],\n",
            "        [0.6263],\n",
            "        [0.6274],\n",
            "        [0.6267],\n",
            "        [0.6263],\n",
            "        [0.6261],\n",
            "        [0.6262],\n",
            "        [0.6266],\n",
            "        [0.6273],\n",
            "        [0.6264],\n",
            "        [0.6268],\n",
            "        [0.6266],\n",
            "        [0.6263],\n",
            "        [0.6264],\n",
            "        [0.6263],\n",
            "        [0.6263],\n",
            "        [0.6260],\n",
            "        [0.6275],\n",
            "        [0.6271],\n",
            "        [0.6267]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6284],\n",
            "        [0.6289],\n",
            "        [0.6285],\n",
            "        [0.6278],\n",
            "        [0.6289],\n",
            "        [0.6284],\n",
            "        [0.6294],\n",
            "        [0.6282],\n",
            "        [0.6283],\n",
            "        [0.6283],\n",
            "        [0.6290],\n",
            "        [0.6282],\n",
            "        [0.6290],\n",
            "        [0.6289],\n",
            "        [0.6278],\n",
            "        [0.6297],\n",
            "        [0.6277],\n",
            "        [0.6283],\n",
            "        [0.6295],\n",
            "        [0.6287],\n",
            "        [0.6292],\n",
            "        [0.6286],\n",
            "        [0.6277],\n",
            "        [0.6291],\n",
            "        [0.6289],\n",
            "        [0.6288],\n",
            "        [0.6287],\n",
            "        [0.6283],\n",
            "        [0.6281],\n",
            "        [0.6290],\n",
            "        [0.6288],\n",
            "        [0.6280]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6234],\n",
            "        [0.6241],\n",
            "        [0.6247],\n",
            "        [0.6240],\n",
            "        [0.6232],\n",
            "        [0.6247],\n",
            "        [0.6244],\n",
            "        [0.6246],\n",
            "        [0.6239],\n",
            "        [0.6229],\n",
            "        [0.6237],\n",
            "        [0.6238],\n",
            "        [0.6243],\n",
            "        [0.6237],\n",
            "        [0.6241],\n",
            "        [0.6239],\n",
            "        [0.6241],\n",
            "        [0.6241],\n",
            "        [0.6239],\n",
            "        [0.6239],\n",
            "        [0.6238],\n",
            "        [0.6245],\n",
            "        [0.6239],\n",
            "        [0.6238],\n",
            "        [0.6245],\n",
            "        [0.6248],\n",
            "        [0.6241],\n",
            "        [0.6243],\n",
            "        [0.6233],\n",
            "        [0.6244],\n",
            "        [0.6240],\n",
            "        [0.6244]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6251],\n",
            "        [0.6257],\n",
            "        [0.6252],\n",
            "        [0.6260],\n",
            "        [0.6264],\n",
            "        [0.6265],\n",
            "        [0.6260],\n",
            "        [0.6269],\n",
            "        [0.6267],\n",
            "        [0.6272],\n",
            "        [0.6257],\n",
            "        [0.6263],\n",
            "        [0.6259],\n",
            "        [0.6257],\n",
            "        [0.6263],\n",
            "        [0.6261],\n",
            "        [0.6261],\n",
            "        [0.6257],\n",
            "        [0.6256],\n",
            "        [0.6263],\n",
            "        [0.6261],\n",
            "        [0.6263],\n",
            "        [0.6262],\n",
            "        [0.6267],\n",
            "        [0.6270],\n",
            "        [0.6263],\n",
            "        [0.6275],\n",
            "        [0.6269],\n",
            "        [0.6251],\n",
            "        [0.6261],\n",
            "        [0.6267],\n",
            "        [0.6257]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6228],\n",
            "        [0.6230],\n",
            "        [0.6236],\n",
            "        [0.6231],\n",
            "        [0.6225],\n",
            "        [0.6233],\n",
            "        [0.6225],\n",
            "        [0.6222],\n",
            "        [0.6225],\n",
            "        [0.6222],\n",
            "        [0.6232],\n",
            "        [0.6222],\n",
            "        [0.6227],\n",
            "        [0.6221],\n",
            "        [0.6224],\n",
            "        [0.6235],\n",
            "        [0.6236],\n",
            "        [0.6225],\n",
            "        [0.6237],\n",
            "        [0.6232],\n",
            "        [0.6229],\n",
            "        [0.6217],\n",
            "        [0.6231],\n",
            "        [0.6231],\n",
            "        [0.6224],\n",
            "        [0.6219],\n",
            "        [0.6229],\n",
            "        [0.6219],\n",
            "        [0.6224],\n",
            "        [0.6232],\n",
            "        [0.6226],\n",
            "        [0.6236]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6272],\n",
            "        [0.6267],\n",
            "        [0.6274],\n",
            "        [0.6281],\n",
            "        [0.6278],\n",
            "        [0.6279],\n",
            "        [0.6273],\n",
            "        [0.6281],\n",
            "        [0.6280],\n",
            "        [0.6278],\n",
            "        [0.6272],\n",
            "        [0.6278],\n",
            "        [0.6272],\n",
            "        [0.6273],\n",
            "        [0.6281],\n",
            "        [0.6277],\n",
            "        [0.6277],\n",
            "        [0.6272],\n",
            "        [0.6275],\n",
            "        [0.6275],\n",
            "        [0.6277],\n",
            "        [0.6271],\n",
            "        [0.6269],\n",
            "        [0.6268],\n",
            "        [0.6280],\n",
            "        [0.6275],\n",
            "        [0.6275],\n",
            "        [0.6279],\n",
            "        [0.6273],\n",
            "        [0.6262],\n",
            "        [0.6277],\n",
            "        [0.6283]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6295],\n",
            "        [0.6289],\n",
            "        [0.6300],\n",
            "        [0.6298],\n",
            "        [0.6296],\n",
            "        [0.6293],\n",
            "        [0.6293],\n",
            "        [0.6295],\n",
            "        [0.6301],\n",
            "        [0.6302],\n",
            "        [0.6286],\n",
            "        [0.6294],\n",
            "        [0.6286],\n",
            "        [0.6291],\n",
            "        [0.6298],\n",
            "        [0.6294],\n",
            "        [0.6291],\n",
            "        [0.6299],\n",
            "        [0.6301],\n",
            "        [0.6301],\n",
            "        [0.6302],\n",
            "        [0.6299],\n",
            "        [0.6291],\n",
            "        [0.6299],\n",
            "        [0.6295],\n",
            "        [0.6301],\n",
            "        [0.6296],\n",
            "        [0.6305],\n",
            "        [0.6287],\n",
            "        [0.6291],\n",
            "        [0.6294],\n",
            "        [0.6291]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6227],\n",
            "        [0.6228],\n",
            "        [0.6227],\n",
            "        [0.6228],\n",
            "        [0.6225],\n",
            "        [0.6220],\n",
            "        [0.6224],\n",
            "        [0.6229],\n",
            "        [0.6227],\n",
            "        [0.6231],\n",
            "        [0.6228],\n",
            "        [0.6233],\n",
            "        [0.6238],\n",
            "        [0.6227],\n",
            "        [0.6231],\n",
            "        [0.6225],\n",
            "        [0.6229],\n",
            "        [0.6220],\n",
            "        [0.6239],\n",
            "        [0.6225],\n",
            "        [0.6227],\n",
            "        [0.6227],\n",
            "        [0.6210],\n",
            "        [0.6235],\n",
            "        [0.6225],\n",
            "        [0.6222],\n",
            "        [0.6237],\n",
            "        [0.6219],\n",
            "        [0.6221],\n",
            "        [0.6233],\n",
            "        [0.6225],\n",
            "        [0.6223]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6225],\n",
            "        [0.6227],\n",
            "        [0.6227],\n",
            "        [0.6232],\n",
            "        [0.6230],\n",
            "        [0.6235],\n",
            "        [0.6228],\n",
            "        [0.6228],\n",
            "        [0.6234],\n",
            "        [0.6235],\n",
            "        [0.6228],\n",
            "        [0.6235],\n",
            "        [0.6221],\n",
            "        [0.6231],\n",
            "        [0.6235],\n",
            "        [0.6229],\n",
            "        [0.6233],\n",
            "        [0.6216],\n",
            "        [0.6236],\n",
            "        [0.6229],\n",
            "        [0.6233],\n",
            "        [0.6227],\n",
            "        [0.6223],\n",
            "        [0.6227],\n",
            "        [0.6229],\n",
            "        [0.6230],\n",
            "        [0.6221],\n",
            "        [0.6231],\n",
            "        [0.6232],\n",
            "        [0.6220],\n",
            "        [0.6224],\n",
            "        [0.6217]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6216],\n",
            "        [0.6209],\n",
            "        [0.6209],\n",
            "        [0.6217],\n",
            "        [0.6213],\n",
            "        [0.6217],\n",
            "        [0.6210],\n",
            "        [0.6215],\n",
            "        [0.6214],\n",
            "        [0.6212],\n",
            "        [0.6215],\n",
            "        [0.6212],\n",
            "        [0.6217],\n",
            "        [0.6221],\n",
            "        [0.6210],\n",
            "        [0.6210],\n",
            "        [0.6223],\n",
            "        [0.6215],\n",
            "        [0.6219],\n",
            "        [0.6216],\n",
            "        [0.6213],\n",
            "        [0.6210],\n",
            "        [0.6219],\n",
            "        [0.6215],\n",
            "        [0.6210],\n",
            "        [0.6216],\n",
            "        [0.6210],\n",
            "        [0.6219],\n",
            "        [0.6223],\n",
            "        [0.6216],\n",
            "        [0.6210],\n",
            "        [0.6224]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6188],\n",
            "        [0.6187],\n",
            "        [0.6187],\n",
            "        [0.6192],\n",
            "        [0.6196],\n",
            "        [0.6193],\n",
            "        [0.6200],\n",
            "        [0.6189],\n",
            "        [0.6201],\n",
            "        [0.6201],\n",
            "        [0.6198],\n",
            "        [0.6195],\n",
            "        [0.6189],\n",
            "        [0.6192],\n",
            "        [0.6196],\n",
            "        [0.6203],\n",
            "        [0.6189],\n",
            "        [0.6191],\n",
            "        [0.6189],\n",
            "        [0.6187],\n",
            "        [0.6199],\n",
            "        [0.6202],\n",
            "        [0.6198],\n",
            "        [0.6200],\n",
            "        [0.6192],\n",
            "        [0.6198],\n",
            "        [0.6199],\n",
            "        [0.6190],\n",
            "        [0.6199],\n",
            "        [0.6189],\n",
            "        [0.6201],\n",
            "        [0.6188]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6201],\n",
            "        [0.6199],\n",
            "        [0.6204],\n",
            "        [0.6197],\n",
            "        [0.6196],\n",
            "        [0.6190],\n",
            "        [0.6193],\n",
            "        [0.6191],\n",
            "        [0.6191],\n",
            "        [0.6202],\n",
            "        [0.6198],\n",
            "        [0.6201],\n",
            "        [0.6206],\n",
            "        [0.6198],\n",
            "        [0.6194],\n",
            "        [0.6200],\n",
            "        [0.6204],\n",
            "        [0.6204],\n",
            "        [0.6203],\n",
            "        [0.6188],\n",
            "        [0.6195],\n",
            "        [0.6205],\n",
            "        [0.6202],\n",
            "        [0.6202],\n",
            "        [0.6195],\n",
            "        [0.6195],\n",
            "        [0.6202],\n",
            "        [0.6194],\n",
            "        [0.6201],\n",
            "        [0.6193],\n",
            "        [0.6198],\n",
            "        [0.6203]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6271],\n",
            "        [0.6272],\n",
            "        [0.6274],\n",
            "        [0.6268],\n",
            "        [0.6272],\n",
            "        [0.6271],\n",
            "        [0.6268],\n",
            "        [0.6267],\n",
            "        [0.6266],\n",
            "        [0.6261],\n",
            "        [0.6271],\n",
            "        [0.6274],\n",
            "        [0.6268],\n",
            "        [0.6270],\n",
            "        [0.6269],\n",
            "        [0.6272],\n",
            "        [0.6267],\n",
            "        [0.6266],\n",
            "        [0.6264],\n",
            "        [0.6262],\n",
            "        [0.6271],\n",
            "        [0.6271],\n",
            "        [0.6269],\n",
            "        [0.6267],\n",
            "        [0.6275],\n",
            "        [0.6275],\n",
            "        [0.6266],\n",
            "        [0.6263],\n",
            "        [0.6268],\n",
            "        [0.6275],\n",
            "        [0.6269],\n",
            "        [0.6272]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6250],\n",
            "        [0.6246],\n",
            "        [0.6259],\n",
            "        [0.6256],\n",
            "        [0.6263],\n",
            "        [0.6251],\n",
            "        [0.6251],\n",
            "        [0.6260],\n",
            "        [0.6253],\n",
            "        [0.6259],\n",
            "        [0.6246],\n",
            "        [0.6254],\n",
            "        [0.6261],\n",
            "        [0.6255],\n",
            "        [0.6260],\n",
            "        [0.6254],\n",
            "        [0.6261],\n",
            "        [0.6249],\n",
            "        [0.6246],\n",
            "        [0.6253],\n",
            "        [0.6261],\n",
            "        [0.6257],\n",
            "        [0.6252],\n",
            "        [0.6256],\n",
            "        [0.6262],\n",
            "        [0.6252],\n",
            "        [0.6251],\n",
            "        [0.6258],\n",
            "        [0.6258],\n",
            "        [0.6254],\n",
            "        [0.6246],\n",
            "        [0.6256]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6330],\n",
            "        [0.6329],\n",
            "        [0.6340],\n",
            "        [0.6340],\n",
            "        [0.6334],\n",
            "        [0.6339],\n",
            "        [0.6322],\n",
            "        [0.6339],\n",
            "        [0.6327],\n",
            "        [0.6322],\n",
            "        [0.6334],\n",
            "        [0.6329],\n",
            "        [0.6342],\n",
            "        [0.6336],\n",
            "        [0.6338],\n",
            "        [0.6339],\n",
            "        [0.6331],\n",
            "        [0.6338],\n",
            "        [0.6332],\n",
            "        [0.6336],\n",
            "        [0.6335],\n",
            "        [0.6331],\n",
            "        [0.6328],\n",
            "        [0.6331],\n",
            "        [0.6338],\n",
            "        [0.6329],\n",
            "        [0.6343],\n",
            "        [0.6337],\n",
            "        [0.6328],\n",
            "        [0.6336],\n",
            "        [0.6329],\n",
            "        [0.6331]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6324],\n",
            "        [0.6321],\n",
            "        [0.6312],\n",
            "        [0.6305],\n",
            "        [0.6321],\n",
            "        [0.6316],\n",
            "        [0.6320],\n",
            "        [0.6326],\n",
            "        [0.6319],\n",
            "        [0.6314],\n",
            "        [0.6316],\n",
            "        [0.6318],\n",
            "        [0.6309],\n",
            "        [0.6317],\n",
            "        [0.6313],\n",
            "        [0.6319],\n",
            "        [0.6326],\n",
            "        [0.6322],\n",
            "        [0.6322],\n",
            "        [0.6323],\n",
            "        [0.6314],\n",
            "        [0.6322],\n",
            "        [0.6321],\n",
            "        [0.6323],\n",
            "        [0.6327],\n",
            "        [0.6318],\n",
            "        [0.6315],\n",
            "        [0.6316],\n",
            "        [0.6320],\n",
            "        [0.6319],\n",
            "        [0.6318],\n",
            "        [0.6314]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6334],\n",
            "        [0.6340],\n",
            "        [0.6334],\n",
            "        [0.6332],\n",
            "        [0.6343],\n",
            "        [0.6347],\n",
            "        [0.6342],\n",
            "        [0.6335],\n",
            "        [0.6338],\n",
            "        [0.6340],\n",
            "        [0.6337],\n",
            "        [0.6344],\n",
            "        [0.6342],\n",
            "        [0.6343],\n",
            "        [0.6342],\n",
            "        [0.6343],\n",
            "        [0.6343],\n",
            "        [0.6336],\n",
            "        [0.6348],\n",
            "        [0.6343],\n",
            "        [0.6341],\n",
            "        [0.6337],\n",
            "        [0.6353],\n",
            "        [0.6345],\n",
            "        [0.6342],\n",
            "        [0.6332],\n",
            "        [0.6345],\n",
            "        [0.6335],\n",
            "        [0.6349],\n",
            "        [0.6336],\n",
            "        [0.6342],\n",
            "        [0.6346]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6374],\n",
            "        [0.6368],\n",
            "        [0.6366],\n",
            "        [0.6365],\n",
            "        [0.6367],\n",
            "        [0.6373],\n",
            "        [0.6371],\n",
            "        [0.6371],\n",
            "        [0.6367],\n",
            "        [0.6374],\n",
            "        [0.6365],\n",
            "        [0.6375],\n",
            "        [0.6373],\n",
            "        [0.6364],\n",
            "        [0.6367],\n",
            "        [0.6371],\n",
            "        [0.6368],\n",
            "        [0.6379],\n",
            "        [0.6366],\n",
            "        [0.6376],\n",
            "        [0.6372],\n",
            "        [0.6378],\n",
            "        [0.6370],\n",
            "        [0.6363],\n",
            "        [0.6377],\n",
            "        [0.6362],\n",
            "        [0.6364],\n",
            "        [0.6374],\n",
            "        [0.6380],\n",
            "        [0.6373],\n",
            "        [0.6377],\n",
            "        [0.6377]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6398],\n",
            "        [0.6400],\n",
            "        [0.6400],\n",
            "        [0.6396],\n",
            "        [0.6400],\n",
            "        [0.6384],\n",
            "        [0.6396],\n",
            "        [0.6408],\n",
            "        [0.6407],\n",
            "        [0.6401],\n",
            "        [0.6403],\n",
            "        [0.6409],\n",
            "        [0.6402],\n",
            "        [0.6402],\n",
            "        [0.6407],\n",
            "        [0.6410],\n",
            "        [0.6403],\n",
            "        [0.6406],\n",
            "        [0.6398],\n",
            "        [0.6398],\n",
            "        [0.6397],\n",
            "        [0.6402],\n",
            "        [0.6391]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6404],\n",
            "        [0.6406],\n",
            "        [0.6409],\n",
            "        [0.6408],\n",
            "        [0.6405],\n",
            "        [0.6399],\n",
            "        [0.6406],\n",
            "        [0.6397],\n",
            "        [0.6414],\n",
            "        [0.6405],\n",
            "        [0.6408],\n",
            "        [0.6407],\n",
            "        [0.6405],\n",
            "        [0.6406],\n",
            "        [0.6398],\n",
            "        [0.6395],\n",
            "        [0.6397],\n",
            "        [0.6412],\n",
            "        [0.6411],\n",
            "        [0.6407],\n",
            "        [0.6410],\n",
            "        [0.6395],\n",
            "        [0.6406],\n",
            "        [0.6407],\n",
            "        [0.6393],\n",
            "        [0.6395],\n",
            "        [0.6404],\n",
            "        [0.6404],\n",
            "        [0.6403],\n",
            "        [0.6395],\n",
            "        [0.6393],\n",
            "        [0.6405]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6430],\n",
            "        [0.6422],\n",
            "        [0.6415],\n",
            "        [0.6418],\n",
            "        [0.6420],\n",
            "        [0.6421],\n",
            "        [0.6422],\n",
            "        [0.6416],\n",
            "        [0.6425],\n",
            "        [0.6424],\n",
            "        [0.6432],\n",
            "        [0.6421],\n",
            "        [0.6423],\n",
            "        [0.6421],\n",
            "        [0.6428],\n",
            "        [0.6427],\n",
            "        [0.6423],\n",
            "        [0.6425],\n",
            "        [0.6428],\n",
            "        [0.6420],\n",
            "        [0.6426],\n",
            "        [0.6414],\n",
            "        [0.6423],\n",
            "        [0.6425],\n",
            "        [0.6425],\n",
            "        [0.6417],\n",
            "        [0.6416],\n",
            "        [0.6413],\n",
            "        [0.6422],\n",
            "        [0.6413],\n",
            "        [0.6405],\n",
            "        [0.6415]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6468],\n",
            "        [0.6471],\n",
            "        [0.6477],\n",
            "        [0.6474],\n",
            "        [0.6467],\n",
            "        [0.6476],\n",
            "        [0.6461],\n",
            "        [0.6466],\n",
            "        [0.6469],\n",
            "        [0.6469],\n",
            "        [0.6468],\n",
            "        [0.6477],\n",
            "        [0.6476],\n",
            "        [0.6476],\n",
            "        [0.6462],\n",
            "        [0.6476],\n",
            "        [0.6471],\n",
            "        [0.6470],\n",
            "        [0.6470],\n",
            "        [0.6467],\n",
            "        [0.6470],\n",
            "        [0.6476],\n",
            "        [0.6470],\n",
            "        [0.6475],\n",
            "        [0.6480],\n",
            "        [0.6478],\n",
            "        [0.6481],\n",
            "        [0.6467],\n",
            "        [0.6469],\n",
            "        [0.6476],\n",
            "        [0.6471],\n",
            "        [0.6466]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6520],\n",
            "        [0.6516],\n",
            "        [0.6516],\n",
            "        [0.6517],\n",
            "        [0.6527],\n",
            "        [0.6524],\n",
            "        [0.6509],\n",
            "        [0.6521],\n",
            "        [0.6516],\n",
            "        [0.6513],\n",
            "        [0.6518],\n",
            "        [0.6521],\n",
            "        [0.6517],\n",
            "        [0.6513],\n",
            "        [0.6521],\n",
            "        [0.6513],\n",
            "        [0.6519],\n",
            "        [0.6517],\n",
            "        [0.6526],\n",
            "        [0.6501],\n",
            "        [0.6516],\n",
            "        [0.6520],\n",
            "        [0.6528],\n",
            "        [0.6509],\n",
            "        [0.6523],\n",
            "        [0.6527],\n",
            "        [0.6522],\n",
            "        [0.6517],\n",
            "        [0.6524],\n",
            "        [0.6520],\n",
            "        [0.6509],\n",
            "        [0.6512]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6520],\n",
            "        [0.6516],\n",
            "        [0.6521],\n",
            "        [0.6517],\n",
            "        [0.6526],\n",
            "        [0.6524],\n",
            "        [0.6526],\n",
            "        [0.6518],\n",
            "        [0.6522],\n",
            "        [0.6517],\n",
            "        [0.6519],\n",
            "        [0.6522],\n",
            "        [0.6526],\n",
            "        [0.6516],\n",
            "        [0.6523],\n",
            "        [0.6520],\n",
            "        [0.6518],\n",
            "        [0.6514],\n",
            "        [0.6525],\n",
            "        [0.6509],\n",
            "        [0.6518],\n",
            "        [0.6523],\n",
            "        [0.6521],\n",
            "        [0.6520],\n",
            "        [0.6518],\n",
            "        [0.6519],\n",
            "        [0.6508],\n",
            "        [0.6518],\n",
            "        [0.6523],\n",
            "        [0.6525],\n",
            "        [0.6521],\n",
            "        [0.6516]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6530],\n",
            "        [0.6535],\n",
            "        [0.6532],\n",
            "        [0.6527],\n",
            "        [0.6540],\n",
            "        [0.6530],\n",
            "        [0.6528],\n",
            "        [0.6539],\n",
            "        [0.6533],\n",
            "        [0.6537],\n",
            "        [0.6534],\n",
            "        [0.6533],\n",
            "        [0.6535],\n",
            "        [0.6534],\n",
            "        [0.6528],\n",
            "        [0.6527],\n",
            "        [0.6522],\n",
            "        [0.6533],\n",
            "        [0.6539],\n",
            "        [0.6536],\n",
            "        [0.6525],\n",
            "        [0.6536],\n",
            "        [0.6529],\n",
            "        [0.6535],\n",
            "        [0.6519],\n",
            "        [0.6534],\n",
            "        [0.6531],\n",
            "        [0.6527],\n",
            "        [0.6527],\n",
            "        [0.6531],\n",
            "        [0.6532],\n",
            "        [0.6532]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6534],\n",
            "        [0.6537],\n",
            "        [0.6537],\n",
            "        [0.6538],\n",
            "        [0.6533],\n",
            "        [0.6540],\n",
            "        [0.6530],\n",
            "        [0.6536],\n",
            "        [0.6529],\n",
            "        [0.6527],\n",
            "        [0.6536],\n",
            "        [0.6535],\n",
            "        [0.6530],\n",
            "        [0.6535],\n",
            "        [0.6534],\n",
            "        [0.6530],\n",
            "        [0.6530],\n",
            "        [0.6525],\n",
            "        [0.6540],\n",
            "        [0.6532],\n",
            "        [0.6534],\n",
            "        [0.6528],\n",
            "        [0.6539],\n",
            "        [0.6541],\n",
            "        [0.6535],\n",
            "        [0.6532],\n",
            "        [0.6540],\n",
            "        [0.6534],\n",
            "        [0.6533],\n",
            "        [0.6534],\n",
            "        [0.6532],\n",
            "        [0.6529]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6507],\n",
            "        [0.6506],\n",
            "        [0.6510],\n",
            "        [0.6513],\n",
            "        [0.6514],\n",
            "        [0.6508],\n",
            "        [0.6509],\n",
            "        [0.6515],\n",
            "        [0.6515],\n",
            "        [0.6507],\n",
            "        [0.6515],\n",
            "        [0.6507],\n",
            "        [0.6509],\n",
            "        [0.6505],\n",
            "        [0.6509],\n",
            "        [0.6510],\n",
            "        [0.6518],\n",
            "        [0.6519],\n",
            "        [0.6507],\n",
            "        [0.6510],\n",
            "        [0.6517],\n",
            "        [0.6510],\n",
            "        [0.6516],\n",
            "        [0.6508],\n",
            "        [0.6503],\n",
            "        [0.6522],\n",
            "        [0.6509],\n",
            "        [0.6513],\n",
            "        [0.6513],\n",
            "        [0.6513],\n",
            "        [0.6500],\n",
            "        [0.6508]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6512],\n",
            "        [0.6513],\n",
            "        [0.6519],\n",
            "        [0.6505],\n",
            "        [0.6512],\n",
            "        [0.6514],\n",
            "        [0.6511],\n",
            "        [0.6517],\n",
            "        [0.6511],\n",
            "        [0.6521],\n",
            "        [0.6512],\n",
            "        [0.6520],\n",
            "        [0.6507],\n",
            "        [0.6511],\n",
            "        [0.6507],\n",
            "        [0.6519],\n",
            "        [0.6506],\n",
            "        [0.6522],\n",
            "        [0.6520],\n",
            "        [0.6513],\n",
            "        [0.6518],\n",
            "        [0.6512],\n",
            "        [0.6519],\n",
            "        [0.6508],\n",
            "        [0.6513],\n",
            "        [0.6521],\n",
            "        [0.6518],\n",
            "        [0.6517],\n",
            "        [0.6496],\n",
            "        [0.6516],\n",
            "        [0.6510],\n",
            "        [0.6515]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6444],\n",
            "        [0.6440],\n",
            "        [0.6437],\n",
            "        [0.6439],\n",
            "        [0.6438],\n",
            "        [0.6437],\n",
            "        [0.6437],\n",
            "        [0.6440],\n",
            "        [0.6440],\n",
            "        [0.6441],\n",
            "        [0.6436],\n",
            "        [0.6436],\n",
            "        [0.6441],\n",
            "        [0.6439],\n",
            "        [0.6443],\n",
            "        [0.6440],\n",
            "        [0.6435],\n",
            "        [0.6440],\n",
            "        [0.6438],\n",
            "        [0.6445],\n",
            "        [0.6436],\n",
            "        [0.6443],\n",
            "        [0.6437],\n",
            "        [0.6441],\n",
            "        [0.6441],\n",
            "        [0.6428],\n",
            "        [0.6441],\n",
            "        [0.6441],\n",
            "        [0.6434],\n",
            "        [0.6451],\n",
            "        [0.6445],\n",
            "        [0.6434]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6482],\n",
            "        [0.6479],\n",
            "        [0.6476],\n",
            "        [0.6481],\n",
            "        [0.6473],\n",
            "        [0.6474],\n",
            "        [0.6480],\n",
            "        [0.6475],\n",
            "        [0.6481],\n",
            "        [0.6467],\n",
            "        [0.6469],\n",
            "        [0.6472],\n",
            "        [0.6477],\n",
            "        [0.6470],\n",
            "        [0.6474],\n",
            "        [0.6482],\n",
            "        [0.6477],\n",
            "        [0.6475],\n",
            "        [0.6484],\n",
            "        [0.6472],\n",
            "        [0.6488],\n",
            "        [0.6471],\n",
            "        [0.6479],\n",
            "        [0.6481],\n",
            "        [0.6471],\n",
            "        [0.6478],\n",
            "        [0.6475],\n",
            "        [0.6477],\n",
            "        [0.6480],\n",
            "        [0.6480],\n",
            "        [0.6474],\n",
            "        [0.6473]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6484],\n",
            "        [0.6482],\n",
            "        [0.6467],\n",
            "        [0.6479],\n",
            "        [0.6473],\n",
            "        [0.6484],\n",
            "        [0.6477],\n",
            "        [0.6483],\n",
            "        [0.6480],\n",
            "        [0.6476],\n",
            "        [0.6486],\n",
            "        [0.6482],\n",
            "        [0.6482],\n",
            "        [0.6485],\n",
            "        [0.6476],\n",
            "        [0.6478],\n",
            "        [0.6466],\n",
            "        [0.6476],\n",
            "        [0.6474],\n",
            "        [0.6483],\n",
            "        [0.6487],\n",
            "        [0.6472],\n",
            "        [0.6477],\n",
            "        [0.6480],\n",
            "        [0.6478],\n",
            "        [0.6477],\n",
            "        [0.6484],\n",
            "        [0.6482],\n",
            "        [0.6479],\n",
            "        [0.6471],\n",
            "        [0.6472],\n",
            "        [0.6474]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6505],\n",
            "        [0.6487],\n",
            "        [0.6492],\n",
            "        [0.6486],\n",
            "        [0.6493],\n",
            "        [0.6491],\n",
            "        [0.6487],\n",
            "        [0.6499],\n",
            "        [0.6483],\n",
            "        [0.6496],\n",
            "        [0.6485],\n",
            "        [0.6495],\n",
            "        [0.6495],\n",
            "        [0.6501],\n",
            "        [0.6498],\n",
            "        [0.6496],\n",
            "        [0.6487],\n",
            "        [0.6485],\n",
            "        [0.6493],\n",
            "        [0.6492],\n",
            "        [0.6500],\n",
            "        [0.6490],\n",
            "        [0.6492],\n",
            "        [0.6485],\n",
            "        [0.6501],\n",
            "        [0.6489],\n",
            "        [0.6493],\n",
            "        [0.6482],\n",
            "        [0.6494],\n",
            "        [0.6495],\n",
            "        [0.6501],\n",
            "        [0.6485]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6491],\n",
            "        [0.6490],\n",
            "        [0.6487],\n",
            "        [0.6490],\n",
            "        [0.6501],\n",
            "        [0.6499],\n",
            "        [0.6486],\n",
            "        [0.6497],\n",
            "        [0.6491],\n",
            "        [0.6500],\n",
            "        [0.6503],\n",
            "        [0.6502],\n",
            "        [0.6502],\n",
            "        [0.6498],\n",
            "        [0.6492],\n",
            "        [0.6496],\n",
            "        [0.6487],\n",
            "        [0.6491],\n",
            "        [0.6492],\n",
            "        [0.6489],\n",
            "        [0.6486],\n",
            "        [0.6499],\n",
            "        [0.6496],\n",
            "        [0.6494],\n",
            "        [0.6490],\n",
            "        [0.6502],\n",
            "        [0.6499],\n",
            "        [0.6492],\n",
            "        [0.6505],\n",
            "        [0.6502],\n",
            "        [0.6497],\n",
            "        [0.6502]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6506],\n",
            "        [0.6496],\n",
            "        [0.6511],\n",
            "        [0.6505],\n",
            "        [0.6520],\n",
            "        [0.6509],\n",
            "        [0.6503],\n",
            "        [0.6511],\n",
            "        [0.6496],\n",
            "        [0.6502],\n",
            "        [0.6511],\n",
            "        [0.6509],\n",
            "        [0.6509],\n",
            "        [0.6512],\n",
            "        [0.6514],\n",
            "        [0.6508],\n",
            "        [0.6506],\n",
            "        [0.6511],\n",
            "        [0.6512],\n",
            "        [0.6506],\n",
            "        [0.6506],\n",
            "        [0.6504],\n",
            "        [0.6508],\n",
            "        [0.6505],\n",
            "        [0.6513],\n",
            "        [0.6505],\n",
            "        [0.6510],\n",
            "        [0.6510],\n",
            "        [0.6507],\n",
            "        [0.6511],\n",
            "        [0.6512],\n",
            "        [0.6505]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6497],\n",
            "        [0.6496],\n",
            "        [0.6498],\n",
            "        [0.6495],\n",
            "        [0.6497],\n",
            "        [0.6502],\n",
            "        [0.6496],\n",
            "        [0.6507],\n",
            "        [0.6503],\n",
            "        [0.6495],\n",
            "        [0.6493],\n",
            "        [0.6494],\n",
            "        [0.6494],\n",
            "        [0.6507],\n",
            "        [0.6505],\n",
            "        [0.6506],\n",
            "        [0.6500],\n",
            "        [0.6500],\n",
            "        [0.6505],\n",
            "        [0.6496],\n",
            "        [0.6502],\n",
            "        [0.6499],\n",
            "        [0.6496],\n",
            "        [0.6494],\n",
            "        [0.6501],\n",
            "        [0.6502],\n",
            "        [0.6497],\n",
            "        [0.6503],\n",
            "        [0.6505],\n",
            "        [0.6500],\n",
            "        [0.6507],\n",
            "        [0.6499]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6455],\n",
            "        [0.6454],\n",
            "        [0.6463],\n",
            "        [0.6458],\n",
            "        [0.6470],\n",
            "        [0.6454],\n",
            "        [0.6448],\n",
            "        [0.6447],\n",
            "        [0.6450],\n",
            "        [0.6450],\n",
            "        [0.6456],\n",
            "        [0.6462],\n",
            "        [0.6456],\n",
            "        [0.6461],\n",
            "        [0.6463],\n",
            "        [0.6459],\n",
            "        [0.6453],\n",
            "        [0.6457],\n",
            "        [0.6453],\n",
            "        [0.6464],\n",
            "        [0.6459],\n",
            "        [0.6459],\n",
            "        [0.6451],\n",
            "        [0.6462],\n",
            "        [0.6458],\n",
            "        [0.6459],\n",
            "        [0.6465],\n",
            "        [0.6462],\n",
            "        [0.6455],\n",
            "        [0.6462],\n",
            "        [0.6455],\n",
            "        [0.6451]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6457],\n",
            "        [0.6456],\n",
            "        [0.6461],\n",
            "        [0.6463],\n",
            "        [0.6468],\n",
            "        [0.6456],\n",
            "        [0.6464],\n",
            "        [0.6464],\n",
            "        [0.6463],\n",
            "        [0.6449],\n",
            "        [0.6469],\n",
            "        [0.6453],\n",
            "        [0.6465],\n",
            "        [0.6462],\n",
            "        [0.6463],\n",
            "        [0.6466],\n",
            "        [0.6467],\n",
            "        [0.6467],\n",
            "        [0.6455],\n",
            "        [0.6452],\n",
            "        [0.6462],\n",
            "        [0.6473],\n",
            "        [0.6462],\n",
            "        [0.6458],\n",
            "        [0.6471],\n",
            "        [0.6455],\n",
            "        [0.6458],\n",
            "        [0.6457],\n",
            "        [0.6463],\n",
            "        [0.6459],\n",
            "        [0.6459],\n",
            "        [0.6468]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6503],\n",
            "        [0.6484],\n",
            "        [0.6497],\n",
            "        [0.6488],\n",
            "        [0.6499],\n",
            "        [0.6497],\n",
            "        [0.6495],\n",
            "        [0.6502],\n",
            "        [0.6494],\n",
            "        [0.6504],\n",
            "        [0.6501],\n",
            "        [0.6505],\n",
            "        [0.6488],\n",
            "        [0.6494],\n",
            "        [0.6504],\n",
            "        [0.6500],\n",
            "        [0.6495],\n",
            "        [0.6507],\n",
            "        [0.6499],\n",
            "        [0.6496],\n",
            "        [0.6499],\n",
            "        [0.6494],\n",
            "        [0.6498],\n",
            "        [0.6504],\n",
            "        [0.6494],\n",
            "        [0.6496],\n",
            "        [0.6488],\n",
            "        [0.6501],\n",
            "        [0.6507],\n",
            "        [0.6498],\n",
            "        [0.6504],\n",
            "        [0.6497]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6478],\n",
            "        [0.6485],\n",
            "        [0.6475],\n",
            "        [0.6474],\n",
            "        [0.6478],\n",
            "        [0.6479],\n",
            "        [0.6485],\n",
            "        [0.6482],\n",
            "        [0.6473],\n",
            "        [0.6471],\n",
            "        [0.6473],\n",
            "        [0.6479],\n",
            "        [0.6472],\n",
            "        [0.6480],\n",
            "        [0.6476],\n",
            "        [0.6477],\n",
            "        [0.6477],\n",
            "        [0.6474],\n",
            "        [0.6471],\n",
            "        [0.6466],\n",
            "        [0.6482],\n",
            "        [0.6480],\n",
            "        [0.6480],\n",
            "        [0.6485],\n",
            "        [0.6474],\n",
            "        [0.6470],\n",
            "        [0.6482],\n",
            "        [0.6478],\n",
            "        [0.6480],\n",
            "        [0.6470],\n",
            "        [0.6482],\n",
            "        [0.6485]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6473],\n",
            "        [0.6480],\n",
            "        [0.6477],\n",
            "        [0.6487],\n",
            "        [0.6483],\n",
            "        [0.6475],\n",
            "        [0.6478],\n",
            "        [0.6481],\n",
            "        [0.6483],\n",
            "        [0.6482],\n",
            "        [0.6486],\n",
            "        [0.6481],\n",
            "        [0.6478],\n",
            "        [0.6478],\n",
            "        [0.6482],\n",
            "        [0.6489],\n",
            "        [0.6489],\n",
            "        [0.6490],\n",
            "        [0.6481],\n",
            "        [0.6482],\n",
            "        [0.6477],\n",
            "        [0.6483],\n",
            "        [0.6486],\n",
            "        [0.6486],\n",
            "        [0.6485],\n",
            "        [0.6478],\n",
            "        [0.6481],\n",
            "        [0.6490],\n",
            "        [0.6483],\n",
            "        [0.6478],\n",
            "        [0.6487],\n",
            "        [0.6485]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6484],\n",
            "        [0.6486],\n",
            "        [0.6484],\n",
            "        [0.6478],\n",
            "        [0.6491],\n",
            "        [0.6482],\n",
            "        [0.6479],\n",
            "        [0.6477],\n",
            "        [0.6483],\n",
            "        [0.6484],\n",
            "        [0.6478],\n",
            "        [0.6477],\n",
            "        [0.6477],\n",
            "        [0.6486],\n",
            "        [0.6477],\n",
            "        [0.6480],\n",
            "        [0.6490],\n",
            "        [0.6488],\n",
            "        [0.6491],\n",
            "        [0.6486],\n",
            "        [0.6480],\n",
            "        [0.6486],\n",
            "        [0.6487],\n",
            "        [0.6492],\n",
            "        [0.6473],\n",
            "        [0.6483],\n",
            "        [0.6487],\n",
            "        [0.6479],\n",
            "        [0.6476],\n",
            "        [0.6490],\n",
            "        [0.6483],\n",
            "        [0.6483]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6472],\n",
            "        [0.6461],\n",
            "        [0.6465],\n",
            "        [0.6467],\n",
            "        [0.6469],\n",
            "        [0.6443],\n",
            "        [0.6459],\n",
            "        [0.6460],\n",
            "        [0.6473],\n",
            "        [0.6458],\n",
            "        [0.6462],\n",
            "        [0.6458],\n",
            "        [0.6464],\n",
            "        [0.6461],\n",
            "        [0.6471],\n",
            "        [0.6456],\n",
            "        [0.6466],\n",
            "        [0.6462],\n",
            "        [0.6461],\n",
            "        [0.6466],\n",
            "        [0.6461],\n",
            "        [0.6465],\n",
            "        [0.6462],\n",
            "        [0.6470],\n",
            "        [0.6459],\n",
            "        [0.6460],\n",
            "        [0.6466],\n",
            "        [0.6464],\n",
            "        [0.6468],\n",
            "        [0.6459],\n",
            "        [0.6473],\n",
            "        [0.6458]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.6444],\n",
            "        [0.6428],\n",
            "        [0.6428],\n",
            "        [0.6437],\n",
            "        [0.6435],\n",
            "        [0.6436],\n",
            "        [0.6441],\n",
            "        [0.6435],\n",
            "        [0.6418],\n",
            "        [0.6428],\n",
            "        [0.6428],\n",
            "        [0.6438],\n",
            "        [0.6431],\n",
            "        [0.6431],\n",
            "        [0.6428],\n",
            "        [0.6428],\n",
            "        [0.6441],\n",
            "        [0.6438],\n",
            "        [0.6434],\n",
            "        [0.6438],\n",
            "        [0.6436],\n",
            "        [0.6440],\n",
            "        [0.6442]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tvZeMKKpRiYS",
        "outputId": "73998f88-3564-4b83-88f2-2e3f156375c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-e08e2dd5a40b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s9zgBpPR33S",
        "outputId": "6e046c5c-d9ee-473a-e343-ec752219ec86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "!pip install torchvision"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.3MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M4V6KSHXTJYA",
        "colab": {}
      },
      "source": [
        "# MNIST PYTORCH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q68oUf1dTq8f",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f-f5J6WLTtZQ",
        "outputId": "a306a6ea-0255-472d-b28c-80858f22aa40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5bea7b7bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AckXDGQ5TwgY",
        "outputId": "2d298774-a4cf-409d-ee79-59d333d2cb36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nFJpNYHiT1ig",
        "colab": {}
      },
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LMXnESlIT7qQ",
        "outputId": "072a2e33-c7c6-484b-eeca-9b04bfbe37f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O09-ReZ9T91o",
        "outputId": "9b5bc683-a44a-4aa0-d5b2-26a4a478c4e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFvCAYAAABguDDMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X98zvX+x/HnNpTMNmKyIj9yyxmN\nyomlohizqGlNO04qicqtbVg5N6lz+ko/FIaO6pbOKamE5RBiqePG7UyWpeOcUTh1O7GSH2M0Ebbr\n+4ebHfq8L10/dl3X+7LH/a/2ut7X5/O69Oqcp88+788V4XK5XAIAALBIZKgbAAAA+CUCCgAAsA4B\nBQAAWIeAAgAArENAAQAA1iGgAAAA69QLdQPecLlcmjdvngoKCnTixAmdPHlSbdq0UW5urjp37hyS\nnu677z7ddtttuuOOO2pqO3bsUHZ2tiSpsrJSlZWVuuSSSyRJgwcP1oMPPujx8ffv36/NmzerT58+\nKisrU79+/bR161afeq2urtZdd92l9u3b6/nnn/fpGAgd5p/5r6uYfd9mv7KyUk899ZRKS0vlcrmU\nlpam3Nxcr44RSmEVUPLz81VcXKzXX39d8fHxqqqq0qJFizR8+HAVFhaqadOmoW5RktShQwetWrVK\nkrR48WJ98MEHevPNN306VnFxsdavX68+ffr43df8+fNVXl6u9u3b+30sBB/z7x/mP3wx+76ZPn26\n6tevrw8//FA//fST0tPT1a1bN/Xs2dPnYwZT2PyKp6KiQnPnztWUKVMUHx8vSYqKilJWVpbWrFlT\nM6DDhg1Tfn6+BgwYoE2bNqmiokK5ubnq37+/0tLS9Nprr0mSysrKlJiYWHP8M39evHixcnJy9Pjj\nj9e8b8eOHZKkXbt2KTMzU3379lVeXp6qqqq8/izFxcXKyspSbm6u8vLyVFxcrJSUlLNeT0lJ0ZYt\nWzRp0iQVFhZq7NixNa8XFBRo0KBB6tWrl5YvXy5J2rNnjwYOHOj2nHv37tW8efN07733et0vQo/5\nZ/7rKmbf99lPSUlRTk6OIiMjFR0drY4dO9Z8nnAQNgFl8+bNatmypdq0aeN4LTo6+qyfS0tLtWLF\nCl1zzTWaPn26YmNjVVhYqHfffVfz589XSUnJr55v3bp1Gjp0qAoLC9W9e3fNnTtXkjR16lQlJyfr\n448/1r333qtNmzb59Hm2bt2qrKwsTZs2ze2aTp066e6771b//v2Vn58v6dRl6hMnTmjZsmWaMGGC\nZsyYIUlq0aJFzcCaPPvss3rkkUfUuHFjn/pFaDH/zH9dxez7PvvJyclq2bKlpFO/7vniiy/UpUsX\nn/oOhbAJKIcOHTrrMt7hw4eVmpqq1NRU3XTTTZozZ07Na7169VJk5KmPtnbtWg0dOlSSFBcXp5SU\nFBUVFf3q+dq3b1/zu83ExETt3r1bklRSUqK0tDRJUlJSktq1a+fT57nwwguVnJzs9ftcLpfS09Nr\n+vrhhx9+9T3r1q3T4cOHz/k3TNiN+T+F+a97mP1TfJn9044fP668vDzdcsstuvrqq70+d6iEzT0o\nTZs21d69e2t+jomJqfld38SJE3Xs2LGa12JjY2v++cCBA4qJiTnrfWcex50z/6YVFRVVcznv0KFD\nZ6X2M4/tjTN79EZUVJQaNmwoSYqMjFR1dfU51x87dkwvvPCCZs+e7dP5YAfm/3+9MP91C7P/v168\nmf3Tjhw5ouzsbLVo0UL/93//59O5QyVsAkrXrl1VXl6urVu3nvX7w1/TrFkzVVRUKCEhQdKp32c2\na9ZMUVFRqq6ulsvlUkREhA4fPuzR8WJiYlRZWVnz84EDB7z7IAZn/kcgyeNePFFaWqoffvih5m8S\nx44d04kTJ3TgwIGa38nCfsy/b5j/8Mfs++7kyZN65JFH1KFDBz3++OO1euxgCJtf8URHR2v06NEa\nP368vv32W0mnfie3YsUKrVy5Uq1btza+r3fv3lqwYIGkUwO1evVq9e7dW02aNFFUVJS2bdsmSVqy\nZIlHfXTt2lWrV6+WJG3atEk7d+7096OpefPm2rdvn8rLy1VVVaVly5bVvFavXj39+OOPPh+7W7du\nKikpUVFRkYqKijRx4sSzbhhDeGD+fcP8hz9m33fz5s1To0aNwjKcSGF0BUWSRo4cqbi4OOXk5Ojn\nn3/W8ePH1bZtW82aNUs33HCD8T1jxozRU089pdTUVEVGRmrUqFFKSkqSJGVnZ+uBBx5QfHy8hg0b\n5lEPjz32mPLy8rR06VJ16dJF119/vd+f6/LLL1dGRobS09OVkJCg22+/XV9++aUkqWfPnnrjjTeU\nkZGhmTNnuj3Gnj17NGLEiHPeKIjwxvwz/3UVs+/b7L/33ns6evSoUlNTa2qpqakaM2aM370HQ4TL\n5XKFugkAAIAzhc2veAAAQN1BQAEAANYhoAAAAOsQUAAAgHUIKAAAwDrn3GYcERERrD4A2bahjPlH\nMNk0/8w+gsnd7HMFBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACw\nDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIA\nAKxDQAEAANYhoAAAAOvUC3UDAPBLCxcudNR69OhhXNu6detAtwMgBLiCAgAArENAAQAA1iGgAAAA\n6xBQAACAdQgoAADAOiHZxVNdXe1V3eSuu+4y1t9//32fegIQfK1atTLWMzMzHbVx48YFuh0gaIYP\nH26sN2rUyFHbtWuXce3SpUtrtSfbcAUFAABYh4ACAACsQ0ABAADWIaAAAADrhO1NsgDC35133unx\n2k8//TSAnQCBY7oh9rXXXjOujYx0Xjf45JNPjGu5SRYAACDICCgAAMA6BBQAAGAdAgoAALAOAQUA\nAFgnJLt46qpOnTo5at7sYnj55ZeN9X379vncE+ou02PmL730UuPaDRs2BKSH6dOne7w2UD0A51K/\nfn1HbeDAgca1vXr1MtZNX81i2q3jzpYtWzxeez7hCgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAA\nAOuwiycAFixYYKwnJCQ4at27d/f4uNddd52xfuutt3p8DOA005wmJycb10ZERPh9PtOuISAUGjVq\n5Kj169fPuHbixImO2tVXX21c6+6/E5fL5UV3TrGxsX69P1xxBQUAAFiHgAIAAKxDQAEAANYhoAAA\nAOtwk2wAZGRkGOvV1dV+HTclJcWv96NuGjt2rLFuuiF2165dAevDm691+PTTTwPWB+qO6OhoYz0t\nLc1Rmz9/vt/nC9RNsrfddpux/tJLLzlq2dnZfp3LJlxBAQAA1iGgAAAA6xBQAACAdQgoAADAOgQU\nAABgnZDs4omM9D8XuXucfHp6uqO2fPlyv8/njdr4fN644oorHLX//Oc/Qe0BdjA9Tn769Okev//R\nRx+tzXbO4u4x+iZlZWUB6wN1R2JiorFeGzt2gqlJkybG+u9+9ztH7e233zauLS4urtWegoErKAAA\nwDoEFAAAYB0CCgAAsA4BBQAAWCckN8m6e+S7v4+Cl6TFixc7ag0aNPD7uN5YtWqVsd63b9+AnM/0\nmZOSkgJyLtitqKjI47Wmx9ovXLiwNts5y2WXXebxWh51j9pw/PhxY33v3r2OWv369Y1rf/rpJ4/P\n98QTTxjr3jzq/tlnn3XUEhISjGtNN8+2bNnS43PZjisoAADAOgQUAABgHQIKAACwDgEFAABYh4AC\nAACsE5JdPC+//LKx/tBDDwW5k8D4+uuvjfVA7eJB3dOjRw9j3fSoe3cC+Vh7E28edV9QUBDATlBX\n/POf/zTWTY/Aj4+PN67dtm1brfb0ayZMmBDU89mMKygAAMA6BBQAAGAdAgoAALAOAQUAAFgnJDfJ\nunsc8LFjxxy1MWPGBLqdWvfwww8b67XxKH9Aklq3bu33MQL1WHt3N/B6w/QYfqC2HDx40KMaQosr\nKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBOSXTyHDx821idPnuyoXXjhhca13jwWf+nSpcb6\n7bff7vExbHbFFVc4ai+++KJx7WOPPRbodlDHZWZmBuS47h7jb9rpl5eXF5AegNry17/+1Vjv2LGj\nx8fYuHGjo7ZkyRKfe7INV1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFgnJLt43DHt7tm2bZtx\n7ZEjR4z1xo0bO2qDBg0yrv3www8dtdrY5RIVFeX3MbzRsGFDR+2yyy4Lag8Irk8//dTvY5i+M2fD\nhg1+Hzc5OdnvY3hz3HHjxjlqM2bMMK7lO34QCm+88YajNmzYMONal8vlqJWVlRnXjhgxwr/GLMcV\nFAAAYB0CCgAAsA4BBQAAWIeAAgAArGPVTbIms2fPNtbr169vrD/55JOOWmxsrHFt3759HbUvvvjC\ni+7MqqqqjPXq6mq/j21iurm4Nm52hL3c3expqrt7RPzChQsdtZ49e3p1PhNvbpJ1d1xTz1OnTvX4\nuEBt6d69u6N25ZVXGtc+99xzxnqLFi0ctYiICI97cPf1MNdcc41Htdry1ltvBezYJlxBAQAA1iGg\nAAAA6xBQAACAdQgoAADAOgQUAABgnQiX6bm6p1/04i5jWzz88MOO2ksvvWRcG6hdNZGR5twXqPOV\nl5c7atnZ2ca1BQUFAemhNpxjFEMiHOfftPtl586dfh930aJFjpq7x+1Pnz7d7/N5sxvJdL68vDy/\newg2m+bf5tlPSEgw1k27bdz5/e9/76glJiYa15p24MTFxRnXuvtzs+nfrT8C9TUu7v58uIICAACs\nQ0ABAADWIaAAAADrEFAAAIB1zrubZE0yMjI8XnvnnXf6fYxg3yRrOp/pMeaSNHTo0ID0UBtsu5Hs\nfJl/dzeXTps2zVHLzMwMdDs+c3fzbTjeEGti0/zbMPs33HCDsT5lyhRjvUePHoFsxyPu/txMN31v\n27Yt0O347KOPPjLWA/V1E9wkCwAAwgYBBQAAWIeAAgAArENAAQAA1iGgAAAA69SJXTzeaN68ubF+\n8cUXB+R8o0ePNtYfeughj4/BLp7AqIvz727HT3JysqPm7tHi48aN8/h87tbm5+d7fIzzhU3zH6jZ\nd/eY+jlz5jhqt9xyi3FtgwYNarWnX1NVVeWoff7558a169atM9b/8pe/OGrbt2/3r7HzCLt4AABA\n2CCgAAAA6xBQAACAdQgoAADAOvVC3YBt9u3b51XdXwcOHDDW3T0u3yQqKspRGzJkiHHt/v37jfUn\nnnjCUTt8+LDHPeD8YHok97nq/qqLN8PWZatXrzbWO3bsGJDzHTt2zFj/73//66i5+9+7yZMnO2or\nVqzwqy94hisoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACswy6eEHP3iN/q6mq/juvu/e4eoW+6\n233jxo3GtYsWLfK9MQB11oYNG4z1xo0be3yMrVu3GuvvvPOOo+Zu1+LKlSs9Ph9ChysoAADAOgQU\nAABgHQIKAACwDgEFAABYh4ACAACsE+Fyt41EUkRERDB7qZN69eplrM+aNctR+81vfmNca/reHn93\nAZ1LgwYNAnLcc4xiSDD/5+bu+54WLFjg8TH4M/4fm+affy8IJnezzxUUAABgHQIKAACwDgEFAABY\nh4ACAACsw02ylurRo4ejNn/+fOPaVq1aOWrcJOs/5t837v497tq1y1Fr3bp1oNsJGzbNP7OPYOIm\nWQAAEDYIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIddPGGkXbt2xnpiYqKjtnjxYuPal19+2Vh/9dVX\nPe7jq6++8nitN2zaxSAx/wgum+af2UcwsYsHAACEDQIKAACwDgEFAABYh4ACAACsw02ysIZNNwlK\nzD+Cy6b5Z/YRTNwkCwAAwgYBBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABg\nHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdCJfL5Qp1EwAA\nAGfiCgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4B\nBQAAWIeAAgAArENAAQAA1qkX6ga84XK5NG/ePBUUFOjEiRM6efKk2rRpo9zcXHXu3DkkPd133326\n7bbbdMcdd9TUduzYoezsbElSZWWlKisrdckll0iSBg8erAcffNDj4+/fv1+bN29Wnz59VFZWpn79\n+mnr1q1e91laWqoxY8aoe/fueuaZZ7x+P0KP+fdt/vft26c//vGP+uabbxQVFaX09HSNGjXKq2Mg\ntJj9ujn7YRVQ8vPzVVxcrNdff13x8fGqqqrSokWLNHz4cBUWFqpp06ahblGS1KFDB61atUqStHjx\nYn3wwQd68803fTpWcXGx1q9frz59+vjcz2effabJkycrKSnJ52Mg9Jh/3zz//PNq27atXnnlFVVW\nViojI0OdO3fW9ddf7/MxEVzMvm/CffbD5lc8FRUVmjt3rqZMmaL4+HhJUlRUlLKysrRmzZqaAR02\nbJjy8/M1YMAAbdq0SRUVFcrNzVX//v2Vlpam1157TZJUVlamxMTEmuOf+fPixYuVk5Ojxx9/vOZ9\nO3bskCTt2rVLmZmZ6tu3r/Ly8lRVVeX1ZykuLlZWVpZyc3OVl5en4uJipaSknPV6SkqKtmzZokmT\nJqmwsFBjx46teb2goECDBg1Sr169tHz5cknSnj17NHDgQOP5mjZtqnfffVdt27b1ulfYgfn3ff63\nb9+u5ORkSVJ0dLQ6d+6s7du3e903QoPZr7uzHzYBZfPmzWrZsqXatGnjeC06Ovqsn0tLS7VixQpd\nc801mj59umJjY1VYWKh3331X8+fPV0lJya+eb926dRo6dKgKCwvVvXt3zZ07V5I0depUJScn6+OP\nP9a9996rTZs2+fR5tm7dqqysLE2bNs3tmk6dOunuu+9W//79lZ+fL0mqrq7WiRMntGzZMk2YMEEz\nZsyQJLVo0aJmYH/piiuucPwZIbww/77Pf3JyslauXKmTJ09qz549+te//qUePXr41DeCj9mvu7Mf\nNgHl0KFDZ13GO3z4sFJTU5WamqqbbrpJc+bMqXmtV69eiow89dHWrl2roUOHSpLi4uKUkpKioqKi\nXz1f+/bta363mZiYqN27d0uSSkpKlJaWJklKSkpSu3btfPo8F154YU2y9YbL5VJ6enpNXz/88INP\n50d4Yf5P8WX+s7Oz9e9//1vdu3fXzTffrP79+6tjx45enxuhweyfUhdnP2wCStOmTbV3796an2Ni\nYrRq1SqtWrVKN954o44dO1bzWmxsbM0/HzhwQDExMWe9r7y8/FfP17hx45p/joqKqrmcd+jQobNS\n+5nH9saZPXojKipKDRs2lCRFRkaqurrap+MgvDD//+vF2/mfMGGC+vfvr5KSEq1fv14bNmzQhx9+\n6NP5EXzM/v96qWuzHzYBpWvXriovL/f6LuZmzZqpoqKi5ueKigo1a9ZMUVFRqq6ulsvlknQqlXsi\nJiZGlZWVNT8fOHDAq35MzvyPwJteUHcw/74rKirSwIEDFRERobi4OPXs2VMbN26s1XMgcJh934X7\n7IdNQImOjtbo0aM1fvx4ffvtt5JO/U5uxYoVWrlypVq3bm18X+/evbVgwQJJpwZq9erV6t27t5o0\naaKoqCht27ZNkrRkyRKP+ujatatWr14tSdq0aZN27tzp70dT8+bNtW/fPpWXl6uqqkrLli2rea1e\nvXr68ccf/T4Hwhvz77u2bdtqzZo1kqRjx46puLhYHTp08OuYCB5m33fhPvthtc145MiRiouLU05O\njn7++WcdP35cbdu21axZs3TDDTcY3zNmzBg99dRTSk1NVWRkpEaNGlWz3TY7O1sPPPCA4uPjNWzY\nMI96eOyxx5SXl6elS5eqS5cutbJd6/LLL1dGRobS09OVkJCg22+/XV9++aUkqWfPnnrjjTeUkZGh\nmTNnuj3Gnj17NGLECOPNUjNmzNCqVat08OBBVVVV6fPPP1dKSory8vL87h3Bw/z7Nv/PP/+8nn76\nab333ntyuVy68cYbNWTIEL/7RvAw+3Vz9iNcp69zAQAAWCJsfsUDAADqDgIKAACwDgEFAABYh4AC\nAACsQ0ABAADWOec244iIiGD1Aci2DWXMP4LJpvln9hFM7mafKygAAMA6BBQAAGAdAgoAALAOAQUA\nAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENA\nAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYp16oG4Dn+vXrZ6z/9re/ddSe\neeaZQLcDAEDAcAUFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB12MUTRjIyMoz1rVu3BrkTnK+u\nvfZaY33UqFEe1STpq6++MtY/+OADR2327NnGtTt37nTXIhBUCQkJjtrTTz9tXHv//fcb6y6Xy1Hb\nsmWLcW1OTo6jtmbNmnO1eN7iCgoAALAOAQUAAFiHgAIAAKxDQAEAANaJcJnu3jn9YkREMHvBGRo1\nauSoffnll8a1paWljlpaWlqt9xRo5xjFkDjf579v376O2tSpU41rr7rqqoD0UFFRYaw/+uijjtpb\nb71lXFtVVVWrPYWKTfN/vs++NxYuXOio3XnnnQE73/Hjxx21V155xbi2pKTEUfvpp5+Ma//2t7/5\n11gAuZt9rqAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOj7q31ODBgx21Sy+91Lh2yZIlgW4H\nYeyiiy4y1t9//31HLTo6OtDtnCUuLs5Yf/311x010842Sfrzn/9cqz2hburRo4exfuuttwa1jwYN\nGjhqubm5Hr//k08+MdZt3sXjDldQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh108lho3bpyj\n5u77MUy7MYDT3M2Nvzt23H3nx8GDB411d7vQPPXcc88Z66bvqHK3kwFwZ/z48cZ6w4YNPT7G0aNH\njfXvvvvOUWvVqpVx7QUXXODx+Ux+/PFHv95vE66gAAAA6xBQAACAdQgoAADAOgQUAABgHW6SDbHh\nw4cb6127dnXUtmzZYlz72Wef1WpPgCcmT55srLu7qfDJJ5/063zuHtk/YcIER42bZOGtxo0b+32M\nKVOmGOuTJk1y1B555BHj2lmzZnl8vp9//tlRmzp1qsfvtx1XUAAAgHUIKAAAwDoEFAAAYB0CCgAA\nsA4BBQAAWIddPEHUpk0bR83d47tPnDjhqI0cOdK41t3jlQHJ/XwsXLjQURsyZIjHx3X3aPBGjRp5\nfIzacPLkyaCeD3CnvLzc47VdunTxeK3L5TLWZ8yY4aitX7/e4+PajisoAADAOgQUAABgHQIKAACw\nDgEFAABYh5tkg2jixImOWvPmzY1rp02b5qht2LCh1nvC+a+6utpYX758uaPmzU2ycXFxPvfki82b\nNxvr7h4vDthi2LBhjlpmZqbH71+zZo2xbvqah/MJV1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUA\nAFiHXTwBcM899xjrI0aMcNSOHz9uXJufn1+rPQG/VFRU5Kjt37/fuLZZs2YB6eH999831mfOnOmo\nudvFVlVVVas9Ab4y7daRpIsuushRi4mJ8fi4n332mc89hTOuoAAAAOsQUAAAgHUIKAAAwDoEFAAA\nYB1ukvVDfHy8sZ6Tk2Osu1wuRy03N9e4dvfu3b43BpwhIiLCWB8wYICjFuzH15eVlRnrxcXFjho3\nwyKQ3N0g7o3rrrvO72Ns377dUfvTn/7k93HDEVdQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABY\nJ8Jl2lpy+kU3d//jlEcffdRYnzJlirH+9ttvO2r333+/cW1d3LFwjlEMifNl/gcPHmysFxQUBLkT\nz6WlpTlqhYWFIegkeGya//Nl9r1x8cUXG+tff/21o+bNY+rdcfc1J7fccoujtn79er/PZzN3s88V\nFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1uG7eDzUsmVLR23y5MnGtZWVlcb6K6+84qjVxd06\nCK5Bgwb5fYw//OEPjtq6deuMa8ePH2+su9tNZJKZmemone+7eBBa5eXlxvrf//53Ry09Pd3v882Z\nM8dYP9937HiDKygAAMA6BBQAAGAdAgoAALAOAQUAAFiHm2Q9NHbsWEftggsuMK594YUXjPUNGzbU\nak/ALyUmJjpqGRkZHr+/tLTUWDfd4H3kyBHj2gULFhjrphsL3T1S/e6773bU3nnnHePaNWvWGOuA\nzZYvXx7qFqzHFRQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANZhF88vJCUlGeu5ubmOWllZmXHt\n1KlTa7UnwFNXXnmloxYdHe3x+/Pz8411dzt2TBYtWmSsmx51f9dddxnX1q9f31G77rrrjGvZxYPa\nUK+e+f8OW7VqFZDz3XTTTcY6X+nwP1xBAQAA1iGgAAAA6xBQAACAdQgoAADAOtwk+wvDhw831k03\nUM2fP9+49vDhw7XaE+CprKwsv96/Y8eOWurE6ZlnnnHU3N0ka+Ju7ZQpU3zuCTitXbt2xvq1114b\n5E5wGldQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYJ8LlcrncvhgREcxegq5ly5aO2pYtW4xr\njx496qglJycb1+7cudO/xuqoc4xiSITj/A8ZMsRRc7fbzOSdd94x1u+55x6fezotPj7eUdu9e7fH\n7//uu++M9datW/vck01smv9wnH1/uXvEfEpKSkDO9/XXXxvrHTp0CMj5bOZu9rmCAgAArENAAQAA\n1iGgAAAA6xBQAACAder0o+5vvfVWRy0uLs64dt68eY4aN8PCNv/4xz8ctYMHDxrXNmnSxFHr3bu3\ncW1sbKyjdujQIe+aAywWGRncv6+3adPGWO/WrZujVlJSEuBu7MQVFAAAYB0CCgAAsA4BBQAAWIeA\nAgAArENAAQAA1qkTu3jq1TO2F2cYAAACYUlEQVR/zIceeshRO3LkiHHtzJkza7UnIBC+//57R23j\nxo3Gtf369XPULr30UuPatWvXOmqvvvqqV71df/31Xq0HzmdRUVHG+rXXXuuosYsHAADAEgQUAABg\nHQIKAACwDgEFAABYh4ACAACsUyd28bj7fp2rr77aUSsqKjKu/eabb2q1JyBYnn76aWPdtKsmOjra\nuPaqq65y1GbPnu1fY14yfc8QUFuOHj0a6hYkSZ06dQp1C9bgCgoAALAOAQUAAFiHgAIAAKxDQAEA\nANapEzfJVlZWGusFBQWOWkRERKDbAYJq/fr1xnpGRoaj9uKLLxrXJiUl1WpPv+a7775z1CZNmhTU\nHlC3jB492lg33bTatm1bv89XXV1trH/00Ud+H/t8wRUUAABgHQIKAACwDgEFAABYh4ACAACsQ0AB\nAADWiXC5XC63L7KjBUF0jlEMibo4/40bNzbWL7/8ckdt5MiRxrVDhgwx1uPj4x2177//3rh2wIAB\njlppaalx7fnCpvmvi7PvTrdu3Ry19957z7i2Xbt2Hh937dq1xvrNN9/s8THOF+5mnysoAADAOgQU\nAABgHQIKAACwDgEFAABYh5tkYQ2bbhKUmH8El03zz+wjmLhJFgAAhA0CCgAAsA4BBQAAWIeAAgAA\nrENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAA\nAADrEFAAAIB1CCgAAMA6ES6XyxXqJgAAAM7EFRQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYh\noAAAAOv8P8tGCI/pSJ3uAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5bed729f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFvCAYAAABguDDMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X98zvX+x/HnNpTMNmKyIj9yyxmN\nyomlohizqGlNO04qicqtbVg5N6lz+ko/FIaO6pbOKamE5RBiqePG7UyWpeOcUTh1O7GSH2M0Ebbr\n+4ebHfq8L10/dl3X+7LH/a/2ut7X5/O69Oqcp88+788V4XK5XAIAALBIZKgbAAAA+CUCCgAAsA4B\nBQAAWIeAAgAArENAAQAA1iGgAAAA69QLdQPecLlcmjdvngoKCnTixAmdPHlSbdq0UW5urjp37hyS\nnu677z7ddtttuuOOO2pqO3bsUHZ2tiSpsrJSlZWVuuSSSyRJgwcP1oMPPujx8ffv36/NmzerT58+\nKisrU79+/bR161afeq2urtZdd92l9u3b6/nnn/fpGAgd5p/5r6uYfd9mv7KyUk899ZRKS0vlcrmU\nlpam3Nxcr44RSmEVUPLz81VcXKzXX39d8fHxqqqq0qJFizR8+HAVFhaqadOmoW5RktShQwetWrVK\nkrR48WJ98MEHevPNN306VnFxsdavX68+ffr43df8+fNVXl6u9u3b+30sBB/z7x/mP3wx+76ZPn26\n6tevrw8//FA//fST0tPT1a1bN/Xs2dPnYwZT2PyKp6KiQnPnztWUKVMUHx8vSYqKilJWVpbWrFlT\nM6DDhg1Tfn6+BgwYoE2bNqmiokK5ubnq37+/0tLS9Nprr0mSysrKlJiYWHP8M39evHixcnJy9Pjj\nj9e8b8eOHZKkXbt2KTMzU3379lVeXp6qqqq8/izFxcXKyspSbm6u8vLyVFxcrJSUlLNeT0lJ0ZYt\nWzRp0iQVFhZq7NixNa8XFBRo0KBB6tWrl5YvXy5J2rNnjwYOHOj2nHv37tW8efN07733et0vQo/5\nZ/7rKmbf99lPSUlRTk6OIiMjFR0drY4dO9Z8nnAQNgFl8+bNatmypdq0aeN4LTo6+qyfS0tLtWLF\nCl1zzTWaPn26YmNjVVhYqHfffVfz589XSUnJr55v3bp1Gjp0qAoLC9W9e3fNnTtXkjR16lQlJyfr\n448/1r333qtNmzb59Hm2bt2qrKwsTZs2ze2aTp066e6771b//v2Vn58v6dRl6hMnTmjZsmWaMGGC\nZsyYIUlq0aJFzcCaPPvss3rkkUfUuHFjn/pFaDH/zH9dxez7PvvJyclq2bKlpFO/7vniiy/UpUsX\nn/oOhbAJKIcOHTrrMt7hw4eVmpqq1NRU3XTTTZozZ07Na7169VJk5KmPtnbtWg0dOlSSFBcXp5SU\nFBUVFf3q+dq3b1/zu83ExETt3r1bklRSUqK0tDRJUlJSktq1a+fT57nwwguVnJzs9ftcLpfS09Nr\n+vrhhx9+9T3r1q3T4cOHz/k3TNiN+T+F+a97mP1TfJn9044fP668vDzdcsstuvrqq70+d6iEzT0o\nTZs21d69e2t+jomJqfld38SJE3Xs2LGa12JjY2v++cCBA4qJiTnrfWcex50z/6YVFRVVcznv0KFD\nZ6X2M4/tjTN79EZUVJQaNmwoSYqMjFR1dfU51x87dkwvvPCCZs+e7dP5YAfm/3+9MP91C7P/v168\nmf3Tjhw5ouzsbLVo0UL/93//59O5QyVsAkrXrl1VXl6urVu3nvX7w1/TrFkzVVRUKCEhQdKp32c2\na9ZMUVFRqq6ulsvlUkREhA4fPuzR8WJiYlRZWVnz84EDB7z7IAZn/kcgyeNePFFaWqoffvih5m8S\nx44d04kTJ3TgwIGa38nCfsy/b5j/8Mfs++7kyZN65JFH1KFDBz3++OO1euxgCJtf8URHR2v06NEa\nP368vv32W0mnfie3YsUKrVy5Uq1btza+r3fv3lqwYIGkUwO1evVq9e7dW02aNFFUVJS2bdsmSVqy\nZIlHfXTt2lWrV6+WJG3atEk7d+7096OpefPm2rdvn8rLy1VVVaVly5bVvFavXj39+OOPPh+7W7du\nKikpUVFRkYqKijRx4sSzbhhDeGD+fcP8hz9m33fz5s1To0aNwjKcSGF0BUWSRo4cqbi4OOXk5Ojn\nn3/W8ePH1bZtW82aNUs33HCD8T1jxozRU089pdTUVEVGRmrUqFFKSkqSJGVnZ+uBBx5QfHy8hg0b\n5lEPjz32mPLy8rR06VJ16dJF119/vd+f6/LLL1dGRobS09OVkJCg22+/XV9++aUkqWfPnnrjjTeU\nkZGhmTNnuj3Gnj17NGLEiHPeKIjwxvwz/3UVs+/b7L/33ns6evSoUlNTa2qpqakaM2aM370HQ4TL\n5XKFugkAAIAzhc2veAAAQN1BQAEAANYhoAAAAOsQUAAAgHUIKAAAwDrn3GYcERERrD4A2bahjPlH\nMNk0/8w+gsnd7HMFBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACw\nDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIA\nAKxDQAEAANYhoAAAAOvUC3UDAPBLCxcudNR69OhhXNu6detAtwMgBLiCAgAArENAAQAA1iGgAAAA\n6xBQAACAdQgoAADAOiHZxVNdXe1V3eSuu+4y1t9//32fegIQfK1atTLWMzMzHbVx48YFuh0gaIYP\nH26sN2rUyFHbtWuXce3SpUtrtSfbcAUFAABYh4ACAACsQ0ABAADWIaAAAADrhO1NsgDC35133unx\n2k8//TSAnQCBY7oh9rXXXjOujYx0Xjf45JNPjGu5SRYAACDICCgAAMA6BBQAAGAdAgoAALAOAQUA\nAFgnJLt46qpOnTo5at7sYnj55ZeN9X379vncE+ou02PmL730UuPaDRs2BKSH6dOne7w2UD0A51K/\nfn1HbeDAgca1vXr1MtZNX81i2q3jzpYtWzxeez7hCgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAA\nAOuwiycAFixYYKwnJCQ4at27d/f4uNddd52xfuutt3p8DOA005wmJycb10ZERPh9PtOuISAUGjVq\n5Kj169fPuHbixImO2tVXX21c6+6/E5fL5UV3TrGxsX69P1xxBQUAAFiHgAIAAKxDQAEAANYhoAAA\nAOtwk2wAZGRkGOvV1dV+HTclJcWv96NuGjt2rLFuuiF2165dAevDm691+PTTTwPWB+qO6OhoYz0t\nLc1Rmz9/vt/nC9RNsrfddpux/tJLLzlq2dnZfp3LJlxBAQAA1iGgAAAA6xBQAACAdQgoAADAOgQU\nAABgnZDs4omM9D8XuXucfHp6uqO2fPlyv8/njdr4fN644oorHLX//Oc/Qe0BdjA9Tn769Okev//R\nRx+tzXbO4u4x+iZlZWUB6wN1R2JiorFeGzt2gqlJkybG+u9+9ztH7e233zauLS4urtWegoErKAAA\nwDoEFAAAYB0CCgAAsA4BBQAAWCckN8m6e+S7v4+Cl6TFixc7ag0aNPD7uN5YtWqVsd63b9+AnM/0\nmZOSkgJyLtitqKjI47Wmx9ovXLiwNts5y2WXXebxWh51j9pw/PhxY33v3r2OWv369Y1rf/rpJ4/P\n98QTTxjr3jzq/tlnn3XUEhISjGtNN8+2bNnS43PZjisoAADAOgQUAABgHQIKAACwDgEFAABYh4AC\nAACsE5JdPC+//LKx/tBDDwW5k8D4+uuvjfVA7eJB3dOjRw9j3fSoe3cC+Vh7E28edV9QUBDATlBX\n/POf/zTWTY/Aj4+PN67dtm1brfb0ayZMmBDU89mMKygAAMA6BBQAAGAdAgoAALAOAQUAAFgnJDfJ\nunsc8LFjxxy1MWPGBLqdWvfwww8b67XxKH9Aklq3bu33MQL1WHt3N/B6w/QYfqC2HDx40KMaQosr\nKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArBOSXTyHDx821idPnuyoXXjhhca13jwWf+nSpcb6\n7bff7vExbHbFFVc4ai+++KJx7WOPPRbodlDHZWZmBuS47h7jb9rpl5eXF5AegNry17/+1Vjv2LGj\nx8fYuHGjo7ZkyRKfe7INV1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFgnJLt43DHt7tm2bZtx\n7ZEjR4z1xo0bO2qDBg0yrv3www8dtdrY5RIVFeX3MbzRsGFDR+2yyy4Lag8Irk8//dTvY5i+M2fD\nhg1+Hzc5OdnvY3hz3HHjxjlqM2bMMK7lO34QCm+88YajNmzYMONal8vlqJWVlRnXjhgxwr/GLMcV\nFAAAYB0CCgAAsA4BBQAAWIeAAgAArGPVTbIms2fPNtbr169vrD/55JOOWmxsrHFt3759HbUvvvjC\ni+7MqqqqjPXq6mq/j21iurm4Nm52hL3c3expqrt7RPzChQsdtZ49e3p1PhNvbpJ1d1xTz1OnTvX4\nuEBt6d69u6N25ZVXGtc+99xzxnqLFi0ctYiICI97cPf1MNdcc41Htdry1ltvBezYJlxBAQAA1iGg\nAAAA6xBQAACAdQgoAADAOgQUAABgnQiX6bm6p1/04i5jWzz88MOO2ksvvWRcG6hdNZGR5twXqPOV\nl5c7atnZ2ca1BQUFAemhNpxjFEMiHOfftPtl586dfh930aJFjpq7x+1Pnz7d7/N5sxvJdL68vDy/\newg2m+bf5tlPSEgw1k27bdz5/e9/76glJiYa15p24MTFxRnXuvtzs+nfrT8C9TUu7v58uIICAACs\nQ0ABAADWIaAAAADrEFAAAIB1zrubZE0yMjI8XnvnnXf6fYxg3yRrOp/pMeaSNHTo0ID0UBtsu5Hs\nfJl/dzeXTps2zVHLzMwMdDs+c3fzbTjeEGti0/zbMPs33HCDsT5lyhRjvUePHoFsxyPu/txMN31v\n27Yt0O347KOPPjLWA/V1E9wkCwAAwgYBBQAAWIeAAgAArENAAQAA1iGgAAAA69SJXTzeaN68ubF+\n8cUXB+R8o0ePNtYfeughj4/BLp7AqIvz727HT3JysqPm7tHi48aN8/h87tbm5+d7fIzzhU3zH6jZ\nd/eY+jlz5jhqt9xyi3FtgwYNarWnX1NVVeWoff7558a169atM9b/8pe/OGrbt2/3r7HzCLt4AABA\n2CCgAAAA6xBQAACAdQgoAADAOvVC3YBt9u3b51XdXwcOHDDW3T0u3yQqKspRGzJkiHHt/v37jfUn\nnnjCUTt8+LDHPeD8YHok97nq/qqLN8PWZatXrzbWO3bsGJDzHTt2zFj/73//66i5+9+7yZMnO2or\nVqzwqy94hisoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACswy6eEHP3iN/q6mq/juvu/e4eoW+6\n233jxo3GtYsWLfK9MQB11oYNG4z1xo0be3yMrVu3GuvvvPOOo+Zu1+LKlSs9Ph9ChysoAADAOgQU\nAABgHQIKAACwDgEFAABYh4ACAACsE+Fyt41EUkRERDB7qZN69eplrM+aNctR+81vfmNca/reHn93\nAZ1LgwYNAnLcc4xiSDD/5+bu+54WLFjg8TH4M/4fm+affy8IJnezzxUUAABgHQIKAACwDgEFAABY\nh4ACAACsw02ylurRo4ejNn/+fOPaVq1aOWrcJOs/5t837v497tq1y1Fr3bp1oNsJGzbNP7OPYOIm\nWQAAEDYIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIddPGGkXbt2xnpiYqKjtnjxYuPal19+2Vh/9dVX\nPe7jq6++8nitN2zaxSAx/wgum+af2UcwsYsHAACEDQIKAACwDgEFAABYh4ACAACsw02ysIZNNwlK\nzD+Cy6b5Z/YRTNwkCwAAwgYBBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABg\nHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdCJfL5Qp1EwAA\nAGfiCgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4B\nBQAAWIeAAgAArENAAQAA1qkX6ga84XK5NG/ePBUUFOjEiRM6efKk2rRpo9zcXHXu3DkkPd133326\n7bbbdMcdd9TUduzYoezsbElSZWWlKisrdckll0iSBg8erAcffNDj4+/fv1+bN29Wnz59VFZWpn79\n+mnr1q1e91laWqoxY8aoe/fueuaZZ7x+P0KP+fdt/vft26c//vGP+uabbxQVFaX09HSNGjXKq2Mg\ntJj9ujn7YRVQ8vPzVVxcrNdff13x8fGqqqrSokWLNHz4cBUWFqpp06ahblGS1KFDB61atUqStHjx\nYn3wwQd68803fTpWcXGx1q9frz59+vjcz2effabJkycrKSnJ52Mg9Jh/3zz//PNq27atXnnlFVVW\nViojI0OdO3fW9ddf7/MxEVzMvm/CffbD5lc8FRUVmjt3rqZMmaL4+HhJUlRUlLKysrRmzZqaAR02\nbJjy8/M1YMAAbdq0SRUVFcrNzVX//v2Vlpam1157TZJUVlamxMTEmuOf+fPixYuVk5Ojxx9/vOZ9\nO3bskCTt2rVLmZmZ6tu3r/Ly8lRVVeX1ZykuLlZWVpZyc3OVl5en4uJipaSknPV6SkqKtmzZokmT\nJqmwsFBjx46teb2goECDBg1Sr169tHz5cknSnj17NHDgQOP5mjZtqnfffVdt27b1ulfYgfn3ff63\nb9+u5ORkSVJ0dLQ6d+6s7du3e903QoPZr7uzHzYBZfPmzWrZsqXatGnjeC06Ovqsn0tLS7VixQpd\nc801mj59umJjY1VYWKh3331X8+fPV0lJya+eb926dRo6dKgKCwvVvXt3zZ07V5I0depUJScn6+OP\nP9a9996rTZs2+fR5tm7dqqysLE2bNs3tmk6dOunuu+9W//79lZ+fL0mqrq7WiRMntGzZMk2YMEEz\nZsyQJLVo0aJmYH/piiuucPwZIbww/77Pf3JyslauXKmTJ09qz549+te//qUePXr41DeCj9mvu7Mf\nNgHl0KFDZ13GO3z4sFJTU5WamqqbbrpJc+bMqXmtV69eiow89dHWrl2roUOHSpLi4uKUkpKioqKi\nXz1f+/bta363mZiYqN27d0uSSkpKlJaWJklKSkpSu3btfPo8F154YU2y9YbL5VJ6enpNXz/88INP\n50d4Yf5P8WX+s7Oz9e9//1vdu3fXzTffrP79+6tjx45enxuhweyfUhdnP2wCStOmTbV3796an2Ni\nYrRq1SqtWrVKN954o44dO1bzWmxsbM0/HzhwQDExMWe9r7y8/FfP17hx45p/joqKqrmcd+jQobNS\n+5nH9saZPXojKipKDRs2lCRFRkaqurrap+MgvDD//+vF2/mfMGGC+vfvr5KSEq1fv14bNmzQhx9+\n6NP5EXzM/v96qWuzHzYBpWvXriovL/f6LuZmzZqpoqKi5ueKigo1a9ZMUVFRqq6ulsvlknQqlXsi\nJiZGlZWVNT8fOHDAq35MzvyPwJteUHcw/74rKirSwIEDFRERobi4OPXs2VMbN26s1XMgcJh934X7\n7IdNQImOjtbo0aM1fvx4ffvtt5JO/U5uxYoVWrlypVq3bm18X+/evbVgwQJJpwZq9erV6t27t5o0\naaKoqCht27ZNkrRkyRKP+ujatatWr14tSdq0aZN27tzp70dT8+bNtW/fPpWXl6uqqkrLli2rea1e\nvXr68ccf/T4Hwhvz77u2bdtqzZo1kqRjx46puLhYHTp08OuYCB5m33fhPvthtc145MiRiouLU05O\njn7++WcdP35cbdu21axZs3TDDTcY3zNmzBg99dRTSk1NVWRkpEaNGlWz3TY7O1sPPPCA4uPjNWzY\nMI96eOyxx5SXl6elS5eqS5cutbJd6/LLL1dGRobS09OVkJCg22+/XV9++aUkqWfPnnrjjTeUkZGh\nmTNnuj3Gnj17NGLECOPNUjNmzNCqVat08OBBVVVV6fPPP1dKSory8vL87h3Bw/z7Nv/PP/+8nn76\nab333ntyuVy68cYbNWTIEL/7RvAw+3Vz9iNcp69zAQAAWCJsfsUDAADqDgIKAACwDgEFAABYh4AC\nAACsQ0ABAADWOec244iIiGD1Aci2DWXMP4LJpvln9hFM7mafKygAAMA6BBQAAGAdAgoAALAOAQUA\nAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENA\nAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYp16oG4Dn+vXrZ6z/9re/ddSe\neeaZQLcDAEDAcAUFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB12MUTRjIyMoz1rVu3BrkTnK+u\nvfZaY33UqFEe1STpq6++MtY/+OADR2327NnGtTt37nTXIhBUCQkJjtrTTz9tXHv//fcb6y6Xy1Hb\nsmWLcW1OTo6jtmbNmnO1eN7iCgoAALAOAQUAAFiHgAIAAKxDQAEAANaJcJnu3jn9YkREMHvBGRo1\nauSoffnll8a1paWljlpaWlqt9xRo5xjFkDjf579v376O2tSpU41rr7rqqoD0UFFRYaw/+uijjtpb\nb71lXFtVVVWrPYWKTfN/vs++NxYuXOio3XnnnQE73/Hjxx21V155xbi2pKTEUfvpp5+Ma//2t7/5\n11gAuZt9rqAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOj7q31ODBgx21Sy+91Lh2yZIlgW4H\nYeyiiy4y1t9//31HLTo6OtDtnCUuLs5Yf/311x010842Sfrzn/9cqz2hburRo4exfuuttwa1jwYN\nGjhqubm5Hr//k08+MdZt3sXjDldQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh108lho3bpyj\n5u77MUy7MYDT3M2Nvzt23H3nx8GDB411d7vQPPXcc88Z66bvqHK3kwFwZ/z48cZ6w4YNPT7G0aNH\njfXvvvvOUWvVqpVx7QUXXODx+Ux+/PFHv95vE66gAAAA6xBQAACAdQgoAADAOgQUAABgHW6SDbHh\nw4cb6127dnXUtmzZYlz72Wef1WpPgCcmT55srLu7qfDJJ5/063zuHtk/YcIER42bZOGtxo0b+32M\nKVOmGOuTJk1y1B555BHj2lmzZnl8vp9//tlRmzp1qsfvtx1XUAAAgHUIKAAAwDoEFAAAYB0CCgAA\nsA4BBQAAWIddPEHUpk0bR83d47tPnDjhqI0cOdK41t3jlQHJ/XwsXLjQURsyZIjHx3X3aPBGjRp5\nfIzacPLkyaCeD3CnvLzc47VdunTxeK3L5TLWZ8yY4aitX7/e4+PajisoAADAOgQUAABgHQIKAACw\nDgEFAABYh5tkg2jixImOWvPmzY1rp02b5qht2LCh1nvC+a+6utpYX758uaPmzU2ycXFxPvfki82b\nNxvr7h4vDthi2LBhjlpmZqbH71+zZo2xbvqah/MJV1AAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUA\nAFiHXTwBcM899xjrI0aMcNSOHz9uXJufn1+rPQG/VFRU5Kjt37/fuLZZs2YB6eH999831mfOnOmo\nudvFVlVVVas9Ab4y7daRpIsuushRi4mJ8fi4n332mc89hTOuoAAAAOsQUAAAgHUIKAAAwDoEFAAA\nYB1ukvVDfHy8sZ6Tk2Osu1wuRy03N9e4dvfu3b43BpwhIiLCWB8wYICjFuzH15eVlRnrxcXFjho3\nwyKQ3N0g7o3rrrvO72Ns377dUfvTn/7k93HDEVdQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABY\nJ8Jl2lpy+kU3d//jlEcffdRYnzJlirH+9ttvO2r333+/cW1d3LFwjlEMifNl/gcPHmysFxQUBLkT\nz6WlpTlqhYWFIegkeGya//Nl9r1x8cUXG+tff/21o+bNY+rdcfc1J7fccoujtn79er/PZzN3s88V\nFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1uG7eDzUsmVLR23y5MnGtZWVlcb6K6+84qjVxd06\nCK5Bgwb5fYw//OEPjtq6deuMa8ePH2+su9tNZJKZmemone+7eBBa5eXlxvrf//53Ry09Pd3v882Z\nM8dYP9937HiDKygAAMA6BBQAAGAdAgoAALAOAQUAAFiHm2Q9NHbsWEftggsuMK594YUXjPUNGzbU\nak/ALyUmJjpqGRkZHr+/tLTUWDfd4H3kyBHj2gULFhjrphsL3T1S/e6773bU3nnnHePaNWvWGOuA\nzZYvXx7qFqzHFRQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANZhF88vJCUlGeu5ubmOWllZmXHt\n1KlTa7UnwFNXXnmloxYdHe3x+/Pz8411dzt2TBYtWmSsmx51f9dddxnX1q9f31G77rrrjGvZxYPa\nUK+e+f8OW7VqFZDz3XTTTcY6X+nwP1xBAQAA1iGgAAAA6xBQAACAdQgoAADAOtwk+wvDhw831k03\nUM2fP9+49vDhw7XaE+CprKwsv96/Y8eOWurE6ZlnnnHU3N0ka+Ju7ZQpU3zuCTitXbt2xvq1114b\n5E5wGldQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYJ8LlcrncvhgREcxegq5ly5aO2pYtW4xr\njx496qglJycb1+7cudO/xuqoc4xiSITj/A8ZMsRRc7fbzOSdd94x1u+55x6fezotPj7eUdu9e7fH\n7//uu++M9datW/vck01smv9wnH1/uXvEfEpKSkDO9/XXXxvrHTp0CMj5bOZu9rmCAgAArENAAQAA\n1iGgAAAA6xBQAACAder0o+5vvfVWRy0uLs64dt68eY4aN8PCNv/4xz8ctYMHDxrXNmnSxFHr3bu3\ncW1sbKyjdujQIe+aAywWGRncv6+3adPGWO/WrZujVlJSEuBu7MQVFAAAYB0CCgAAsA4BBQAAWIeA\nAgAArENAAQAA1qkTu3jq1TO2F2cYAAACYUlEQVR/zIceeshRO3LkiHHtzJkza7UnIBC+//57R23j\nxo3Gtf369XPULr30UuPatWvXOmqvvvqqV71df/31Xq0HzmdRUVHG+rXXXuuosYsHAADAEgQUAABg\nHQIKAACwDgEFAABYh4ACAACsUyd28bj7fp2rr77aUSsqKjKu/eabb2q1JyBYnn76aWPdtKsmOjra\nuPaqq65y1GbPnu1fY14yfc8QUFuOHj0a6hYkSZ06dQp1C9bgCgoAALAOAQUAAFiHgAIAAKxDQAEA\nANapEzfJVlZWGusFBQWOWkRERKDbAYJq/fr1xnpGRoaj9uKLLxrXJiUl1WpPv+a7775z1CZNmhTU\nHlC3jB492lg33bTatm1bv89XXV1trH/00Ud+H/t8wRUUAABgHQIKAACwDgEFAABYh4ACAACsQ0AB\nAADWiXC5XC63L7KjBUF0jlEMibo4/40bNzbWL7/8ckdt5MiRxrVDhgwx1uPj4x2177//3rh2wIAB\njlppaalx7fnCpvmvi7PvTrdu3Ry19957z7i2Xbt2Hh937dq1xvrNN9/s8THOF+5mnysoAADAOgQU\nAABgHQIKAACwDgEFAABYh5tkYQ2bbhKUmH8El03zz+wjmLhJFgAAhA0CCgAAsA4BBQAAWIeAAgAA\nrENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAA\nAADrEFAAAIB1CCgAAMA6ES6XyxXqJgAAAM7EFRQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYh\noAAAAOv8P8tGCI/pSJ3uAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5bed729f98>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UDSYGuEKUD74",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OePx5QzSUNbQ",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_XHyfbBUQmI",
        "colab": {}
      },
      "source": [
        "network = Net()\n",
        "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
        "                      momentum=momentum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3pKj3WgUW4q",
        "outputId": "184b8742-6d47-4fc6-9c9f-ce4f68112e1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(network)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2_drop): Dropout2d(p=0.5)\n",
            "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
            "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HpjRabvUUZig",
        "colab": {}
      },
      "source": [
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6v_WgC_XUfwI",
        "colab": {}
      },
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xL5u97ueU_Og",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sg70WNpWUwnJ",
        "colab": {}
      },
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      output = network(data)\n",
        "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  test_losses.append(test_loss)\n",
        "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "    test_loss, correct, len(test_loader.dataset),\n",
        "    100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IqiYMk3AUzzY",
        "outputId": "884c29f0-9e41-4a8e-9378-e2f38f235167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5086
        }
      },
      "source": [
        "test()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 2.3078, Accuracy: 628/10000 (6%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.362025\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.289933\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.291891\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.288467\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.242674\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.259193\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.253553\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.196242\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.142521\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.085870\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.149853\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.963920\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.893729\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.817108\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.705804\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.428541\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.345388\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.336809\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.160391\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.124189\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.319039\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.097037\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 1.040970\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.801618\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.887169\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.090481\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 1.091631\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.817938\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.659770\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 1.175809\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.646873\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.803679\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.847082\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.721968\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.932368\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.816115\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.787486\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.739069\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.695625\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.792010\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.649382\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.631877\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.564670\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.624809\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.569690\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.624802\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.618333\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.526337\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.714820\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.518163\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.597949\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.543396\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.439167\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.946855\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.645736\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.432629\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.677677\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.648278\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.530108\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.635498\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.604702\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.545781\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.612677\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.646472\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.442542\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.417540\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.530044\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.481959\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.414147\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.502099\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.659981\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.328328\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.354984\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.537045\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.389318\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.922827\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.326336\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.349139\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.474791\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.484637\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.407444\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.899682\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.431270\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.427875\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.482770\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.447571\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.691974\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.501848\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.523152\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.339798\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.440649\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.635118\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.367366\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.623719\n",
            "\n",
            "Test set: Avg. loss: 0.1958, Accuracy: 9426/10000 (94%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.280587\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.391879\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.422928\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.508240\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.456054\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.483971\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.430119\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.388580\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.349738\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.212993\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.593394\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.341823\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.359613\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.364589\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.213215\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.350422\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.405329\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.533178\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.336648\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.563343\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.412442\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.409323\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.677314\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.355002\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.495777\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.435362\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.521199\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.464494\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.396503\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.434241\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.430859\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.456047\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.403376\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.322058\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.380275\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.426341\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.251527\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.326623\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.189038\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.743423\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.418712\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.381397\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.580966\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.422122\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.482254\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.233697\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.464113\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.295370\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.360488\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.341920\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.282011\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.538942\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.230522\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.472044\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.359734\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.213942\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.295433\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.343520\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.226510\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.286761\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.311231\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.358493\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.519886\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.475562\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.263514\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.519041\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.459164\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.473354\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.249761\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.309748\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.554218\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.207339\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.244845\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.355448\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.391673\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.206531\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.389100\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.440067\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.352176\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.399527\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.298971\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.311719\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.306022\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.257955\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.671964\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.147295\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.332345\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.278165\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.343398\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.355160\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.345428\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.351796\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.407963\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.366157\n",
            "\n",
            "Test set: Avg. loss: 0.1188, Accuracy: 9642/10000 (96%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.304249\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.401061\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.223941\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.313374\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.204246\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.270924\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.315207\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.221554\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.359634\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.318444\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.251301\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.546083\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.548665\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.287627\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.382664\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.523736\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.303728\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.334616\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.106162\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.226554\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.286374\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.343674\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.523962\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.368810\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.289495\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.428925\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.461929\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.309727\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.207836\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.200933\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.232628\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.203071\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.327981\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.201257\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.598296\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.293646\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.312054\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.260643\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.291572\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.803564\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.364907\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.375942\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.257359\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.391130\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.366757\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.426961\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.276003\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.508597\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.371846\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.326291\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.204466\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.124851\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.412365\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.275590\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.278837\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.252295\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.534556\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.123538\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.262403\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.149373\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.437077\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.951254\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.362806\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.316644\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.221248\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.229833\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.150583\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.500905\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.281510\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.163680\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.264597\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.300393\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.258759\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.164161\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.588230\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.345010\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.451706\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.259948\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.488908\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.294887\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.254597\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.276913\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.326584\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.234516\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.335788\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.224990\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.242022\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.436767\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.293985\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.162592\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.319599\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.295076\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.297297\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.140032\n",
            "\n",
            "Test set: Avg. loss: 0.0952, Accuracy: 9703/10000 (97%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EIEvhqAQU_9g",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lalFiUB6WPmq"
      },
      "source": [
        "**Evaluating the Model's** **Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lBKywY7DU4FZ",
        "outputId": "74a2ad7b-0113-4bde-ad38-4ac70af04ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8333
        }
      },
      "source": [
        "\n",
        "for i in range(4,9):\n",
        "  test_counter.append(i*len(train_loader.dataset))\n",
        "  train(i)\n",
        "  test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.256014\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.191865\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.355651\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.179464\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.239450\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.317984\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.253163\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.547503\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.214091\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.184661\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.118751\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.297275\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.215770\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.289820\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.185903\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.207649\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.210765\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.171862\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.297926\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.164916\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.216986\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.207865\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.220320\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.315309\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.228359\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.171561\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.173027\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.416028\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.182904\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.388877\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.449631\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.120953\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.195354\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.294915\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.150296\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.264506\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.166406\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.277727\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.213233\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.273609\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.296003\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.224742\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.357221\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.221919\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.209158\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.149932\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.327109\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.205184\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.267200\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.382301\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.211788\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.240557\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.138842\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.250126\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.251855\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.212960\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.244027\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.221096\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.254536\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.186266\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.432497\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.330239\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.272716\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.212593\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.274948\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.241328\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.433210\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.148146\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.339916\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.227341\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.233336\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.287627\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.299958\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.166093\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.138275\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.253858\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.192768\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.407392\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.200380\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.288721\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.170272\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.377076\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.135161\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.302487\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.316077\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.255233\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.178906\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.258059\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.141710\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.362198\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.330260\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.149171\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.262863\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.258103\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Avg. loss: 0.0807, Accuracy: 9740/10000 (97%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.217236\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.268043\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.284232\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.334542\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.233531\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.355643\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.328393\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.354256\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.286955\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.164742\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.313874\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.427233\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.231068\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.153646\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.250476\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.261483\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.261759\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.355156\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.314081\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.194796\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.248247\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.327296\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.643930\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.263630\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.296072\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.350028\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.259092\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.115149\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.344856\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.125233\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.144200\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.287010\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.315088\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.184766\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.162697\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.117621\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.204659\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.276794\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.316131\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.151877\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.400182\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.297477\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.311870\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.176838\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.274974\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.322910\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.266704\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.110175\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.155815\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.445470\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.447775\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.122083\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.487942\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.273988\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.311777\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.279944\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.197926\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.182828\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.322513\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.323595\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.301266\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.146014\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.126731\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.233232\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.414494\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.230560\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.160476\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.318720\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.186747\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.539294\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.325798\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.240789\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.104531\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.188602\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.095682\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.333903\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.256879\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.233581\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.100831\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.178391\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.217614\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.097238\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.311675\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.200401\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.358597\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.186926\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.171675\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.249989\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.177451\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.129538\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.138895\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.442295\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.056931\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.327191\n",
            "\n",
            "Test set: Avg. loss: 0.0718, Accuracy: 9765/10000 (97%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.162934\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.214829\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.147721\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.280168\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.483028\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.271664\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.364358\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.283073\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.184336\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.188527\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.326657\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.159824\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.299432\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.177869\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.140910\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.325806\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.145360\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.282055\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.309166\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.281967\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.331009\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.205399\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.179659\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.294113\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.249662\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.185944\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.223792\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.254659\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.209347\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.153251\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.133000\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.172676\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.185291\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.220752\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.392223\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.318685\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.158681\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.102804\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.163787\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.289648\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.145417\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.148196\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.286796\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.109285\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.201280\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.144361\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.270446\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.235029\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.209283\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.207799\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.308139\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.148025\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.077091\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.269055\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.213545\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.371840\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.129304\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.185893\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.254947\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.230950\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.303288\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.109196\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.111914\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.091179\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.147279\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.081002\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.227574\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.271361\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.197974\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.357522\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.182711\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.124240\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.485832\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.477811\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.186454\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.220827\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.298860\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.142799\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.188875\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.132158\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.107212\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.446270\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.173038\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.160791\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.118105\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.107752\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.390490\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.198093\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.194491\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.214130\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.324427\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.175455\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.256460\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.218492\n",
            "\n",
            "Test set: Avg. loss: 0.0658, Accuracy: 9777/10000 (97%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.118163\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.298160\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.183843\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.283796\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.319361\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.126914\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.089634\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.174873\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.220320\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.154277\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.262841\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.156970\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.143002\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.130390\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.149912\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.290118\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.141268\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.251422\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.083058\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.173304\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.089263\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.114390\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.383069\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.092681\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.452889\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.288864\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.216056\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.191369\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.173749\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.094889\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.088040\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.116227\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.175591\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.173773\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.206455\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.117230\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.253173\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.331520\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.135395\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.224511\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.105587\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.258379\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.267171\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.260559\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.114989\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.247664\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.079204\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.241615\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.149018\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.272526\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.194373\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.217198\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.048715\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.270161\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.160608\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.271629\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.244357\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.176268\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.278867\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.232871\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.329672\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.165592\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.253270\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.518741\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.188155\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.299428\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.150923\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.119779\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.265938\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.193879\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.183047\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.208990\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.196524\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.332622\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.211260\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.152139\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.161696\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.274284\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.313508\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.117194\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.654772\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.157582\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.292166\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.579877\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.271210\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.297422\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.111394\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.101429\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.155384\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.154634\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.176208\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.189606\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.499391\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.110744\n",
            "\n",
            "Test set: Avg. loss: 0.0589, Accuracy: 9800/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.223264\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.101770\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.156699\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.224264\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.128017\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.211597\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.293498\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.082702\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.173243\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.130965\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.185239\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.121597\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.148370\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.320710\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.275099\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.209632\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.108424\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.232514\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.352797\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.153883\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.125608\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.178487\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.181349\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.025857\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.317253\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.287945\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.113035\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.104772\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.228241\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.180830\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.152317\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.124780\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.304012\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.191061\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.088672\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.088008\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.267742\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.299828\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.208005\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.290671\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.127063\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.338080\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.187715\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.222086\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.259670\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.278214\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.104051\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.064353\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.326948\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.204489\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.092129\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.167538\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.274602\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.163096\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.136739\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.159214\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.110379\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.156098\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.132339\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.141022\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.193736\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.212749\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.103570\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.112830\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.078604\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.134204\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.133521\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.209894\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.193718\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.178380\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.155384\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.231009\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.312383\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.452246\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.171389\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.341903\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.193049\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.200712\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.124505\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.127469\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.100864\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.288317\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.080415\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.302338\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.162243\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.114874\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.161691\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.217046\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.152350\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.171725\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.138380\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.124152\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.317345\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.138772\n",
            "\n",
            "Test set: Avg. loss: 0.0557, Accuracy: 9826/10000 (98%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bVUQOqzUXBzH",
        "outputId": "f6d1dc1b-18cc-4dc3-eb06-c03a21495bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "  output = network(example_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OGmNZCCWYNpm",
        "outputId": "279e7431-ae95-48db-c857-bac2cf91ad84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Prediction: {}\".format(\n",
        "    output.data.max(1, keepdim=True)[1][i].item()))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "fig"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFvCAYAAABguDDMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XtQ1XX+x/H3EUELIRk3NVFqnTYv\njJbiBRc3FwOlsswQajcvS01taaVZdllr3AGyMTVNV6q1dmeKxkJz7bZtNV10w0tQ1k6UtrnTePCS\nFwiQNJHz+f3RyC/9fg4eOLc35zwff8WLz/l+Pxw/6cuv38/3uIwxRgAAABTpFO4JAAAAnImCAgAA\n1KGgAAAAdSgoAABAHQoKAABQh4ICAADUibiCMmDAAMnOzpacnByZOHGi5ObmytatW/0+bklJiTz4\n4IMiIjJz5kypqqpqdXxZWVnLf/sy3l87d+6U1NRU2b59e1DPA91Y/6z/aBVta/+TTz6RvLw8ufLK\nK+X666+XioqKoJwnrEyEueSSS8z+/ftbvq6srDQjR440R44c8eu4q1evNg888IBPY0+ePGnS0tL8\nOl9bNDc3mxtuuMFcfvnlZtu2bSE7L/Rh/bP+o1U0rf0ff/zRjBo1ymzdutUYY8yHH35oxo4dG/Tz\nhlrEXUE5U1pamqSkpMiOHTukurpaxo4dK4sWLZJp06aJyE8tNDc3V7KzsyU/P1/cbreIiBw/flzm\nzp0rmZmZMm3aNDlw4EDLMcePHy+VlZUiIrJx40aZOHGiTJw4UebPny8nTpyQgoICaWhokJycHHG7\n3aeNf+utt2TSpEmSk5MjM2bMkD179oiIyKpVq6SwsFBmz54tV1xxhUydOlUOHjwoIiKlpaWyYsUK\nrz/j2rVrZeDAgZKSkhL4NxAdGusf0SqS135TU5MUFRVJenp6y8968OBBqa+vD9K7GSbhbkiBdmaL\nNsaYyZMnm82bNxu3221SU1PNhg0bjDHGNDQ0mJEjR5qPPvrIGGPM66+/bqZMmWKMMaa0tNTcdNNN\npqmpydTU1JjMzMyWFp2ZmWkqKiqM2+026enp5sCBA8bj8ZjZs2ebNWvWGLfbbQYNGtRy/lPj9+7d\na9LS0sy3335rjDHmueeeMzNnzjTGGLNy5UozZswYU11dbTwej7nttttMSUnJWX/egwcPmgkTJpi6\nujozbdo0/gYZ5Vj/rP9oFW1r/+fefPNNM2HChLa/acpF/BWUTZs2yeHDh2X48OEi8lPzzM7OFpGf\nGnSvXr0kIyNDREQmTZoke/bskX379kllZaVkZ2dL586dJSkpSTIzMx3HLi8vl2HDhkmvXr3E5XLJ\nsmXL5A9/+IPXuZSXl8vo0aPlwgsvFBGRvLw82b59u5w8eVJEREaMGCHJycnicrlk0KBBsn///rP+\nfIsWLZLZs2dLYmJim94XRAfWP6JVpK/9U3bu3CmLFi2SwsJCn1/TUXQO9wSCYfr06RITEyPGGElO\nTpY1a9ZIfHy81NbWSkxMjHTr1k1EROrr68XtdktOTk7La+Pi4qSmpkbq6uokISGhJU9MTJTGxsbT\nzlNbW3vab4xdunRpdV5njk9ISBBjjNTW1rZ8fUpMTIw0Nze3erx///vf8v3338u1117b6jhEF9Y/\nolW0rP1TPv30U5k7d648+uijMnr0aJ9e05FEZEF54YUXpHfv3mcd17NnT+nfv79s2LDB8b3ExERp\naGho+bqmpsYxJikpSXbs2NHy9dGjR+X48eNez9ejR4/TxtfV1UmnTp0kKSnprHO1effdd+XLL79s\n+VtAXV2d3HXXXfKnP/1JrrvuunYdEx0f65/1H62iZe2L/HTlZM6cObJ8+XIZMWJEu4+jWcT/E09r\nLr30Ujl06JB8/vnnIiLidrtl/vz5YoyRyy67TN5//31pbm6Wmpoa2bx5s+P148aNk08//VSqq6vF\nGCMLFy6U9evXS2xsrHg8Hjl69Ohp4zMyMqSysrLlZqyXXnpJMjIypHPn9vXEwsJC2b59u5SXl7dc\ncly1ahW/OcMnrH9Eq46+9o0x8uCDD8rChQsjtpyIROgVFF917dpVVq5cKUVFRdLY2CixsbEyZ84c\ncblckp+fL5WVlZKVlSV9+vSRrKys01q1iEjv3r2lsLBQZs6cKTExMTJkyBApKCiQ2NhYSUtLk8zM\nTHnmmWdOG19cXCyzZs2SpqYm6du3rxQVFZ11nqWlpXL48GGZO3duwN8DRC/WP6JVR1/7n332meza\ntUuWLl0qS5cubcmXLVsmqampfr47eriMMSbckwAAAPi5qP4nHgAAoBMFBQAAqENBAQAA6lBQAACA\nOhQUAACgTqvbjF0uV6jmAYi2DWWsf4SSpvXP2kcoeVv7XEEBAADqUFAAAIA6FBQAAKAOBQUAAKhD\nQQEAAOpQUAAAgDoUFAAAoA4FBQAAqENBAQAA6lBQAACAOhQUAACgDgUFAACoQ0EBAADqUFAAAIA6\nFBQAAKAOBQUAAKhDQQEAAOpQUAAAgDoUFAAAoA4FBQAAqNM53BMAgDOVlZU5svT0dOvYlJSUYE8H\nQBhwBQUAAKhDQQEAAOpQUAAAgDoUFAAAoA4FBQAAqBOWXTwej6dNuc0NN9xgzV955ZV2zQlA6PXr\n18+a5+XlObJ58+YFezpAyBQUFFjz+Ph4R+Z2u61jX3311YDOSRuuoAAAAHUoKAAAQB0KCgAAUIeC\nAgAA1OmwN8kC6PimTp3q89itW7cGcSZA8NhuiP3rX/9qHdupk/O6wXvvvWcdy02yAAAAIUZBAQAA\n6lBQAACAOhQUAACgDgUFAACoE5ZdPNEqNTXVkbVlF0NJSYk1P3ToULvnhOhle8x8cnKydey2bduC\nMocnnnjC57HBmgPQmtjYWEc2adIk69hx48ZZc9tHs9h263hTVVXl89hIwhUUAACgDgUFAACoQ0EB\nAADqUFAAAIA6FBQAAKAOu3iC4OWXX7bmffr0cWSjR4/2+bijRo2y5ldffbXPxwBOsa3TMWPGWMe6\nXC6/z2fbNQSEQ3x8vCObMGGCdeyCBQsc2bBhw6xjvf1/Yoxpw+yczjvvPL9e31FxBQUAAKhDQQEA\nAOpQUAAAgDoUFAAAoA43yQZBbm6uNfd4PH4dNzs726/XIzrdc8891tx2Q6zb7Q7aPNrysQ5bt24N\n2jwQPbp162bNr7rqKke2du1av88XrJtkr732Wmu+atUqR3bXXXf5dS5NuIICAADUoaAAAAB1KCgA\nAEAdCgoAAFCHggIAANQJyy6eTp3870XeHid/3XXXObI33njD7/O1RSB+vra4+OKLHdk333wT0jlA\nB9vj5J944gmfX3/fffcFcjqn8fYYfZvq6uqgzQPRY/DgwdY8EDt2QikpKcma/+53v3NkpaWl1rHb\nt28P6JxCgSsoAABAHQoKAABQh4ICAADUoaAAAAB1wnKTrLdHvvv7KHgRkQ0bNjiyuLg4v4/bFv/6\n17+seVZWVlDOZ/uZhw4dGpRzQbfy8nKfx9oea19WVhbI6Zymb9++Po/lUfcIhBMnTljzgwcPOrLY\n2Fjr2B9++MHn8z388MPWvC2Pul+0aJEj69Onj3Ws7ebZCy64wOdzaccVFAAAoA4FBQAAqENBAQAA\n6lBQAACAOhQUAACgTlh28ZSUlFjz22+/PcQzCY7du3db82Dt4kH0SU9Pt+a2R917E8zH2tu05VH3\n69evD+JMEC0+++wza257BH7Pnj2tY3ft2hXQOZ3NQw89FNLzacYVFAAAoA4FBQAAqENBAQAA6lBQ\nAACAOmG5Sdbb44CPHz/uyObOnRvs6QTcHXfcYc0D8Sh/QEQkJSXF72ME67H23m7gbQvbY/iBQKmt\nrfUpQ3hxBQUAAKhDQQEAAOpQUAAAgDoUFAAAoA4FBQAAqBOWXTz19fXWvLi42JF17drVOrYtj8V/\n9dVXrfnkyZN9PoZmF198sSNbsmSJdez8+fODPR1Euby8vKAc19tj/G07/e69996gzAEIlL/97W/W\nfODAgT4fo6KiwpFt3Lix3XPShisoAABAHQoKAABQh4ICAADUoaAAAAB1KCgAAECdsOzi8ca2u2fX\nrl3WsY2NjdY8ISHBkV1zzTXWsf/85z8dWSB2ucTExPh9jLY455xzHFnfvn1DOgeE1tatW/0+hu0z\nc7Zt2+b3cceMGeP3Mdpy3Hnz5jmyFStWWMfyGT8Ih7///e+ObPr06daxxhhHVl1dbR17yy23+Dcx\n5biCAgAA1KGgAAAAdSgoAABAHQoKAABQR9VNsjarV6+25rGxsdb8kUcecWTnnXeedWxWVpYj27Fj\nRxtmZ9fc3GzNPR6P38e2sd1cHIibHaGXt5s9bbm3R8SXlZU5soyMjDadz6YtN8l6O65tzkuXLvX5\nuECgjB492pENGDDAOvaxxx6z5r169XJkLpfL5zl4+3iY4cOH+5QFyvPPPx+0Y9twBQUAAKhDQQEA\nAOpQUAAAgDoUFAAAoA4FBQAAqOMytufqnvpmG+4y1uKOO+5wZKtWrbKODdaumk6d7L0vWOc7cuSI\nI7vrrrusY9evXx+UOQRCK0sxLDri+rftftmzZ4/fx123bp0j8/a4/SeeeMLv87VlN5LtfPfee6/f\ncwg1Tetf89rv06ePNbfttvHmpptucmSDBw+2jrXtwOnevbt1rLf3TdOvrT+C9TEu3t4frqAAAAB1\nKCgAAEAdCgoAAFCHggIAANSJuJtkbXJzc30eO3XqVL+PEeqbZG3nsz3GXETk97//fVDmEAjabiSL\nlPXv7ebSZcuWObK8vLxgT6fdvN182xFviLXRtP41rP2xY8da88WLF1vz9PT0YE7HJ97eN9tN37t2\n7Qr2dNrtnXfesebB+rgJbpIFAAAdBgUFAACoQ0EBAADqUFAAAIA6FBQAAKBOVOziaYvzzz/fmvfo\n0SMo55s1a5Y1v/32230+Brt4giMa17+3HT9jxoxxZN4eLT5v3jyfz+dt7PLly30+RqTQtP6Dtfa9\nPaZ+zZo1jmz8+PHWsXFxcQGd09k0Nzc7sk8++cQ6dvPmzdb8ueeec2Rff/21fxOLIOziAQAAHQYF\nBQAAqENBAQAA6lBQAACAOp3DPQFtDh061KbcXzU1Ndbc2+PybWJiYhxZfn6+dezhw4et+cMPP+zI\n6uvrfZ4DIoPtkdyt5f6Kxptho9m7775rzQcOHBiU8x0/ftyaf/vtt47M2+93xcXFjuzNN9/0a17w\nDVdQAACAOhQUAACgDgUFAACoQ0EBAADqUFAAAIA67OIJM2+P+PV4PH4d19vrvT1C33a3e0VFhXXs\nunXr2j8xAFFr27Zt1jwhIcHnY3z55ZfW/MUXX3Rk3nYtvvXWWz6fD+HDFRQAAKAOBQUAAKhDQQEA\nAOpQUAAAgDoUFAAAoI7LeNtGIiIulyuUc4lK48aNs+YrV650ZIMGDbKOtX1uj7+7gFoTFxcXlOO2\nshTDgvXfOm+f9/Tyyy/7fAze4/+naf3z64JQ8rb2uYICAADUoaAAAAB1KCgAAEAdCgoAAFCHm2SV\nSk9Pd2Rr1661ju3Xr58j4yZZ/7H+28fbr6Pb7XZkKSkpwZ5Oh6Fp/bP2EUrcJAsAADoMCgoAAFCH\nggIAANShoAAAAHUoKAAAQB128XQg/fv3t+aDBw92ZBs2bLCOLSkpseZPP/20z/PYuXOnz2PbQtMu\nBhHWP0JL0/pn7SOU2MUDAAA6DAoKAABQh4ICAADUoaAAAAB1uEkWami6SVCE9Y/Q0rT+WfsIJW6S\nBQAAHQYFBQAAqENBAQAA6lBQAACAOhQUAACgDgUFAACoQ0EBAADqUFAAAIA6FBQAAKAOBQUAAKhD\nQQEAAOpQUAAAgDoUFAAAoA4FBQAAqENBAQAA6lBQAACAOi5jjAn3JAAAAH6OKygAAEAdCgoAAFCH\nggIAANShoAAAAHUoKAAAQB0KCgAAUIeCAgAA1KGgAAAAdSgoAABAHQoKAABQh4ICAADUoaAAAAB1\nIq6gDBgwQLKzsyUnJ0cmTpwoubm5snXrVr+PW1JSIg8++KCIiMycOVOqqqpaHV9WVtby376Mby9j\njDz77LOSmpoqlZWVQTkHOo5oW/8ffvihTJ48WXJycuTGG2+U//znP0E5D/Rj7Ufg2jcR5pJLLjH7\n9+9v+bqystKMHDnSHDlyxK/jrl692jzwwAM+jT158qRJS0vz63y+euSRR8yCBQvM2LFjTUVFRUjO\nCb2iaf3X1dWZ4cOHm6+++soYY8ymTZvM5ZdfHvTzQifWfuSt/Yi7gnKmtLQ0SUlJkR07dkh1dbWM\nHTtWFi1aJNOmTRMRkU8++URyc3MlOztb8vPzxe12i4jI8ePHZe7cuZKZmSnTpk2TAwcOtBxz/Pjx\nLVcrNm7cKBMnTpSJEyfK/Pnz5cSJE1JQUCANDQ2Sk5Mjbrf7tPFvvfWWTJo0SXJycmTGjBmyZ88e\nERFZtWqVFBYWyuzZs+WKK66QqVOnysGDB0VEpLS0VFasWGH9+aZMmSLFxcUSGxsbnDcQHVokr3+3\n2y3nnHOODBw4UERE0tPT5cCBA1JfXx+kdxMdCWs/AoS7IQXamS3aGGMmT55sNm/ebNxut0lNTTUb\nNmwwxhjT0NBgRo4caT766CNjjDGvv/66mTJlijHGmNLSUnPTTTeZpqYmU1NTYzIzM1tadGZmpqmo\nqDBut9ukp6ebAwcOGI/HY2bPnm3WrFlj3G63GTRoUMv5T43fu3evSUtLM99++60xxpjnnnvOzJw5\n0xhjzMqVK82YMWNMdXW18Xg85rbbbjMlJSU+/9ynzoHoFk3r/9ixY+Y3v/mN2bJlizHGmNdee81c\nf/31fr6D6KhY+5G39juHuyAF26ZNm+Tw4cMyfPhwqa2tlaamJsnOzhaRnxp0r169JCMjQ0REJk2a\nJH/+859l3759UllZKdnZ2dK5c2dJSkqSzMxMaWxsPO3Y5eXlMmzYMOnVq5eIiCxbtkxiYmJOa9xn\njh89erRceOGFIiKSl5cnS5YskZMnT4qIyIgRIyQ5OVlERAYNGiT79+8P/BuCqBLJ679r165SVFQk\nf/zjH6Vr167i8Xjk2Wefbec7hUjD2u/4IrKgTJ8+XWJiYsQYI8nJybJmzRqJj4+X2tpaiYmJkW7d\nuomISH19vbjdbsnJyWl5bVxcnNTU1EhdXZ0kJCS05ImJiY5FWltbK4mJiS1fd+nSpdV5nTk+ISFB\njDFSW1vb8vUpMTEx0tzc3I6fHtEuWtb/d999JwsWLJB169bJgAEDZPv27XLnnXfK22+/LfHx8a2+\nFpGJtR9Zaz8iC8oLL7wgvXv3Puu4nj17Sv/+/WXDhg2O7yUmJkpDQ0PL1zU1NY4xSUlJsmPHjpav\njx49KsePH/d6vh49epw2vq6uTjp16iRJSUlnnSvgq2hZ/zt27JC+ffvKgAEDRERk9OjR0qlTJ9m9\ne7cMHTq0XcdEx8baj6y1H/E3ybbm0ksvlUOHDsnnn38uIj/deDR//nwxxshll10m77//vjQ3N0tN\nTY1s3rzZ8fpx48bJp59+KtXV1WKMkYULF8r69eslNjZWPB6PHD169LTxGRkZUllZ2XIz1ksvvSQZ\nGRnSuXNE9kQo19HX/0UXXSTffPONVFdXi4hIVVWVNDQ0SEpKSruOh+jB2u8YovpPxq5du8rKlSul\nqKhIGhsbJTY2VubMmSMul0vy8/OlsrJSsrKypE+fPpKVlXVaqxYR6d27txQWFsrMmTMlJiZGhgwZ\nIgUFBRIbGytpaWmSmZkpzzzzzGnji4uLZdasWdLU1CR9+/aVoqKis86ztLRUDh8+LHPnznV8b9Kk\nSXLy5En57rvvZP78+dKlSxd5/PHHI6pFIzg6+vofOHCg3HvvvXLrrbeKx+ORuLg4WbJkiXTv3j0w\nbxAiFmu/Y3AZY0y4JwEAAPBzUf1PPAAAQCcKCgAAUIeCAgAA1KGgAAAAdSgoAABAnVa3GbtcrlDN\nAxBtG8pY/wglTeuftY9Q8rb2uYICAADUoaAAAAB1KCgAAEAdCgoAAFCHggIAANShoAAAAHUoKAAA\nQB0KCgAAUIeCAgAA1KGgAAAAdSgoAABAHQoKAABQh4ICAADUoaAAAAB1KCgAAEAdCgoAAFCHggIA\nANShoAAAAHUoKAAAQB0KCgAAUKdzuCcA302YMMGajxw50pE9+uijwZ4OAABBwxUUAACgDgUFAACo\nQ0EBAADqUFAAAIA6FBQAAKAOu3g6kNzcXGv+5ZdfhngmiFRpaWnW/LbbbvMpExHZuXOnNX/ttdcc\n2erVq61j9+zZ422KQEj16dPHkRUVFVnH3nzzzdbcGOPIqqqqrGPvvvtuR/bBBx+0NsWIxRUUAACg\nDgUFAACoQ0EBAADqUFAAAIA6LmO7e+fUN12uUM4FPxMfH+/IvvrqK+vYL774wpFdddVVAZ9TsLWy\nFMMi0td/VlaWI1u6dKl17JAhQ4Iyh++//96a33fffY7s+eeft45tbm4O6JzCRdP6j/S13xZlZWWO\nbOrUqUE734kTJxzZU089ZR1bWVnpyH744Qfr2H/84x/+TSyIvK19rqAAAAB1KCgAAEAdCgoAAFCH\nggIAANShoAAAAHV41L1SU6ZMcWTJycnWsRs3bgz2dNCBnXvuudb8lVdecWTdunUL9nRO0717d2v+\n7LPPOjLbzjYRkb/85S8BnROiU3p6ujW/+uqrQzqPuLg4RzZnzhyfX//ee+9Zc827eLzhCgoAAFCH\nggIAANShoAAAAHUoKAAAQB0KCgAAUIddPErNmzfPkXn7fAzbbgzgFG/rxt8dO94+86O2ttaae9uF\n5qvHHnvMmts+o8rbTgbAm/vvv9+an3POOT4f49ixY9Z87969jqxfv37WsV26dPH5fDYNDQ1+vV4T\nrqAAAAB1KCgAAEAdCgoAAFCHggIAANThJtkwKygosOaXXXaZI6uqqrKO/fjjjwM6J8AXxcXF1tzb\nTYWPPPKIX+fz9sj+hx56yJFxkyzaKiEhwe9jLF682JoXFhY6sjvvvNM6duXKlT6f78cff3RkS5cu\n9fn12nEFBQAAqENBAQAA6lBQAACAOhQUAACgDgUFAACowy6eELroooscmbfHdzc1NTmyW2+91TrW\n2+OVARHv66OsrMyR5efn+3xcb48Gj4+P9/kYgXDy5MmQng/w5siRIz6PvfTSS30ea4yx5itWrHBk\nW7Zs8fm42nEFBQAAqENBAQAA6lBQAACAOhQUAACgDjfJhtCCBQsc2fnnn28du2zZMke2bdu2gM8J\nkc/j8VjzN954w5G15SbZ7t27t3tO7fH5559bc2+PFwe0mD59uiPLy8vz+fUffPCBNbd9zEMk4QoK\nAABQh4ICAADUoaAAAAB1KCgAAEAdCgoAAFCHXTxBMGPGDGt+yy23OLITJ05Yxy5fvjygcwLOVF5e\n7sgOHz5sHfuLX/wiKHN45ZVXrPmTTz7pyLztYmtubg7onID2su3WERE599xzHVliYqLPx/3444/b\nPaeOjCsoAABAHQoKAABQh4ICAADUoaAAAAB1uEnWDz179rTmd999tzU3xjiyOXPmWMfu37+//RMD\nfsblclnzK6+80pGF+vH11dXV1nz79u2OjJthEUzebhBvi1GjRvl9jK+//tqRLVy40O/jdkRcQQEA\nAOpQUAAAgDoUFAAAoA4FBQAAqENBAQAA6riMbWvJqW96ufsfP7nvvvus+eLFi615aWmpI7v55put\nY6Nxx0IrSzEsImX9T5kyxZqvX78+xDPx3VVXXeXI3n777TDMJHQ0rf9IWftt0aNHD2u+e/duR9aW\nx9R74+1jTsaPH+/ItmzZ4vf5NPO29rmCAgAA1KGgAAAAdSgoAABAHQoKAABQh4ICAADU4bN4fHTB\nBRc4suLiYuvYo0ePWvOnnnrKkUXjbh2E1jXXXOP3MR544AFHtnnzZuvY+++/35p7201kk5eX58gi\nfRcPwuvIkSPW/P3333dk1113nd/nW7NmjTWP9B07bcEVFAAAoA4FBQAAqENBAQAA6lBQAACAOtwk\n66N77rnHkXXp0sU69vHHH7fm27ZtC+icgDMNHjzYkeXm5vr8+i+++MKa227wbmxstI59+eWXrbnt\nxkJvj1SfNm2aI3vxxRetYz/44ANrDmj2xhtvhHsK6nEFBQAAqENBAQAA6lBQAACAOhQUAACgDgUF\nAACowy6eMwwdOtSaz5kzx5FVV1dbxy5dujSgcwJ8NWDAAEfWrVs3n1+/fPlya+5tx47NunXrrLnt\nUfc33HCDdWxsbKwjGzVqlHUsu3gQCJ072/847NevX1DOd/nll1tzPtLh/3EFBQAAqENBAQAA6lBQ\nAACAOhQUAACgDjfJnqGgoMCa226gWrt2rXVsfX19QOcE+OrGG2/06/X//e9/AzQTp0cffdSRebtJ\n1sbb2MWLF7d7TsAp/fv3t+ZpaWkhnglO4QoKAABQh4ICAADUoaAAAAB1KCgAAEAdCgoAAFDHZYwx\nXr/pcoVyLiF3wQUXOLKqqirr2GPHjjmyMWPGWMfu2bPHv4lFqVaWYlh0xPWfn5/vyLztNrN58cUX\nrfmMGTPaPadTevbs6cj279/v8+v37t1rzVNSUto9J000rf+OuPb95e0R89nZ2UE53+7du635r371\nq6CcTzNva58rKAAAQB0KCgAAUIeCAgAA1KGgAAAAdaL6UfdXX321I+vevbt17AsvvODIuBkW2nz0\n0UeOrLa21jo2KSnJkf32t7+1jj3vvPMcWV1dXdsmByjWqVNo/75+0UUXWfMRI0Y4ssrKyiDPRieu\noAAAAHUoKAAAQB0KCgAAUIeCAgAA1KGgAAAAdaJiF0/nzvYf8/bbb3dkjY2N1rFPPvlkQOcEBMO+\nffscWUVFhXXshAkTHFlycrJ17KZNmxzZ008/3aa5/frXv27TeCCSxcTEWPO0tDRHxi4eAAAAJSgo\nAABAHQoKAABQh4ICAADUoaAAAAB1omIXj7fP1xk2bJgjKy8vt4793//+F9A5AaFSVFRkzW27arp1\n62YdO2TIEEe2evVq/ybWRrbPGQIC5dixY+GegoiIpKamhnsKanAFBQAAqENBAQAA6lBQAACAOhQU\nAACgTlTcJHv06FFrvn79ekceJhWhAAABg0lEQVTmcrmCPR0gpLZs2WLNc3NzHdmSJUusY4cOHRrQ\nOZ3N3r17HVlhYWFI54DoMmvWLGtuu2n1l7/8pd/n83g81vydd97x+9iRgisoAABAHQoKAABQh4IC\nAADUoaAAAAB1KCgAAEAdlzHGeP0mO1oQQq0sxbCIxvWfkJBgzS+88EJHduutt1rH5ufnW/OePXs6\nsn379lnHXnnllY7siy++sI6NFJrWfzSufW9GjBjhyF566SXr2P79+/t83E2bNlnzzMxMn48RKbyt\nfa6gAAAAdSgoAABAHQoKAABQh4ICAADU4SZZqKHpJkER1j9CS9P6Z+0jlLhJFgAAdBgUFAAAoA4F\nBQAAqENBAQAA6lBQAACAOhQUAACgDgUFAACoQ0EBAADqUFAAAIA6FBQAAKAOBQUAAKhDQQEAAOpQ\nUAAAgDoUFAAAoA4FBQAAqENBAQAA6riMMSbckwAAAPg5rqAAAAB1KCgAAEAdCgoAAFCHggIAANSh\noAAAAHUoKAAAQJ3/A4kk7rGYIkPBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5ba532f630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFvCAYAAABguDDMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noAAAAHUoKAAAQJ3/A4kk7rGYIkPBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f5ba532f630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5wTbS2A4Ybu-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8egCEgc-ZLgA"
      },
      "source": [
        "**bold text** Reference : http://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JKimTTiLZOhl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eLNam21_ZWvh",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pYrfId4oZXrZ"
      },
      "source": [
        "Image classfication on CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZdMqc0EhZbyM",
        "colab": {}
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BWYWBJb7sQek",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UD83nFk2ZcXV",
        "outputId": "262d8946-576d-43fd-a1db-ba70bb39d92c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f5bea7b7bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u5GpFsKesRa0",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcAbvD8DZegd",
        "outputId": "e7325c7e-4202-4bf8-9562-04cdb79d8cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
        "\n",
        "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dTFUPUmDsSpr",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S81sqb8yZjRF",
        "colab": {}
      },
      "source": [
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o4qGr9SOsUYj",
        "colab": {}
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "#Training\n",
        "n_training_samples = 20000\n",
        "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
        "\n",
        "#Validation\n",
        "n_val_samples = 5000\n",
        "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
        "\n",
        "#Test\n",
        "n_test_samples = 5000\n",
        "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AnLU4lW9sdlr",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bR0wQJXcsybK"
      },
      "source": [
        "Designing a Neural Network in PyTorch\n",
        "PyTorch makes it pretty easy to implement all of those feature-engineering steps that we described above. We’ll be making use of four major functions in our CNN class:\n",
        "\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding) – applies convolution\n",
        "torch.nn.relu(x) – applies ReLU\n",
        "torch.nn.MaxPool2d(kernel_size, stride, padding) – applies max pooling\n",
        "torch.nn.Linear(in_features, out_features) – fully connected layer (multiply inputs by learned weights)* **bold text***bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BYwrcrBks1Tr",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNN(torch.nn.Module):\n",
        "    \n",
        "    #Our batch shape for input x is (3, 32, 32)\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        \n",
        "        #Input channels = 3, output channels = 18\n",
        "        self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        #4608 input features, 64 output features (see sizing flow below)\n",
        "        self.fc1 = torch.nn.Linear(18 * 16 * 16, 64)\n",
        "        \n",
        "        #64 input features, 10 output features for our 10 defined classes\n",
        "        self.fc2 = torch.nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        #Computes the activation of the first convolution\n",
        "        #Size changes from (3, 32, 32) to (18, 32, 32)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        \n",
        "        #Size changes from (18, 32, 32) to (18, 16, 16)\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        #Reshape data to input to the input layer of the neural net\n",
        "        #Size changes from (18, 16, 16) to (1, 4608)\n",
        "        #Recall that the -1 infers this dimension from the other given dimension\n",
        "        x = x.view(-1, 18 * 16 *16)\n",
        "        \n",
        "        #Computes the activation of the first fully connected layer\n",
        "        #Size changes from (1, 4608) to (1, 64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        \n",
        "        #Computes the second fully connected layer (activation applied later)\n",
        "        #Size changes from (1, 64) to (1, 10)\n",
        "        x = self.fc2(x)\n",
        "        return(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFDBlbXgs8U7",
        "colab": {}
      },
      "source": [
        "def outputSize(in_size, kernel_size, stride, padding):\n",
        "\n",
        "    output = int((in_size - kernel_size + 2*(padding)) / stride) + 1\n",
        "\n",
        "    return(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "som65AuqtQod",
        "colab": {}
      },
      "source": [
        "#DataLoader takes in a dataset and a sampler for loading (num_workers deals with system level memory) \n",
        "def get_train_loader(batch_size):\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                           sampler=train_sampler, num_workers=2)\n",
        "    return(train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d9xjxq0ytbmb",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
        "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d86XBhi-te5L",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def createLossAndOptimizer(net, learning_rate=0.001):\n",
        "    \n",
        "    #Loss function\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    \n",
        "    #Optimizer\n",
        "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
        "    \n",
        "    return(loss, optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ht3tUretp-r",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
        "    \n",
        "    #Print all of the hyperparameters of the training iteration:\n",
        "    print(\"===== HYPERPARAMETERS =====\")\n",
        "    print(\"batch_size=\", batch_size)\n",
        "    print(\"epochs=\", n_epochs)\n",
        "    print(\"learning_rate=\", learning_rate)\n",
        "    print(\"=\" * 30)\n",
        "    \n",
        "    #Get training data\n",
        "    train_loader = get_train_loader(batch_size)\n",
        "    n_batches = len(train_loader)\n",
        "    \n",
        "    #Create our loss and optimizer functions\n",
        "    loss, optimizer = createLossAndOptimizer(net, learning_rate)\n",
        "    \n",
        "    #Time for printing\n",
        "    training_start_time = time.time()\n",
        "    \n",
        "    #Loop for n_epochs\n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        print_every = n_batches // 10\n",
        "        start_time = time.time()\n",
        "        total_train_loss = 0\n",
        "        \n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            \n",
        "            #Get inputs\n",
        "            inputs, labels = data\n",
        "            \n",
        "            #Wrap them in a Variable object\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            #Set the parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            #Forward pass, backward pass, optimize\n",
        "            outputs = net(inputs)\n",
        "            loss_size = loss(outputs, labels)\n",
        "            loss_size.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            #Print statistics\n",
        "            \n",
        "            \n",
        "            #Print every 10th batch of an epoch\n",
        "            if (i + 1) % (print_every + 1) == 0:\n",
        "                print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took: {:.2f}s\".format(\n",
        "                        epoch+1, int(100 * (i+1) / n_batches), running_loss / print_every, time.time() - start_time))\n",
        "                #Reset running loss and time\n",
        "                running_loss = 0.0\n",
        "                start_time = time.time()\n",
        "            \n",
        "        #At the end of the epoch, do a pass on the validation set\n",
        "        total_val_loss = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            \n",
        "            #Wrap tensors in Variables\n",
        "            inputs, labels = Variable(inputs), Variable(labels)\n",
        "            \n",
        "            #Forward pass\n",
        "            val_outputs = net(inputs)\n",
        "            val_loss_size = loss(val_outputs, labels)\n",
        "            \n",
        "            \n",
        "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
        "        \n",
        "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R3FzvF9euJya",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DB7Azjd7t0k7",
        "outputId": "92c2565c-ba6f-4f85-9068-08e205f71294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "CNN = SimpleCNN()\n",
        "trainNet(CNN, batch_size=32, n_epochs=5, learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===== HYPERPARAMETERS =====\n",
            "batch_size= 32\n",
            "epochs= 5\n",
            "learning_rate= 0.001\n",
            "==============================\n",
            "Epoch 1, 10% \t train_loss: 0.00 took: 3.11s\n",
            "Epoch 1, 20% \t train_loss: 0.00 took: 2.71s\n",
            "Epoch 1, 30% \t train_loss: 0.00 took: 2.70s\n",
            "Epoch 1, 40% \t train_loss: 0.00 took: 2.71s\n",
            "Epoch 1, 50% \t train_loss: 0.00 took: 2.70s\n",
            "Epoch 1, 60% \t train_loss: 0.00 took: 2.65s\n",
            "Epoch 1, 70% \t train_loss: 0.00 took: 2.72s\n",
            "Epoch 1, 80% \t train_loss: 0.00 took: 2.71s\n",
            "Epoch 1, 90% \t train_loss: 0.00 took: 2.69s\n",
            "Validation loss = 0.00\n",
            "Epoch 2, 10% \t train_loss: 0.00 took: 3.01s\n",
            "Epoch 2, 20% \t train_loss: 0.00 took: 3.75s\n",
            "Epoch 2, 30% \t train_loss: 0.00 took: 3.88s\n",
            "Epoch 2, 40% \t train_loss: 0.00 took: 3.58s\n",
            "Epoch 2, 50% \t train_loss: 0.00 took: 3.42s\n",
            "Epoch 2, 60% \t train_loss: 0.00 took: 3.40s\n",
            "Epoch 2, 70% \t train_loss: 0.00 took: 3.42s\n",
            "Epoch 2, 80% \t train_loss: 0.00 took: 3.38s\n",
            "Epoch 2, 90% \t train_loss: 0.00 took: 3.37s\n",
            "Validation loss = 0.00\n",
            "Epoch 3, 10% \t train_loss: 0.00 took: 3.45s\n",
            "Epoch 3, 20% \t train_loss: 0.00 took: 3.35s\n",
            "Epoch 3, 30% \t train_loss: 0.00 took: 3.37s\n",
            "Epoch 3, 40% \t train_loss: 0.00 took: 3.39s\n",
            "Epoch 3, 50% \t train_loss: 0.00 took: 3.42s\n",
            "Epoch 3, 60% \t train_loss: 0.00 took: 3.40s\n",
            "Epoch 3, 70% \t train_loss: 0.00 took: 3.41s\n",
            "Epoch 3, 80% \t train_loss: 0.00 took: 3.47s\n",
            "Epoch 3, 90% \t train_loss: 0.00 took: 3.45s\n",
            "Validation loss = 0.00\n",
            "Epoch 4, 10% \t train_loss: 0.00 took: 3.48s\n",
            "Epoch 4, 20% \t train_loss: 0.00 took: 3.45s\n",
            "Epoch 4, 30% \t train_loss: 0.00 took: 3.41s\n",
            "Epoch 4, 40% \t train_loss: 0.00 took: 3.40s\n",
            "Epoch 4, 50% \t train_loss: 0.00 took: 3.36s\n",
            "Epoch 4, 60% \t train_loss: 0.00 took: 3.37s\n",
            "Epoch 4, 70% \t train_loss: 0.00 took: 3.38s\n",
            "Epoch 4, 80% \t train_loss: 0.00 took: 3.39s\n",
            "Epoch 4, 90% \t train_loss: 0.00 took: 3.36s\n",
            "Validation loss = 0.00\n",
            "Epoch 5, 10% \t train_loss: 0.00 took: 3.49s\n",
            "Epoch 5, 20% \t train_loss: 0.00 took: 3.37s\n",
            "Epoch 5, 30% \t train_loss: 0.00 took: 3.39s\n",
            "Epoch 5, 40% \t train_loss: 0.00 took: 3.43s\n",
            "Epoch 5, 50% \t train_loss: 0.00 took: 3.45s\n",
            "Epoch 5, 60% \t train_loss: 0.00 took: 3.41s\n",
            "Epoch 5, 70% \t train_loss: 0.00 took: 3.38s\n",
            "Epoch 5, 80% \t train_loss: 0.00 took: 3.38s\n",
            "Epoch 5, 90% \t train_loss: 0.00 took: 3.44s\n",
            "Validation loss = 0.00\n",
            "Training finished, took 177.83s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_ssjxvguLTa",
        "colab": {}
      },
      "source": [
        "https://www.kaggle.com/juiyangchang/cnn-with-pytorch-0-995-accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kA5MX-mkvTuI"
      },
      "source": [
        "https://www.kaggle.com/juiyangchang/cnn-with-pytorch-0-995-accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7lxp5uzbvVCZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQwKSHw2uE8K",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QqP2vNJcs_Kb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NkQz7y9lYXfP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9-DO7jsYVbu",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TtTimVFJYTMG",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R-uX4pi-YRUG",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYRtEd50WYlH",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Pr82hvqT43g",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grLy31-TTFT4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JVpnbsAaS9Xs",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lBn3W3c1S22Q",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1X1Sw9fGRelS",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hWhb6isyRcIq",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ep_RG2gQdkp",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cH_As1jrQqES",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3XJFZiDuQqMi",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uw9aYOvPQZH5",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vC2OHArTPWM5",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DkfGCiC3PPsZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qVl9YqCePELZ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q3fqljwvPBQy",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mATSj8TMO9Iq",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SNlBUSxqNtT7",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}